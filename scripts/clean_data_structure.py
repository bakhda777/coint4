#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ—á–∏—â–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö.
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ parquet —Ñ–∞–π–ª—ã –∑–∞ –¥–µ–Ω—å –≤ –æ–¥–∏–Ω –∏ —É–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã.
–°–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å symbol= –ø–∞—Ä—Ç–∏—Ü–∏—è–º–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –æ—Å–Ω–æ–≤–Ω—ã–º –∫–æ–¥–æ–º.
"""

import pandas as pd
from pathlib import Path
from typing import List, Tuple
import logging
from tqdm import tqdm
import shutil

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def get_trading_pairs(data_dir: Path) -> List[str]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–∞–Ω–Ω—ã—Ö."""
    pairs = []
    for item in data_dir.iterdir():
        if item.is_dir() and not item.name.startswith('.'):
            pairs.append(item.name)
    return sorted(pairs)


def get_day_directories(pair_dir: Path) -> List[Tuple[str, str, str, Path]]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–Ω–µ–π –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–π –ø–∞—Ä—ã."""
    day_dirs = []
    
    for year_dir in pair_dir.iterdir():
        if not year_dir.is_dir() or not year_dir.name.startswith('year='):
            continue
        
        year = year_dir.name.split('=')[1]
        
        for month_dir in year_dir.iterdir():
            if not month_dir.is_dir() or not month_dir.name.startswith('month='):
                continue
            
            month = month_dir.name.split('=')[1]
            
            for day_dir in month_dir.iterdir():
                if not day_dir.is_dir() or not day_dir.name.startswith('day='):
                    continue
                
                day = day_dir.name.split('=')[1]
                day_dirs.append((year, month, day, day_dir))
    
    return day_dirs


def clean_day_data(day_dir: Path) -> pd.DataFrame:
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ parquet —Ñ–∞–π–ª—ã –∑–∞ –¥–µ–Ω—å –∏ —É–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã."""
    parquet_files = list(day_dir.glob('*.parquet'))
    
    if not parquet_files:
        logger.warning(f"–ù–µ—Ç parquet —Ñ–∞–π–ª–æ–≤ –≤ {day_dir}")
        return pd.DataFrame()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∑–∞ –¥–µ–Ω—å
    dataframes = []
    for file in parquet_files:
        try:
            df = pd.read_parquet(file)
            dataframes.append(df)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {file}: {e}")
            continue
    
    if not dataframes:
        return pd.DataFrame()
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
    combined_df = pd.concat(dataframes, ignore_index=True)
    
    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ timestamp –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
    cleaned_df = combined_df.drop_duplicates(subset=['timestamp']).sort_values('timestamp')
    
    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å
    cleaned_df = cleaned_df.reset_index(drop=True)
    
    logger.debug(f"–î–µ–Ω—å {day_dir.name}: {len(combined_df)} -> {len(cleaned_df)} –∑–∞–ø–∏—Å–µ–π")
    
    return cleaned_df


def create_clean_structure(source_dir: Path, target_dir: Path, pairs_limit: int = None):
    """–°–æ–∑–¥–∞–µ—Ç –æ—á–∏—â–µ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö —Å symbol= –ø–∞—Ä—Ç–∏—Ü–∏—è–º–∏."""
    
    # –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    target_dir.mkdir(exist_ok=True)
    
    # –ö–æ–ø–∏—Ä—É–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ñ–∞–π–ª—ã
    for file_name in ['.symbols_cache.json', '.gitkeep', 'ignore.txt']:
        source_file = source_dir / file_name
        if source_file.exists():
            shutil.copy2(source_file, target_dir / file_name)
            logger.info(f"–°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω —Ñ–∞–π–ª: {file_name}")
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä
    trading_pairs = get_trading_pairs(source_dir)
    
    if pairs_limit:
        trading_pairs = trading_pairs[:pairs_limit]
        logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ {pairs_limit} –ø–∞—Ä –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
    
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(trading_pairs)} —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_days_processed = 0
    total_files_created = 0
    errors = 0
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Ç–æ—Ä–≥–æ–≤—É—é –ø–∞—Ä—É
    for pair in tqdm(trading_pairs, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä"):
        source_pair_dir = source_dir / pair
        
        # ‚úÖ –ì–õ–ê–í–ù–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: —Å–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å symbol= –ø—Ä–µ—Ñ–∏–∫—Å–æ–º
        target_pair_dir = target_dir / f"symbol={pair}"
        target_pair_dir.mkdir(exist_ok=True)
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–Ω–∏ –¥–ª—è —ç—Ç–æ–π –ø–∞—Ä—ã
        day_directories = get_day_directories(source_pair_dir)
        
        if not day_directories:
            logger.warning(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–∞—Ä—ã {pair}")
            continue
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –¥–µ–Ω—å
        for year, month, day, source_day_dir in tqdm(day_directories, 
                                                     desc=f"–î–Ω–∏ –¥–ª—è {pair}", 
                                                     leave=False):
            try:
                # –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞ –¥–µ–Ω—å
                cleaned_df = clean_day_data(source_day_dir)
                
                if cleaned_df.empty:
                    logger.warning(f"–ü—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è {pair} {year}-{month}-{day}")
                    errors += 1
                    continue
                
                # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Å—Ç–æ–ª–±–µ—Ü symbol –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π
                if 'symbol' not in cleaned_df.columns:
                    cleaned_df['symbol'] = pair
                elif not cleaned_df['symbol'].eq(pair).all():
                    cleaned_df['symbol'] = pair
                
                # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –≤ —Ü–µ–ª–µ–≤–æ–π –ø–∞–ø–∫–µ
                target_year_dir = target_pair_dir / f"year={year}"
                # ‚úÖ –î–æ–±–∞–≤–ª—è–µ–º –≤–µ–¥—É—â–∏–µ –Ω—É–ª–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                target_month_dir = target_year_dir / f"month={month.zfill(2)}"
                target_day_dir = target_month_dir / f"day={day.zfill(2)}"
                
                target_day_dir.mkdir(parents=True, exist_ok=True)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª
                output_file = target_day_dir / "data.parquet"
                cleaned_df.to_parquet(output_file, index=False)
                
                total_days_processed += 1
                total_files_created += 1
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {pair} {year}-{month}-{day}: {e}")
                errors += 1
                continue
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    logger.info("=" * 50)
    logger.info("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò:")
    logger.info(f"–¢–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(trading_pairs)}")
    logger.info(f"–î–Ω–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_days_processed}")
    logger.info(f"–§–∞–π–ª–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {total_files_created}")
    logger.info(f"–û—à–∏–±–æ–∫: {errors}")
    logger.info("=" * 50)


def update_config_for_clean_data():
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è data_clean."""
    config_file = Path("configs/main.yaml")
    
    if not config_file.exists():
        logger.warning("–§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ configs/main.yaml –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    # –ß–∏—Ç–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    with open(config_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # –ó–∞–º–µ–Ω—è–µ–º data_dir –Ω–∞ data_clean
    if 'data_dir: "data_clean"' in content:
        logger.info("–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ –Ω–∞ data_clean")
    else:
        # –ó–∞–º–µ–Ω—è–µ–º –ª—é–±–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ data_dir –Ω–∞ data_clean
        import re
        new_content = re.sub(
            r'^data_dir:\s*"[^"]*"',
            'data_dir: "data_clean"',
            content,
            flags=re.MULTILINE
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        with open(config_file, 'w', encoding='utf-8') as f:
            f.write(new_content)
        
        logger.info("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞: data_dir -> data_clean")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    source_dir = Path("data")
    target_dir = Path("data_clean")
    
    if not source_dir.exists():
        logger.error(f"–ò—Å—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {source_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        return
    
    logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –æ—á–∏—Å—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö –∏–∑ {source_dir} –≤ {target_dir}")
    logger.info("‚úÖ –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å symbol= –ø–∞—Ä—Ç–∏—Ü–∏—è–º–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏")
    
    # –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–∂–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä
    # create_clean_structure(source_dir, target_dir, pairs_limit=5)
    
    # –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    create_clean_structure(source_dir, target_dir)
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    update_config_for_clean_data()
    
    logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    logger.info("üéâ –¢–µ–ø–µ—Ä—å –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥ –ø—Ä–æ–µ–∫—Ç–∞ –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å data_clean –Ω–∞–ø—Ä—è–º—É—é!")


if __name__ == "__main__":
    main() 