import logging
import os

import numpy as np
import threading
import time
from pathlib import Path
import dask.dataframe as dd
import numpy as np
import pandas as pd  # type: ignore
import pyarrow as pa
import pyarrow.dataset as ds

from coint2.utils import empty_ddf, ensure_datetime_index
from coint2.utils.config import AppConfig
from coint2.utils.timing_utils import logged_time, time_block, log_progress

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
logger = logging.getLogger(__name__)


def _scan_parquet_files(path: Path | str, glob: str = "*.parquet", max_shards: int | None = None) -> ds.Dataset:
    """Build pyarrow dataset from parquet files under ``path``."""
    base = Path(path)
    files = sorted(base.rglob(glob))
    if max_shards is not None:
        files = files[:max_shards]
    if not files:
        return ds.dataset(pa.table({}))
    return ds.dataset([str(f) for f in files], format="parquet", partitioning="hive")


def _dir_mtime_hash(path: Path) -> float:
    """Return hash value based on modification times of ``.parquet`` files."""
    mtimes = [f.stat().st_mtime for f in path.rglob("*.parquet")]
    return max(mtimes) if mtimes else 0.0


class DataHandler:
    """Utility class for loading local parquet price files."""

    def __init__(self, cfg: AppConfig, autorefresh: bool = True) -> None:
        self.data_dir = Path(cfg.data_dir)
        self.cache_dir = self.data_dir.parent / ".cache"  # –î–æ–±–∞–≤–ª—è–µ–º cache_dir
        self.fill_limit_pct = cfg.backtest.fill_limit_pct
        self.max_shards = cfg.max_shards
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.autorefresh = autorefresh
        self._all_data_cache: dict[str, tuple[dd.DataFrame, float]] = {}
        self._freq: str | None = None
        self._lock = threading.Lock()
        self.lookback_days: int = cfg.pair_selection.lookback_days

    @property
    def freq(self) -> str | None:
        """Return detected time step of the loaded data."""
        with self._lock:
            return self._freq

    def clear_cache(self) -> None:
        """Clears the in-memory Dask DataFrame cache."""
        with self._lock:
            self._all_data_cache.clear()
            self._freq = None

    @logged_time("get_all_symbols")
    def get_all_symbols(self) -> list[str]:
        """Return list of symbols based on partition directory names."""
        if not self.data_dir.exists():
            logger.warning(f"Data directory does not exist: {self.data_dir}")
            return []

        symbols = []
        with time_block("scanning symbol directories"):
            for p in self.data_dir.iterdir():
                if not p.is_dir():
                    continue
                if p.name.startswith("symbol="):
                    symbols.append(p.name.replace("symbol=", ""))
                else:
                    symbols.append(p.name)
        
        logger.info(f"Found {len(symbols)} symbols: {symbols[:10]}{'...' if len(symbols) > 10 else ''}")
        return sorted(symbols)

    @logged_time("load_full_dataset")
    def _load_full_dataset(self) -> dd.DataFrame:
        """
        Load the dataset for the configured period as a dask dataframe,
        using predicate pushdown for efficiency.
        """
        try:
            # –≠—Ç–∏ –¥–∞—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã –≤ —ç–∫–∑–µ–º–ø–ª—è—Ä–µ DataHandler
            # –∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –≤–æ –≤—Ä–µ–º—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏.
            start_date = self.start_date
            end_date = self.end_date

            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥ {start_date} - {end_date}")

            # dd.read_parquet –ª—É—á—à–µ –≤—Å–µ–≥–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å np.datetime64 –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–æ–≤
            start_ts = np.datetime64(start_date)
            end_ts = np.datetime64(end_date)

            logger.debug(f"–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ timestamp: {start_ts} - {end_ts}")

            cols_to_load = ["timestamp", "symbol", "close"]

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º dd.read_parquet —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            ddf = dd.read_parquet(
                self.data_dir,
                engine="pyarrow",
                columns=cols_to_load,
                filters=[("timestamp", ">=", start_ts), ("timestamp", "<=", end_ts)],
                gather_statistics=True,  # –í–∞–∂–Ω–æ –¥–ª—è –æ—Ç—Å–µ—á–µ–Ω–∏—è –ø–∞—Ä—Ç–∏—Ü–∏–π (partition pruning)
            )

            # –†–µ–ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä—É–µ–º –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –∏ –∫–µ—à–∏—Ä—É–µ–º –≤ –ø–∞–º—è—Ç–∏
            # npartitions –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–∏—Å—Ç–µ–º—ã
            num_partitions = os.cpu_count() * 2
            ddf = ddf.repartition(npartitions=num_partitions).persist()

            logger.info(f"Dask dataframe —Å–æ–∑–¥–∞–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –ø–∞–º—è—Ç–∏: {len(ddf.columns)} –∫–æ–ª–æ–Ω–æ–∫, {ddf.npartitions} –ø–∞—Ä—Ç–∏—Ü–∏–π.")

            return ddf

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π DataFrame –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            return empty_ddf()

    @logged_time("load_all_data_for_period")
    def load_all_data_for_period(self, lookback_days: int | None = None) -> pd.DataFrame:
        """Load close prices for all symbols for the specified or configured lookback period.
        
        Parameters
        ----------
        lookback_days : int | None, optional
            Number of days to look back. If None, uses self.lookback_days.
            
        Returns
        -------
        pd.DataFrame
            DataFrame with close prices for all symbols.
        """
        try:
            with time_block("loading full dataset"):
                ddf = self._load_full_dataset()

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ–π DataFrame
            if not ddf.columns.tolist():
                logger.warning("Empty dataset - no columns found")
                return pd.DataFrame()

            with time_block("computing dataset and applying filters"):
                # –ü—Ä–∏–≤–æ–¥–∏–º ddf['timestamp'] –∫ datetime –µ—Å–ª–∏ —ç—Ç–æ –µ—â–µ –Ω–µ —Å–¥–µ–ª–∞–Ω–æ
                if not pd.api.types.is_datetime64_any_dtype(ddf["timestamp"]):
                    ddf["timestamp"] = dd.to_datetime(ddf["timestamp"], unit="ms")

                # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ–¥–∏–Ω —Ä–∞–∑
                all_data = ddf.compute()
                logger.info(f"Computed dataset: {len(all_data):,} rows, {len(all_data.columns)} columns")

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–º–∫–∏  
                days_back = lookback_days or self.lookback_days
                max_ts = all_data["timestamp"].max()
                start_ts = max_ts - pd.Timedelta(days=days_back)
                
                logger.info(f"Time range: {start_ts} to {max_ts} ({days_back} days)")

            # –§–∏–ª—å—Ç—Ä—É–µ–º —É–∂–µ –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            with time_block("filtering data by time range"):
                filtered_df = all_data[all_data["timestamp"] >= start_ts].copy()
                logger.info(f"After time filtering: {len(filtered_df):,} rows")
                
                if filtered_df.empty:
                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π DataFrame —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏ –∏ –∏–º–µ–Ω–µ–º –∏–Ω–¥–µ–∫—Å–∞
                    logger.warning("No data in specified time range")
                    return pd.DataFrame(columns=all_data.columns, index=pd.DatetimeIndex([], name="timestamp"))

                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º timestamp –≤ datetime
                filtered_df["timestamp"] = pd.to_datetime(filtered_df["timestamp"], unit="ms")

            # –ü–∏–≤–æ—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
            with time_block("pivoting data"):
                result = filtered_df.pivot_table(
                    index="timestamp", columns="symbol", values="close"
                )
                logger.info(f"Pivoted data: {len(result)} rows, {len(result.columns)} symbols")

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º missing data
            with time_block("handling missing data"):
                initial_symbols = len(result.columns)
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º fill_limit_pct
                if hasattr(self, 'fill_limit_pct'):
                    fill_limit = int(len(result) * self.fill_limit_pct)
                    result = result.fillna(method='ffill', limit=fill_limit)
                    result = result.fillna(method='bfill', limit=fill_limit)
                
                # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ª–±—Ü—ã —Å —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º NaN
                nan_threshold = 0.5  # Remove symbols with >50% NaN
                clean_result = result.dropna(axis=1, thresh=int(len(result) * (1 - nan_threshold)))
                
                removed_symbols = initial_symbols - len(clean_result.columns)
                if removed_symbols > 0:
                    logger.info(f"Removed {removed_symbols} symbols with >50% missing data")
                
                logger.info(f"Final result: {len(clean_result)} rows, {len(clean_result.columns)} symbols")

            return clean_result

        except Exception as e:
            logger.error(f"Error in load_all_data_for_period: {e}")
            return pd.DataFrame()

    def load_pair_data(
        self,
        symbol1: str,
        symbol2: str,
        start_date: pd.Timestamp,
        end_date: pd.Timestamp,
    ) -> pd.DataFrame:
        """Load and align data for two symbols within the given date range."""
        # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º, —á—Ç–æ –¥–∞—Ç—ã –≤ –Ω–∞–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (–±–µ–∑ timezone)
        if start_date.tzinfo is not None:
            logger.debug(f"–£–¥–∞–ª—è—é timezone –∏–∑ start_date: {start_date}")
            start_date = start_date.tz_localize(None)
        if end_date.tzinfo is not None:
            logger.debug(f"–£–¥–∞–ª—è—é timezone –∏–∑ end_date: {end_date}")
            end_date = end_date.tz_localize(None)
            
        logger.debug(
            f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–∞—Ä—ã {symbol1}-{symbol2} ({start_date} - {end_date})"
        )

        if not self.data_dir.exists():
            logger.warning(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–∞–Ω–Ω—ã—Ö {self.data_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            return pd.DataFrame()

        start_ms = int(start_date.timestamp() * 1000)
        end_ms = int(end_date.timestamp() * 1000)
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º partition-filters (symbol, year, month, day)
        filters = [
            ("symbol", "in", [symbol1, symbol2]),
            ("year", ">=", start_date.year),
            ("year", "<=", end_date.year),
            ("month", ">=", start_date.month if start_date.year == end_date.year else 1),
            ("month", "<=", end_date.month if start_date.year == end_date.year else 12),
            ("timestamp", ">=", start_ms),
            ("timestamp", "<=", end_ms),
        ]

        try:
            logger.debug("–ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É —Å partition-filters")
            ddf = dd.read_parquet(
                self.data_dir,
                engine="pyarrow",
                columns=["timestamp", "close", "symbol"],
                ignore_metadata_file=True,
                calculate_divisions=False,
                filters=filters,
                validate_schema=False,
            )
            pair_pdf = ddf.compute()
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å partition-filters: {str(e)}")
            # –û—Ç–≤–∞–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ ¬´—Å—ã—Ä–æ–µ¬ª —á—Ç–µ–Ω–∏–µ
            try:
                logger.debug("Fallback: –∑–∞–≥—Ä—É–∑–∫–∞ –±–µ–∑ partition-filters")
                basic_filters = [
                    ("symbol", "in", [symbol1, symbol2]),
                    ("timestamp", ">=", start_ms),
                    ("timestamp", "<=", end_ms),
                ]
                ddf = dd.read_parquet(
                    self.data_dir,
                    engine="pyarrow",
                    columns=["timestamp", "close", "symbol"],
                    ignore_metadata_file=True,
                    calculate_divisions=False,
                    filters=basic_filters,
                    validate_schema=False,
                )
                pair_pdf = ddf.compute()
            except Exception as e2:  # pragma: no cover - fallback rarely used
                logger.debug(f"Error loading pair data via Dask: {str(e2)}")
                try:
                    logger.debug("Fallback to pyarrow dataset scanning in load_pair_data")
                    dataset = _scan_parquet_files(self.data_dir, max_shards=self.max_shards)
                    arrow_filter = (
                        ds.field("symbol").isin([symbol1, symbol2])
                        & (ds.field("timestamp") >= pa.scalar(start_date))
                        & (ds.field("timestamp") <= pa.scalar(end_date))
                    )
                    table = dataset.to_table(
                        columns=["timestamp", "close", "symbol"],
                        filter=arrow_filter,
                    )
                    pair_pdf = table.to_pandas()
                except Exception as e3:  # pragma: no cover - fallback rarely used
                    logger.debug(f"Error loading pair data manually: {str(e3)}")
                    return pd.DataFrame()

        if pair_pdf.empty:
            logger.debug(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–∞—Ä—ã {symbol1}-{symbol2}")
            return pd.DataFrame()
        pair_pdf["timestamp"] = pd.to_datetime(pair_pdf["timestamp"], unit="ms").dt.tz_localize(None)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ timestamp
        if pair_pdf.duplicated(subset=["timestamp", "symbol"]).any():
            logger.debug(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã timestamp –¥–ª—è –ø–∞—Ä—ã {symbol1}-{symbol2}. –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã.")
            pair_pdf = pair_pdf.drop_duplicates(subset=["timestamp", "symbol"])

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —à–∏—Ä–æ–∫–∏–π —Ñ–æ—Ä–º–∞—Ç (timestamp x symbols)
        wide_df = pair_pdf.pivot_table(index="timestamp", columns="symbol", values="close")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
        if wide_df.empty or len(wide_df.columns) < 2:
            return pd.DataFrame()

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        freq_val = pd.infer_freq(wide_df.index)
        with self._lock:
            self._freq = freq_val
        if freq_val:
            wide_df = wide_df.asfreq(freq_val)

        # —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –ø–æ–¥—Ä—è–¥ –∏–¥—É—â–∏—Ö NA –ø–æ –ø—Ä–æ—Ü–µ–Ω—Ç—É
        limit = max(1, int(len(wide_df) * self.fill_limit_pct))

        wide_df = wide_df.ffill(limit=limit).bfill(limit=limit)


        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ —É–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NA
        if symbol1 in wide_df.columns and symbol2 in wide_df.columns:
            return wide_df[[symbol1, symbol2]].dropna()
        else:
            return pd.DataFrame()

    def load_and_normalize_data(
        self, start_date: pd.Timestamp, end_date: pd.Timestamp
    ) -> pd.DataFrame:
        """
        Load and normalize data for all symbols within the given date range.
        
        Parameters
        ----------
        start_date : pd.Timestamp
            –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        end_date : pd.Timestamp
            –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
            
        Returns
        -------
        pd.DataFrame
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π DataFrame —Å —Ü–µ–Ω–∞–º–∏ (–Ω–∞—á–∞–ª–æ = 100)
        """
        # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º, —á—Ç–æ –¥–∞—Ç—ã –≤ –Ω–∞–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (–±–µ–∑ timezone)
        if start_date.tzinfo is not None:
            logger.debug(f"–£–¥–∞–ª—è—é timezone –∏–∑ start_date: {start_date}")
            start_date = start_date.tz_localize(None)
        if end_date.tzinfo is not None:
            logger.debug(f"–£–¥–∞–ª—è—é timezone –∏–∑ end_date: {end_date}")
            end_date = end_date.tz_localize(None)
            
        logger.debug(f"–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–µ—Ä–∏–æ–¥ {start_date} - {end_date}")
        
        start_time = time.time()
        data_df = self.preload_all_data(start_date, end_date)
        elapsed_time = time.time() - start_time
        logger.info(f"–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥. –†–∞–∑–º–µ—Ä: {data_df.shape}")

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ü–µ–Ω
        if not data_df.empty:
            logger.debug(
                f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –¥–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {len(data_df.columns)}"
            )

            numeric_cols = [
                c
                for c in data_df.columns
                if pd.api.types.is_numeric_dtype(data_df[c]) and c != "timestamp"
            ]
            if numeric_cols:
                for col in numeric_cols:
                    series = data_df[col]
                    first_val = series.loc[series.first_valid_index()] if series.first_valid_index() is not None else pd.NA
                    if pd.isna(first_val) or first_val == 0:
                        data_df[col] = 0.0
                    else:
                        data_df[col] = 100 * series / first_val

            logger.debug(
                f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {len(data_df.columns)}"
            )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã—Ö —Å–µ—Ä–∏–π –∏ —Å–µ—Ä–∏–π —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–æ–ø—É—Å–∫–æ–≤
            valid_columns = []
            for column in data_df.columns:
                if pd.api.types.is_numeric_dtype(data_df[column]):
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω–æ—Å—Ç—å —Å–µ—Ä–∏–∏
                    if data_df[column].nunique() > 1:
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤ (–±–æ–ª–µ–µ 50%)
                        na_pct = data_df[column].isna().mean()
                        if na_pct < 0.5:
                            valid_columns.append(column)
                            
            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã
            if valid_columns:
                logger.debug(
                    f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(data_df.columns) - len(valid_columns)} "
                    "–∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã—Ö –∏–ª–∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö —Å–µ—Ä–∏–π"
                )
                data_df = data_df[valid_columns]

        return data_df

    def preload_all_data(
        self, start_date: pd.Timestamp, end_date: pd.Timestamp
    ) -> pd.DataFrame:
        """Loads and pivots all data for a given wide date range."""
        # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º, —á—Ç–æ –¥–∞—Ç—ã –≤ –Ω–∞–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (–±–µ–∑ timezone)
        if start_date.tzinfo is not None:
            logger.debug(f"–£–¥–∞–ª—è—é timezone –∏–∑ start_date –≤ preload_all_data: {start_date}")
            start_date = start_date.tz_localize(None)
        if end_date.tzinfo is not None:
            logger.debug(f"–£–¥–∞–ª—è—é timezone –∏–∑ end_date –≤ preload_all_data: {end_date}")
            end_date = end_date.tz_localize(None)
            
        logger.debug(f"–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–µ—Ä–∏–æ–¥ {start_date} - {end_date}")
            
        if not self.data_dir.exists():
            logger.warning(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–∞–Ω–Ω—ã—Ö {self.data_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            return pd.DataFrame()

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç—ã –≤ timestamp —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        start_ts = int(start_date.timestamp() * 1000)
        end_ts = int(end_date.timestamp() * 1000)
        logger.debug(f"–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ timestamp: {start_ts} - {end_ts}")
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º partition-filters (year, month, day)
        partition_filters = [
            ("year", ">=", start_date.year),
            ("year", "<=", end_date.year),
            ("month", ">=", start_date.month if start_date.year == end_date.year else 1),
            ("month", "<=", end_date.month if start_date.year == end_date.year else 12),
            ("timestamp", ">=", start_ts),
            ("timestamp", "<=", end_ts),
        ]

        try:
            logger.debug("–ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É —Å partition-filters")
            ddf = dd.read_parquet(
                self.data_dir,
                engine="pyarrow",
                columns=["timestamp", "close", "symbol"],
                ignore_metadata_file=True,
                calculate_divisions=False,
                filters=partition_filters,
                validate_schema=False,
            )

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ pandas –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            all_data = ddf.compute()
            logger.debug(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π –≤—Å–µ–≥–æ: {len(all_data)}")
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å partition-filters: {str(e)}")
            # –û—Ç–≤–∞–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ ¬´—Å—ã—Ä–æ–µ¬ª —á—Ç–µ–Ω–∏–µ
            try:
                # –î–ª—è –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ª—É—á—à–µ –∑–∞–≥—Ä—É–∂–∞—Ç—å –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∏ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ
                logger.debug("Fallback: –∑–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–æ–≤...")
                ddf = dd.read_parquet(
                    self.data_dir,
                    engine="pyarrow",
                    columns=["timestamp", "close", "symbol"],
                    ignore_metadata_file=True,
                    calculate_divisions=False,
                    validate_schema=False,
                )

                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ pandas –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                all_data = ddf.compute()
                logger.debug(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π –≤—Å–µ–≥–æ: {len(all_data)}")
            except Exception as e2:
                logger.error(f"Error loading data via Dask: {str(e2)}")

                try:
                    logger.debug("Fallback to pyarrow dataset scanning in preload_all_data")
                    dataset = _scan_parquet_files(self.data_dir, max_shards=self.max_shards)
                    table = dataset.to_table(columns=["timestamp", "close", "symbol"])
                    all_data = table.to_pandas()
                    if all_data.empty:
                        return pd.DataFrame()
                except Exception as e3:
                    logger.error(f"Error loading data manually: {str(e3)}")
                    return pd.DataFrame()

        logger.debug(f"–¢–∏–ø all_data['timestamp']: {all_data['timestamp'].dtype}")
        logger.debug(f"–ü—Ä–∏–º–µ—Ä—ã –∑–Ω–∞—á–µ–Ω–∏–π timestamp: {all_data['timestamp'].head(5).tolist()}")
        
        if np.issubdtype(all_data['timestamp'].dtype, np.datetime64):
            logger.debug('–ü—Ä–µ–æ–±—Ä–∞–∑—É—é timestamp –∏–∑ datetime64[ns] –≤ int/ms')
            all_data['timestamp'] = all_data['timestamp'].astype('int64') // 10**6
            logger.debug(f"–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–æ. –ü—Ä–∏–º–µ—Ä—ã: {all_data['timestamp'].head(5).tolist()}")
        
        if all_data.empty:
            logger.warning(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {self.data_dir}")
            return pd.DataFrame()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –≤ pandas
        mask = (all_data["timestamp"] >= start_ts) & (all_data["timestamp"] <= end_ts)
        filtered_df = all_data[mask]
        logger.debug(f"–ó–∞–ø–∏—Å–µ–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏: {len(filtered_df)}")
        
        if filtered_df.empty:
            logger.warning(f"No data found between {start_date} and {end_date}")
            logger.debug(f"–î–æ—Å—Ç—É–ø–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –≤—Ä–µ–º–µ–Ω–∏: {pd.to_datetime(all_data['timestamp'].min(), unit='ms')} - {pd.to_datetime(all_data['timestamp'].max(), unit='ms')}")
            return pd.DataFrame()

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º timestamp –≤ datetime –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞
        filtered_df["timestamp"] = pd.to_datetime(filtered_df["timestamp"], unit="ms")
        
        if filtered_df.duplicated(subset=["timestamp", "symbol"]).any():
            wide_pdf = filtered_df.pivot_table(
                index="timestamp",
                columns="symbol",
                values="close",
                aggfunc="last",
            )
        else:
            wide_pdf = filtered_df.pivot(
                index="timestamp",
                columns="symbol",
                values="close",
            )

        if wide_pdf.empty:
            logger.debug(f"–ü—É—Å—Ç–æ–π pivot –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è")
            return pd.DataFrame()

        wide_pdf = ensure_datetime_index(wide_pdf)
        logger.debug(f"–ò—Ç–æ–≥–æ–≤—ã–µ –¥–∞–Ω–Ω—ã: {wide_pdf.shape}, –ø–µ—Ä–∏–æ–¥: {wide_pdf.index.min()} - {wide_pdf.index.max()}")

        freq_val = pd.infer_freq(wide_pdf.index)
        with self._lock:
            self._freq = freq_val
        if freq_val:
            wide_pdf = wide_pdf.asfreq(freq_val)

        return wide_pdf


# New function for optimized dataset loading
import pandas as pd
import pyarrow.dataset as ds


def load_master_dataset(data_path: str, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–∞–¥–µ–∂–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä
    –ø–æ –ø–∞—Ä—Ç–∏—Ü–∏—è–º 'year' –∏ 'month', –∫–æ—Ç–æ—Ä—ã–π –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–µ—Ä–µ—Ö–æ–¥—ã —á–µ—Ä–µ–∑ –≥–æ–¥.
    """
    print(f"‚öôÔ∏è  –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–µ—Ä–∏–æ–¥: {start_date.date()} -> {end_date.date()}")

    # --- –ù–ê–ß–ê–õ–û –ü–†–ê–í–ò–õ–¨–ù–û–ì–û –ö–û–î–ê ---

    # –ó–∞–¥–∞—á–∞ —ç—Ç–æ–≥–æ –±–ª–æ–∫–∞ ‚Äî —Å–æ–∑–¥–∞—Ç—å –Ω–∞–¥–µ–∂–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä, –∫–æ—Ç–æ—Ä—ã–π –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç
    # –ª—é–±–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç, –≤–∫–ª—é—á–∞—è –ø–µ—Ä–µ—Ö–æ–¥—ã —á–µ—Ä–µ–∑ –≥–æ–¥.

    # 1. –°–æ–∑–¥–∞–µ–º —É—Å–ª–æ–≤–∏–µ –¥–ª—è –Ω–∏–∂–Ω–µ–π –≥—Ä–∞–Ω–∏—Ü—ã –¥–∏–∞–ø–∞–∑–æ–Ω–∞.
    #    –û–Ω–æ –≤—ã–±–µ—Ä–µ—Ç –≤—Å–µ –ø–∞—Ä—Ç–∏—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è:
    #    - –≤ –≥–æ–¥–∞—Ö –ü–û–ó–ñ–ï –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –≥–æ–¥–∞
    #    - –ò–õ–ò –≤ —Ç–æ–º –∂–µ –≥–æ–¥—É, –Ω–æ –≤ –º–µ—Å—è—Ü–µ, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–≤–µ–Ω –∏–ª–∏ –ü–û–ó–ñ–ï –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ.
    start_filter = (ds.field('year') > start_date.year) | \
                   ((ds.field('year') == start_date.year) & (ds.field('month') >= start_date.month))

    # 2. –°–æ–∑–¥–∞–µ–º —É—Å–ª–æ–≤–∏–µ –¥–ª—è –≤–µ—Ä—Ö–Ω–µ–π –≥—Ä–∞–Ω–∏—Ü—ã –¥–∏–∞–ø–∞–∑–æ–Ω–∞.
    #    –û–Ω–æ –≤—ã–±–µ—Ä–µ—Ç –≤—Å–µ –ø–∞—Ä—Ç–∏—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—Ö–æ–¥—è—Ç—Å—è:
    #    - –≤ –≥–æ–¥–∞—Ö –†–ê–ù–¨–®–ï –∫–æ–Ω–µ—á–Ω–æ–≥–æ –≥–æ–¥–∞
    #    - –ò–õ–ò –≤ —Ç–æ–º –∂–µ –≥–æ–¥—É, –Ω–æ –≤ –º–µ—Å—è—Ü–µ, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–≤–µ–Ω –∏–ª–∏ –†–ê–ù–¨–®–ï –∫–æ–Ω–µ—á–Ω–æ–≥–æ.
    end_filter = (ds.field('year') < end_date.year) | \
                 ((ds.field('year') == end_date.year) & (ds.field('month') <= end_date.month))

    # 3. –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–≤–∞ —É—Å–ª–æ–≤–∏—è —á–µ—Ä–µ–∑ –ª–æ–≥–∏—á–µ—Å–∫–æ–µ "–ò".
    #    –í –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é `filter_expr` –ø–æ–ø–∞–¥—É—Ç —Ç–æ–ª—å–∫–æ —Ç–µ –ø–∞—Ä—Ç–∏—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ
    #    —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—é—Ç –û–ë–ê —É—Å–ª–æ–≤–∏—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ.
    filter_expr = start_filter & end_filter

    # --- –ö–û–ù–ï–¶ –ü–†–ê–í–ò–õ–¨–ù–û–ì–û –ö–û–î–ê ---

    try:
        dataset = ds.dataset(data_path, format="parquet", partitioning=['year', 'month'])
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ PyArrow: {e}")
        print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–∞–ø–æ–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç 'year=YYYY/month=MM/'.")
        return pd.DataFrame()

    table = dataset.to_table(filter=filter_expr)

    if table.num_rows == 0:
        print("‚ö†Ô∏è  –î–∞–Ω–Ω—ã–µ –ø–æ —Ñ–∏–ª—å—Ç—Ä—É –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        return pd.DataFrame()

    df = table.to_pandas()

    if 'timestamp' not in df.columns and df.index.name == 'timestamp':
        df = df.reset_index()

    df['timestamp'] = pd.to_datetime(df['timestamp'])
    mask = (df['timestamp'] >= start_date) & (df['timestamp'] <= end_date)

    final_df = df.loc[mask]
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(final_df)} –∑–∞–ø–∏—Å–µ–π.")
    return final_df
