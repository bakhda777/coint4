import logging
import os

import numpy as np
import threading
import time
from pathlib import Path
import dask.dataframe as dd
import numpy as np
import pandas as pd  # type: ignore
import pyarrow as pa
import pyarrow.dataset as ds

from coint2.utils import empty_ddf, ensure_datetime_index
from coint2.utils.config import AppConfig
from coint2.utils.timing_utils import logged_time, time_block, log_progress

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
logger = logging.getLogger(__name__)


def _scan_parquet_files(path: Path | str, glob: str = "*.parquet", max_shards: int | None = None) -> ds.Dataset:
    """Build pyarrow dataset from parquet files under ``path``."""
    base = Path(path)
    files = sorted(base.rglob(glob))
    if max_shards is not None:
        files = files[:max_shards]
    if not files:
        return ds.dataset(pa.table({}))
    return ds.dataset([str(f) for f in files], format="parquet", partitioning="hive")


def _dir_mtime_hash(path: Path) -> float:
    """Return hash value based on modification times of ``.parquet`` files."""
    mtimes = [f.stat().st_mtime for f in path.rglob("*.parquet")]
    return max(mtimes) if mtimes else 0.0


class DataHandler:
    """Utility class for loading local parquet price files."""

    def __init__(self, cfg: AppConfig, autorefresh: bool = True) -> None:
        self.data_dir = Path(cfg.data_dir)
        self.cache_dir = self.data_dir.parent / ".cache"  # –î–æ–±–∞–≤–ª—è–µ–º cache_dir
        self.fill_limit_pct = cfg.backtest.fill_limit_pct
        self.max_shards = cfg.max_shards
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.autorefresh = autorefresh
        self._all_data_cache: dict[str, tuple[dd.DataFrame, float]] = {}
        self._freq: str | None = None
        self._lock = threading.Lock()
        self.lookback_days: int = cfg.pair_selection.lookback_days

    @property
    def freq(self) -> str | None:
        """Return detected time step of the loaded data."""
        with self._lock:
            return self._freq

    def clear_cache(self) -> None:
        """Clears the in-memory Dask DataFrame cache."""
        with self._lock:
            self._all_data_cache.clear()
            self._freq = None

    @logged_time("get_all_symbols")
    def get_all_symbols(self) -> list[str]:
        """Return list of symbols based on data in optimized parquet files."""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        optimized_dir = Path(self.data_dir.parent / "data_optimized")
        
        if optimized_dir.exists():
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö: {optimized_dir}")
            
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–¥–∏–Ω —Ñ–∞–π–ª –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ —Å–∏–º–≤–æ–ª–æ–≤
                # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π —Ñ–∞–π–ª parquet
                parquet_files = []
                for year_dir in optimized_dir.iterdir():
                    if not year_dir.is_dir() or not year_dir.name.startswith("year="):
                        continue
                    
                    for month_dir in year_dir.iterdir():
                        if not month_dir.is_dir() or not month_dir.name.startswith("month="):
                            continue
                            
                        for file in month_dir.glob("*.parquet"):
                            parquet_files.append(file)
                            break
                        
                        if parquet_files:
                            break
                    
                    if parquet_files:
                        break
                
                if not parquet_files:
                    logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ parquet —Ñ–∞–π–ª–æ–≤ –≤ {optimized_dir}")
                    return []
                
                # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–π —Ñ–∞–π–ª –∏ –ø–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
                import polars as pl
                df = pl.read_parquet(parquet_files[0])
                symbols = df["symbol"].unique().to_list()
                
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(symbols)} —Å–∏–º–≤–æ–ª–æ–≤ –≤ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ: {symbols[:10]}{'...' if len(symbols) > 10 else ''}")
                return sorted(symbols)
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
                # –ï—Å–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞, –ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∞—Ä—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –µ—Å–ª–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞
        if not self.data_dir.exists():
            logger.warning(f"Data directory does not exist: {self.data_dir}")
            return []

        symbols = []
        with time_block("scanning symbol directories"):
            for p in self.data_dir.iterdir():
                if not p.is_dir():
                    continue
                if p.name.startswith("symbol="):
                    symbols.append(p.name.replace("symbol=", ""))
                else:
                    symbols.append(p.name)
        
        logger.info(f"Found {len(symbols)} symbols in legacy structure: {symbols[:10]}{'...' if len(symbols) > 10 else ''}")
        return sorted(symbols)

    @logged_time("load_full_dataset")
    def _load_full_dataset(self, start_date: pd.Timestamp = None, end_date: pd.Timestamp = None, symbols: list[str] = None) -> dd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã parquet.
        
        Parameters
        ----------
        start_date : pd.Timestamp, optional
            –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        end_date : pd.Timestamp, optional
            –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        symbols : list[str], optional
            –°–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        
        Returns
        -------
        dd.DataFrame
            Dask DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            optimized_dir = Path(self.data_dir.parent / "data_optimized")
            data_path = optimized_dir if optimized_dir.exists() else self.data_dir
            
            logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–∞–Ω–Ω—ã—Ö: {data_path}")
            
            cols_to_load = ["timestamp", "symbol", "close"]
            filters = []
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ –¥–∞—Ç–µ, –µ—Å–ª–∏ –æ–Ω–∏ —É–∫–∞–∑–∞–Ω—ã
            if start_date is not None:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                start_ts = int(start_date.timestamp() * 1000)
                logger.info(f"–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—Ç –¥–∞—Ç—ã: {start_date} (timestamp: {start_ts})")
                filters.append(("timestamp", ">=", start_ts))
                
            if end_date is not None:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                end_ts = int(end_date.timestamp() * 1000)
                logger.info(f"–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–æ –¥–∞—Ç—ã: {end_date} (timestamp: {end_ts})")
                filters.append(("timestamp", "<=", end_ts))
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ —Å–∏–º–≤–æ–ª–∞–º, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã
            if symbols is not None and len(symbols) > 0:
                logger.info(f"–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Å–∏–º–≤–æ–ª–∞–º: {symbols[:10]}{'...' if len(symbols) > 10 else ''}")
                # –î–ª—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ —Å—Ç–æ–ª–±—Ü—É symbol
                if optimized_dir.exists():
                    filters.append(("symbol", "in", symbols))
            
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏: {filters if filters else '–±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–æ–≤'}")

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º dd.read_parquet —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            ddf = dd.read_parquet(
                data_path,
                engine="pyarrow",
                columns=cols_to_load,
                filters=filters if filters else None,
                gather_statistics=True,  # –í–∞–∂–Ω–æ –¥–ª—è –æ—Ç—Å–µ—á–µ–Ω–∏—è –ø–∞—Ä—Ç–∏—Ü–∏–π (partition pruning)
                schema_overrides={
                    # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º —Ç–∏–ø—ã –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –æ—à–∏–±–æ–∫ —Ç–∏–ø–æ–≤
                    "timestamp": np.int64,
                    "close": np.float64,
                    "symbol": str
                }
            )

            # –†–µ–ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä—É–µ–º –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –∏ –∫–µ—à–∏—Ä—É–µ–º –≤ –ø–∞–º—è—Ç–∏
            # npartitions –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–∏—Å—Ç–µ–º—ã
            num_partitions = os.cpu_count() * 2
            ddf = ddf.repartition(npartitions=num_partitions).persist()

            logger.info(f"Dask dataframe —Å–æ–∑–¥–∞–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –ø–∞–º—è—Ç–∏: {len(ddf.columns)} –∫–æ–ª–æ–Ω–æ–∫, {ddf.npartitions} –ø–∞—Ä—Ç–∏—Ü–∏–π.")

            return ddf

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π DataFrame –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            return empty_ddf()

    @logged_time("load_all_data_for_period")
    def load_all_data_for_period(self, lookback_days: int | None = None, symbols: list[str] = None) -> pd.DataFrame:
        """Load close prices for all symbols for the specified or configured lookback period.
        
        Parameters
        ----------
        lookback_days : int | None, optional
            Number of days to look back. If None, uses self.lookback_days.
        symbols : list[str], optional
            List of symbols to filter by. If None, loads data for all symbols.
            
        Returns
        -------
        pd.DataFrame
            DataFrame with close prices for all symbols.
        """
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–º–∫–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            days_back = lookback_days or self.lookback_days
            end_date = pd.Timestamp.now()
            start_date = end_date - pd.Timedelta(days=days_back)
            
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–µ—Ä–∏–æ–¥: {start_date} - {end_date} ({days_back} –¥–Ω–µ–π)")
            
            with time_block("loading filtered dataset"):
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –¥–∞—Ç–∞–º –∏ —Å–∏–º–≤–æ–ª–∞–º
                ddf = self._load_full_dataset(start_date=start_date, end_date=end_date, symbols=symbols)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ–π DataFrame
            if not ddf.columns.tolist():
                logger.warning("Empty dataset - no columns found")
                return pd.DataFrame()

            with time_block("computing dataset and preparing data"):
                # –ü—Ä–∏–≤–æ–¥–∏–º ddf['timestamp'] –∫ datetime –µ—Å–ª–∏ —ç—Ç–æ –µ—â–µ –Ω–µ —Å–¥–µ–ª–∞–Ω–æ
                if not pd.api.types.is_datetime64_any_dtype(ddf["timestamp"]):
                    ddf["timestamp"] = dd.to_datetime(ddf["timestamp"], unit="ms")

                # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                all_data = ddf.compute()
                logger.info(f"Computed dataset: {len(all_data):,} rows, {len(all_data.columns)} columns")
                
                if all_data.empty:
                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π DataFrame —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏ –∏ –∏–º–µ–Ω–µ–º –∏–Ω–¥–µ–∫—Å–∞
                    logger.warning("No data in specified time range")
                    return pd.DataFrame(columns=["timestamp", "symbol", "close"], index=pd.DatetimeIndex([], name="timestamp"))

                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º timestamp –≤ datetime
                all_data["timestamp"] = pd.to_datetime(all_data["timestamp"], unit="ms")

            # –ü–∏–≤–æ—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
            with time_block("pivoting data"):
                result = all_data.pivot_table(
                    index="timestamp", columns="symbol", values="close"
                )
                logger.info(f"Pivoted data: {len(result)} rows, {len(result.columns)} symbols")

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º missing data
            with time_block("handling missing data"):
                initial_symbols = len(result.columns)
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º fill_limit_pct
                if hasattr(self, 'fill_limit_pct'):
                    fill_limit = int(len(result) * self.fill_limit_pct)
                    result = result.fillna(method='ffill', limit=fill_limit)
                    result = result.fillna(method='bfill', limit=fill_limit)
                
                # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ª–±—Ü—ã —Å —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º NaN
                nan_threshold = 0.5  # Remove symbols with >50% NaN
                clean_result = result.dropna(axis=1, thresh=int(len(result) * (1 - nan_threshold)))
                
                removed_symbols = initial_symbols - len(clean_result.columns)
                if removed_symbols > 0:
                    logger.info(f"Removed {removed_symbols} symbols with >50% missing data")
                
                logger.info(f"Final result: {len(clean_result)} rows, {len(clean_result.columns)} symbols")

            return clean_result

        except Exception as e:
            logger.error(f"Error in load_all_data_for_period: {e}")
            return pd.DataFrame()

    def load_pair_data(
        self,
        symbol1: str,
        symbol2: str,
        start_date: pd.Timestamp,
        end_date: pd.Timestamp,
    ) -> pd.DataFrame:
        """Load and align data for two symbols within the given date range."""
        # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º, —á—Ç–æ –¥–∞—Ç—ã –≤ –Ω–∞–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (–±–µ–∑ timezone)
        if start_date.tzinfo is not None:
            logger.debug(f"–£–¥–∞–ª—è—é timezone –∏–∑ start_date: {start_date}")
            start_date = start_date.tz_localize(None)
        if end_date.tzinfo is not None:
            logger.debug(f"–£–¥–∞–ª—è—é timezone –∏–∑ end_date: {end_date}")
            end_date = end_date.tz_localize(None)
            
        logger.debug(
            f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–∞—Ä—ã {symbol1}-{symbol2} ({start_date} - {end_date})"
        )

        if not self.data_dir.exists():
            logger.warning(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–∞–Ω–Ω—ã—Ö {self.data_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            return pd.DataFrame()

        start_ms = int(start_date.timestamp() * 1000)
        end_ms = int(end_date.timestamp() * 1000)
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º partition-filters (symbol, year, month, day)
        filters = [
            ("symbol", "in", [symbol1, symbol2]),
            ("year", ">=", start_date.year),
            ("year", "<=", end_date.year),
            ("month", ">=", start_date.month if start_date.year == end_date.year else 1),
            ("month", "<=", end_date.month if start_date.year == end_date.year else 12),
            ("timestamp", ">=", start_ms),
            ("timestamp", "<=", end_ms),
        ]

        try:
            logger.debug("–ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É —Å partition-filters")
            ddf = dd.read_parquet(
                self.data_dir,
                engine="pyarrow",
                columns=["timestamp", "close", "symbol"],
                ignore_metadata_file=True,
                calculate_divisions=False,
                filters=filters,
                validate_schema=False,
            )
            pair_pdf = ddf.compute()
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å partition-filters: {str(e)}")
            # –û—Ç–≤–∞–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ ¬´—Å—ã—Ä–æ–µ¬ª —á—Ç–µ–Ω–∏–µ
            try:
                logger.debug("Fallback: –∑–∞–≥—Ä—É–∑–∫–∞ –±–µ–∑ partition-filters")
                basic_filters = [
                    ("symbol", "in", [symbol1, symbol2]),
                    ("timestamp", ">=", start_ms),
                    ("timestamp", "<=", end_ms),
                ]
                ddf = dd.read_parquet(
                    self.data_dir,
                    engine="pyarrow",
                    columns=["timestamp", "close", "symbol"],
                    ignore_metadata_file=True,
                    calculate_divisions=False,
                    filters=basic_filters,
                    validate_schema=False,
                )
                pair_pdf = ddf.compute()
            except Exception as e2:  # pragma: no cover - fallback rarely used
                logger.debug(f"Error loading pair data via Dask: {str(e2)}")
                try:
                    logger.debug("Fallback to pyarrow dataset scanning in load_pair_data")
                    dataset = _scan_parquet_files(self.data_dir, max_shards=self.max_shards)
                    arrow_filter = (
                        ds.field("symbol").isin([symbol1, symbol2])
                        & (ds.field("timestamp") >= pa.scalar(start_date))
                        & (ds.field("timestamp") <= pa.scalar(end_date))
                    )
                    table = dataset.to_table(
                        columns=["timestamp", "close", "symbol"],
                        filter=arrow_filter,
                    )
                    pair_pdf = table.to_pandas()
                except Exception as e3:  # pragma: no cover - fallback rarely used
                    logger.debug(f"Error loading pair data manually: {str(e3)}")
                    return pd.DataFrame()

        if pair_pdf.empty:
            logger.debug(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–∞—Ä—ã {symbol1}-{symbol2}")
            return pd.DataFrame()
        pair_pdf["timestamp"] = pd.to_datetime(pair_pdf["timestamp"], unit="ms").dt.tz_localize(None)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ timestamp
        if pair_pdf.duplicated(subset=["timestamp", "symbol"]).any():
            logger.debug(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã timestamp –¥–ª—è –ø–∞—Ä—ã {symbol1}-{symbol2}. –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã.")
            pair_pdf = pair_pdf.drop_duplicates(subset=["timestamp", "symbol"])

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —à–∏—Ä–æ–∫–∏–π —Ñ–æ—Ä–º–∞—Ç (timestamp x symbols)
        wide_df = pair_pdf.pivot_table(index="timestamp", columns="symbol", values="close")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
        if wide_df.empty or len(wide_df.columns) < 2:
            return pd.DataFrame()

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        freq_val = pd.infer_freq(wide_df.index)
        with self._lock:
            self._freq = freq_val
        if freq_val:
            wide_df = wide_df.asfreq(freq_val)

        # —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –ø–æ–¥—Ä—è–¥ –∏–¥—É—â–∏—Ö NA –ø–æ –ø—Ä–æ—Ü–µ–Ω—Ç—É
        limit = max(1, int(len(wide_df) * self.fill_limit_pct))

        wide_df = wide_df.ffill(limit=limit).bfill(limit=limit)


        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ —É–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NA
        if symbol1 in wide_df.columns and symbol2 in wide_df.columns:
            return wide_df[[symbol1, symbol2]].dropna()
        else:
            return pd.DataFrame()

    def load_and_normalize_data(
        self, start_date: pd.Timestamp, end_date: pd.Timestamp
    ) -> pd.DataFrame:
        """
        Load and normalize data for all symbols within the given date range.
        
        Parameters
        ----------
        start_date : pd.Timestamp
            –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        end_date : pd.Timestamp
            –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
            
        Returns
        -------
        pd.DataFrame
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π DataFrame —Å —Ü–µ–Ω–∞–º–∏ (–Ω–∞—á–∞–ª–æ = 100)
        """
        # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º, —á—Ç–æ –¥–∞—Ç—ã –≤ –Ω–∞–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (–±–µ–∑ timezone)
        if start_date.tzinfo is not None:
            logger.debug(f"–£–¥–∞–ª—è—é timezone –∏–∑ start_date: {start_date}")
            start_date = start_date.tz_localize(None)
        if end_date.tzinfo is not None:
            logger.debug(f"–£–¥–∞–ª—è—é timezone –∏–∑ end_date: {end_date}")
            end_date = end_date.tz_localize(None)
            
        logger.debug(f"–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–µ—Ä–∏–æ–¥ {start_date} - {end_date}")
        
        start_time = time.time()
        data_df = self.preload_all_data(start_date, end_date)
        elapsed_time = time.time() - start_time
        logger.info(f"–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥. –†–∞–∑–º–µ—Ä: {data_df.shape}")

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ü–µ–Ω
        if not data_df.empty:
            logger.debug(
                f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –¥–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {len(data_df.columns)}"
            )

            numeric_cols = [
                c
                for c in data_df.columns
                if pd.api.types.is_numeric_dtype(data_df[c]) and c != "timestamp"
            ]
            if numeric_cols:
                for col in numeric_cols:
                    series = data_df[col]
                    first_val = series.loc[series.first_valid_index()] if series.first_valid_index() is not None else pd.NA
                    if pd.isna(first_val) or first_val == 0:
                        data_df[col] = 0.0
                    else:
                        data_df[col] = 100 * series / first_val

            logger.debug(
                f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {len(data_df.columns)}"
            )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã—Ö —Å–µ—Ä–∏–π –∏ —Å–µ—Ä–∏–π —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–æ–ø—É—Å–∫–æ–≤
            valid_columns = []
            for column in data_df.columns:
                if pd.api.types.is_numeric_dtype(data_df[column]):
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω–æ—Å—Ç—å —Å–µ—Ä–∏–∏
                    if data_df[column].nunique() > 1:
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤ (–±–æ–ª–µ–µ 50%)
                        na_pct = data_df[column].isna().mean()
                        if na_pct < 0.5:
                            valid_columns.append(column)
                            
            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã
            if valid_columns:
                logger.debug(
                    f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(data_df.columns) - len(valid_columns)} "
                    "–∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã—Ö –∏–ª–∏ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö —Å–µ—Ä–∏–π"
                )
                data_df = data_df[valid_columns]

        return data_df

    def preload_all_data(
        self, start_date: pd.Timestamp, end_date: pd.Timestamp
    ) -> pd.DataFrame:
        """Loads and pivots all data for a given wide date range."""
        # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º, —á—Ç–æ –¥–∞—Ç—ã –≤ –Ω–∞–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (–±–µ–∑ timezone)
        if start_date.tzinfo is not None:
            logger.debug(f"–£–¥–∞–ª—è—é timezone –∏–∑ start_date –≤ preload_all_data: {start_date}")
            start_date = start_date.tz_localize(None)
        if end_date.tzinfo is not None:
            logger.debug(f"–£–¥–∞–ª—è—é timezone –∏–∑ end_date –≤ preload_all_data: {end_date}")
            end_date = end_date.tz_localize(None)
            
        logger.debug(f"–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–µ—Ä–∏–æ–¥ {start_date} - {end_date}")
            
        if not self.data_dir.exists():
            logger.warning(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–∞–Ω–Ω—ã—Ö {self.data_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            return pd.DataFrame()

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç—ã –≤ timestamp —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        start_ts = int(start_date.timestamp() * 1000)
        end_ts = int(end_date.timestamp() * 1000)
        logger.debug(f"–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ timestamp: {start_ts} - {end_ts}")
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º partition-filters (year, month, day)
        partition_filters = [
            ("year", ">=", start_date.year),
            ("year", "<=", end_date.year),
            ("month", ">=", start_date.month if start_date.year == end_date.year else 1),
            ("month", "<=", end_date.month if start_date.year == end_date.year else 12),
            ("timestamp", ">=", start_ts),
            ("timestamp", "<=", end_ts),
        ]

        try:
            logger.debug("–ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É —Å partition-filters")
            ddf = dd.read_parquet(
                self.data_dir,
                engine="pyarrow",
                columns=["timestamp", "close", "symbol"],
                ignore_metadata_file=True,
                calculate_divisions=False,
                filters=partition_filters,
                validate_schema=False,
            )

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ pandas –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            all_data = ddf.compute()
            logger.debug(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π –≤—Å–µ–≥–æ: {len(all_data)}")
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å partition-filters: {str(e)}")
            # –û—Ç–≤–∞–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ ¬´—Å—ã—Ä–æ–µ¬ª —á—Ç–µ–Ω–∏–µ
            try:
                # –î–ª—è –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ª—É—á—à–µ –∑–∞–≥—Ä—É–∂–∞—Ç—å –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∏ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ
                logger.debug("Fallback: –∑–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–æ–≤...")
                ddf = dd.read_parquet(
                    self.data_dir,
                    engine="pyarrow",
                    columns=["timestamp", "close", "symbol"],
                    ignore_metadata_file=True,
                    calculate_divisions=False,
                    validate_schema=False,
                )

                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ pandas –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                all_data = ddf.compute()
                logger.debug(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π –≤—Å–µ–≥–æ: {len(all_data)}")
            except Exception as e2:
                logger.error(f"Error loading data via Dask: {str(e2)}")

                try:
                    logger.debug("Fallback to pyarrow dataset scanning in preload_all_data")
                    dataset = _scan_parquet_files(self.data_dir, max_shards=self.max_shards)
                    table = dataset.to_table(columns=["timestamp", "close", "symbol"])
                    all_data = table.to_pandas()
                    if all_data.empty:
                        return pd.DataFrame()
                except Exception as e3:
                    logger.error(f"Error loading data manually: {str(e3)}")
                    return pd.DataFrame()

        logger.debug(f"–¢–∏–ø all_data['timestamp']: {all_data['timestamp'].dtype}")
        logger.debug(f"–ü—Ä–∏–º–µ—Ä—ã –∑–Ω–∞—á–µ–Ω–∏–π timestamp: {all_data['timestamp'].head(5).tolist()}")
        logger.debug(f"–¢–∏–ø timestamp: {all_data['timestamp'].dtype}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º timestamp –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø
        if np.issubdtype(all_data['timestamp'].dtype, np.datetime64):
            logger.debug('–ü—Ä–µ–æ–±—Ä–∞–∑—É—é timestamp –∏–∑ datetime64[ns] –≤ int/ms')
            all_data['timestamp'] = all_data['timestamp'].astype('int64') // 10**6
            logger.debug(f"–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–æ. –ü—Ä–∏–º–µ—Ä—ã: {all_data['timestamp'].head(5).tolist()}")
        elif all_data['timestamp'].dtype == 'object' or str(all_data['timestamp'].dtype).startswith('string') or 'str' in str(all_data['timestamp'].dtype).lower():
            logger.warning('–û–±–Ω–∞—Ä—É–∂–µ–Ω —Å—Ç—Ä–æ–∫–æ–≤—ã–π —Ç–∏–ø timestamp! –ü—Ä–µ–æ–±—Ä–∞–∑—É—é –≤ int64')
            # –ü—Ä–æ–±—É–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Å—Ç—Ä–æ–∫–∏ –≤ —á–∏—Å–ª–∞
            try:
                all_data['timestamp'] = pd.to_numeric(all_data['timestamp'], errors='coerce')
                # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è, –µ—Å–ª–∏ –æ–Ω–∏ –ø–æ—è–≤–∏–ª–∏—Å—å –ø—Ä–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏
                if all_data['timestamp'].isna().any():
                    logger.warning(f"–ü—Ä–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏ timestamp –ø–æ—è–≤–∏–ª–∏—Å—å NaN –∑–Ω–∞—á–µ–Ω–∏—è: {all_data['timestamp'].isna().sum()} –∏–∑ {len(all_data)}")
                    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ timestamp, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∏ –Ω–µ –º–æ–≥—É—Ç –±—ã—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã
                    all_data = all_data.dropna(subset=['timestamp'])
                logger.debug(f"–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–æ –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø. –ü—Ä–∏–º–µ—Ä—ã: {all_data['timestamp'].head(5).tolist()}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏ timestamp –∏–∑ —Å—Ç—Ä–æ–∫–∏ –≤ —á–∏—Å–ª–æ: {str(e)}")
        
        if all_data.empty:
            logger.warning(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {self.data_dir}")
            return pd.DataFrame()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –≤ pandas
        mask = (all_data["timestamp"] >= start_ts) & (all_data["timestamp"] <= end_ts)
        filtered_df = all_data[mask]
        logger.debug(f"–ó–∞–ø–∏—Å–µ–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏: {len(filtered_df)}")
        
        if filtered_df.empty:
            logger.warning(f"No data found between {start_date} and {end_date}")
            logger.debug(f"–î–æ—Å—Ç—É–ø–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –≤—Ä–µ–º–µ–Ω–∏: {pd.to_datetime(all_data['timestamp'].min(), unit='ms')} - {pd.to_datetime(all_data['timestamp'].max(), unit='ms')}")
            return pd.DataFrame()

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º timestamp –≤ datetime –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞
        filtered_df["timestamp"] = pd.to_datetime(filtered_df["timestamp"], unit="ms")
        
        if filtered_df.duplicated(subset=["timestamp", "symbol"]).any():
            wide_pdf = filtered_df.pivot_table(
                index="timestamp",
                columns="symbol",
                values="close",
                aggfunc="last",
            )
        else:
            wide_pdf = filtered_df.pivot(
                index="timestamp",
                columns="symbol",
                values="close",
            )

        if wide_pdf.empty:
            logger.debug(f"–ü—É—Å—Ç–æ–π pivot –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è")
            return pd.DataFrame()

        wide_pdf = ensure_datetime_index(wide_pdf)
        logger.debug(f"–ò—Ç–æ–≥–æ–≤—ã–µ –¥–∞–Ω–Ω—ã: {wide_pdf.shape}, –ø–µ—Ä–∏–æ–¥: {wide_pdf.index.min()} - {wide_pdf.index.max()}")

        freq_val = pd.infer_freq(wide_pdf.index)
        with self._lock:
            self._freq = freq_val
        if freq_val:
            wide_pdf = wide_pdf.asfreq(freq_val)

        return wide_pdf


# New function for optimized dataset loading
import pandas as pd
import pyarrow.dataset as ds


def load_master_dataset(data_path: str, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É—è Polars –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    –∏ —Å—Ç—Ä–æ–≥–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö.
    
    Parameters
    ----------
    data_path : str
        –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –¥–∞–Ω–Ω—ã–º–∏
    start_date : pd.Timestamp
        –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
    end_date : pd.Timestamp
        –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
        
    Returns
    -------
    pd.DataFrame
        DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥
    """
    print(f"‚öôÔ∏è  –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–µ—Ä–∏–æ–¥: {start_date.date()} -> {end_date.date()}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
    import os
    from pathlib import Path
    import polars as pl
    
    data_path_obj = Path(data_path)
    optimized_dir = Path(data_path_obj.parent / "data_optimized")
    
    if optimized_dir.exists() and os.listdir(optimized_dir):
        print(f"‚ÑπÔ∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö: {optimized_dir}")
        data_path = str(optimized_dir)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç—ã –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    start_ts = int(start_date.timestamp() * 1000)
    end_ts = int(end_date.timestamp() * 1000)
    
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Polars –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
        # –°–Ω–∞—á–∞–ª–∞ –Ω–∞–π–¥–µ–º –≤—Å–µ parquet —Ñ–∞–π–ª—ã –≤ –Ω—É–∂–Ω—ã—Ö –ø–∞—Ä—Ç–∏—Ü–∏—è—Ö
        parquet_files = []
        
        # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        if str(data_path) == str(optimized_dir):
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ year –∏ month
            for year_dir in Path(data_path).glob("year=*"):
                year = int(year_dir.name.split('=')[1])
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≥–æ–¥—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ç–æ—á–Ω–æ –Ω–µ –≤—Ö–æ–¥—è—Ç –≤ –¥–∏–∞–ø–∞–∑–æ–Ω
                if year < start_date.year or year > end_date.year:
                    continue
                    
                for month_dir in year_dir.glob("month=*"):
                    month = int(month_dir.name.split('=')[1])
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤—Ö–æ–¥–∏—Ç –ª–∏ –º–µ—Å—è—Ü –≤ –¥–∏–∞–ø–∞–∑–æ–Ω
                    if year == start_date.year and month < start_date.month:
                        continue
                    if year == end_date.year and month > end_date.month:
                        continue
                        
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ parquet —Ñ–∞–π–ª—ã –∏–∑ —ç—Ç–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                    for file in month_dir.glob("*.parquet"):
                        parquet_files.append(str(file))
        else:
            # –î–ª—è —Å—Ç–∞—Ä–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º glob
            for p in Path(data_path).glob("**/*.parquet"):
                parquet_files.append(str(p))
        
        if not parquet_files:
            print("‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω–æ parquet —Ñ–∞–π–ª–æ–≤ –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –ø—É—Ç–∏ –∏ —Ñ–∏–ª—å—Ç—Ä–∞–º.")
            return pd.DataFrame()
            
        print(f"üìÇ –ù–∞–π–¥–µ–Ω–æ {len(parquet_files)} parquet —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é Polars
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º LazyFrame –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        ldf = pl.scan_parquet(parquet_files)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–µ–º—É –¥–∞–Ω–Ω—ã—Ö
        print(f"üìä –°—Ö–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö: {ldf.schema}")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ timestamp
        filtered_ldf = ldf.filter(
            (pl.col("timestamp") >= start_ts) & 
            (pl.col("timestamp") <= end_ts)
        )
        
        # –í—ã–±–∏—Ä–∞–µ–º –Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –∏ —Å–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        result = filtered_ldf.select(
            "timestamp", "symbol", "close"
        ).collect()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if result.height == 0:
            print("‚ö†Ô∏è  –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
            return pd.DataFrame()
            
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {result.height} –∑–∞–ø–∏—Å–µ–π —Å –ø–æ–º–æ—â—å—é Polars.")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º timestamp –≤ datetime –¥–ª—è pandas
        result = result.with_columns(
            pl.col("timestamp").cast(pl.Int64).alias("timestamp_ms"),
            pl.col("timestamp").cast(pl.Datetime).alias("timestamp")
        )
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ pandas DataFrame
        pandas_df = result.to_pandas()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
        print(f"üìä –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –≤ pandas: {pandas_df.dtypes}")
        
        return pandas_df
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é Polars: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()
