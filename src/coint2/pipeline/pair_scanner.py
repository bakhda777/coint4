import logging

import dask
import pandas as pd
from dask import delayed
# from statsmodels.tsa.stattools import coint  # –ó–∞–º–µ–Ω–µ–Ω–æ –Ω–∞ fast_coint

from coint2.core import math_utils
from coint2.core.fast_coint import fast_coint
from coint2.utils.config import AppConfig
from coint2.utils.timing_utils import logged_time, time_block, ProgressTracker

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
logger = logging.getLogger(__name__)


@delayed
def _test_pair_for_tradability(
    handler,
    symbol1: str,
    symbol2: str,
    start_date: pd.Timestamp,
    end_date: pd.Timestamp,
    min_half_life: float,
    max_half_life: float,
    min_crossings: int,
) -> tuple[str, str] | None:
    """Lazy tradability filter for a pair."""
    # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º, —á—Ç–æ –¥–∞—Ç—ã –≤ –Ω–∞–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (–±–µ–∑ timezone)
    if start_date.tzinfo is not None:
        start_date = start_date.tz_localize(None)
    if end_date.tzinfo is not None:
        end_date = end_date.tz_localize(None)
    
    logger.debug(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ tradability –¥–ª—è –ø–∞—Ä—ã {symbol1}-{symbol2} ({start_date} - {end_date})")
    
    pair_data = handler.load_pair_data(symbol1, symbol2, start_date, end_date)
    if pair_data.empty or len(pair_data.columns) < 2:
        logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ç–æ–ª–±—Ü–æ–≤")
        return None

    logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: –ø–æ–ª—É—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä–æ–º {len(pair_data)}")
    
    y = pair_data[symbol1]
    x = pair_data[symbol2]
    beta = y.cov(x) / x.var()
    spread = y - beta * x

    spread_np = spread.to_numpy()
    half_life = math_utils.half_life_numba(spread_np)
    logger.debug(
        f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: half_life = {half_life:.2f} "
        f"(—Ç—Ä–µ–±—É–µ—Ç—Å—è {min_half_life:.2f}-{max_half_life:.2f})"
    )
    
    if half_life < min_half_life or half_life > max_half_life:
        logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: –æ—Ç–∫–ª–æ–Ω–µ–Ω–∞ –ø–æ half_life")
        return None

    crossings = math_utils.mean_crossings_numba(spread_np)
    logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: mean_crossings = {crossings} (—Ç—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω. {min_crossings})")
    
    if crossings < min_crossings:
        logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: –æ—Ç–∫–ª–æ–Ω–µ–Ω–∞ –ø–æ mean_crossings")
        return None

    logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: –ø—Ä–æ—à–ª–∞ —Ñ–∏–ª—å—Ç—Ä tradability")
    return symbol1, symbol2


def _coint_test(series1: pd.Series, series2: pd.Series) -> float:
    """Run fast cointegration test and return p-value."""
    _score, pvalue, _ = fast_coint(series1, series2, trend='n')
    return pvalue


@delayed
def _test_pair_for_coint(
    handler,
    symbol1: str,
    symbol2: str,
    start_date: pd.Timestamp,
    end_date: pd.Timestamp,
    p_value_threshold: float,
) -> tuple[str, str, float, float, float] | None:
    """Lazy test for a single pair using provided handler and dates."""
    # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º, —á—Ç–æ –¥–∞—Ç—ã –≤ –Ω–∞–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (–±–µ–∑ timezone)
    if start_date.tzinfo is not None:
        start_date = start_date.tz_localize(None)
    if end_date.tzinfo is not None:
        end_date = end_date.tz_localize(None)
    
    logger.debug(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –¥–ª—è –ø–∞—Ä—ã {symbol1}-{symbol2} ({start_date} - {end_date})")
    
    pair_data = handler.load_pair_data(symbol1, symbol2, start_date, end_date)
    if pair_data.empty or len(pair_data.columns) < 2:
        logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è –∫–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏")
        return None

    logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: –ø–æ–ª—É—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä–æ–º {len(pair_data)} –¥–ª—è —Ç–µ—Å—Ç–∞ –∫–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –∏ –ª–æ–≥–∏—Ä—É–µ–º
    if (
        not pd.api.types.is_numeric_dtype(pair_data[symbol1])
        or not pd.api.types.is_numeric_dtype(pair_data[symbol2])
    ):
        logger.warning(
            "–ü–∞—Ä–∞ %s-%s: –¥–∞–Ω–Ω—ã–µ –Ω–µ —á–∏—Å–ª–æ–≤—ã–µ: %s, %s",
            symbol1,
            symbol2,
            pair_data[symbol1].dtype,
            pair_data[symbol2].dtype,
        )
        return None
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ NaN, –µ—Å–ª–∏ –±–æ–ª–µ–µ 10% - –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º –ø–∞—Ä—É
    nan_ratio1 = pair_data[symbol1].isna().mean()
    nan_ratio2 = pair_data[symbol2].isna().mean()
    if nan_ratio1 > 0.1 or nan_ratio2 > 0.1:
        logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ NaN ({nan_ratio1:.2f}, {nan_ratio2:.2f})")
        return None
    
    # –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ç NaN –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø–æ –æ–±–æ–∏–º —Å—Ç–æ–ª–±—Ü–∞–º, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é
    clean_data = pair_data[[symbol1, symbol2]].dropna()
    pvalue = _coint_test(clean_data[symbol1], clean_data[symbol2])
    logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: p-value = {pvalue:.4f} (—Ç—Ä–µ–±—É–µ—Ç—Å—è < {p_value_threshold:.4f})")
    
    if pvalue >= p_value_threshold:
        logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: –æ—Ç–∫–ª–æ–Ω–µ–Ω–∞ –ø–æ p-value –∫–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏")
        return None

    y = pair_data[symbol1]
    x = pair_data[symbol2]
    beta = y.cov(x) / x.var()
    spread = y - beta * x
    mean = spread.mean()
    std = spread.std()
    
    logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—à–ª–∞ —Ñ–∏–ª—å—Ç—Ä –∫–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏")
    return symbol1, symbol2, beta, mean, std


@logged_time("find_cointegrated_pairs")
def find_cointegrated_pairs(
    handler,
    start_date: pd.Timestamp,
    end_date: pd.Timestamp,
    cfg: AppConfig,
) -> list[tuple[str, str, float, float, float]]:
    """Find cointegrated pairs using SSD pre-filtering."""
    
    # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º, —á—Ç–æ –¥–∞—Ç—ã –≤ –Ω–∞–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (–±–µ–∑ timezone)
    if start_date.tzinfo is not None:
        logger.debug(f"–£–¥–∞–ª—è—é timezone –∏–∑ start_date: {start_date}")
        start_date = start_date.tz_localize(None)
    if end_date.tzinfo is not None:
        logger.debug(f"–£–¥–∞–ª—è—é timezone –∏–∑ end_date: {end_date}")
        end_date = end_date.tz_localize(None)

    logger.info(f"–ü–æ–∏—Å–∫ –∫–æ–∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä –≤ –ø–µ—Ä–∏–æ–¥–µ {start_date} - {end_date}")
    p_value_threshold = cfg.pair_selection.coint_pvalue_threshold
    logger.info(
        "–ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: p_value < %s, half_life = %s-%s –¥–Ω–µ–π, min_crossings = %s",
        p_value_threshold,
        cfg.pair_selection.min_half_life_days,
        cfg.pair_selection.max_half_life_days,
        cfg.pair_selection.min_mean_crossings,
    )

    # Stage 1: SSD pre-filter
    logger.info("üîç –≠–¢–ê–ü 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ SSD-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è")
    with time_block("loading and normalizing data for SSD"):
        normalized = handler.load_and_normalize_data(start_date, end_date)
        if normalized.empty or len(normalized.columns) < 2:
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–∏–º–≤–æ–ª–æ–≤")
            return []
        
        logger.info(
            f"–ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è {len(normalized.columns)} —Å–∏–º–≤–æ–ª–æ–≤ –∑–∞ –ø–µ—Ä–∏–æ–¥ "
            f"{normalized.index[0]} - {normalized.index[-1]}"
        )

    # Calculate theoretical number of pairs
    num_symbols = len(normalized.columns)
    theoretical_pairs = (num_symbols * (num_symbols - 1)) // 2
    logger.info(f"–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ —á–∏—Å–ª–æ –ø–∞—Ä: {theoretical_pairs:,} –∏–∑ {num_symbols} —Å–∏–º–≤–æ–ª–æ–≤")

    with time_block("SSD computation"):
        logger.info(
            f"–í—ã–ø–æ–ª–Ω—è—é SSD-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è (–≤—Å–µ–≥–æ {theoretical_pairs:,} –ø–∞—Ä)"
        )
        ssd = math_utils.calculate_ssd(
            normalized,
            top_k=None,
        )
        top_pairs = ssd.index.tolist()
        logger.info(f"SSD –æ—Ç–æ–±—Ä–∞–ª {len(top_pairs):,} –∏–∑ {theoretical_pairs:,} –ø–∞—Ä ({len(top_pairs)/theoretical_pairs*100:.1f}%)")

    # Stage 2: tradability filter
    logger.info("üîç –≠–¢–ê–ü 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ tradability (half-life, mean crossings)")
    with time_block("tradability testing"):
        lazy_tradable = []
        for s1, s2 in top_pairs:
            task = _test_pair_for_tradability(
                handler,
                s1,
                s2,
                start_date,
                end_date,
                cfg.pair_selection.min_half_life_days,
                cfg.pair_selection.max_half_life_days,
                cfg.pair_selection.min_mean_crossings,
            )
            lazy_tradable.append(task)

        logger.info(f"–ó–∞–ø—É—Å–∫–∞—é –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É tradability –¥–ª—è {len(lazy_tradable):,} –ø–∞—Ä")
        tradable_results = dask.compute(*lazy_tradable, scheduler="threads")
        tradable_pairs = [p for p in tradable_results if p is not None]
        
        logger.info(f"–ü—Ä–æ—à–ª–∏ —Ñ–∏–ª—å—Ç—Ä tradability: {len(tradable_pairs):,} –∏–∑ {len(top_pairs):,} –ø–∞—Ä ({len(tradable_pairs)/len(top_pairs)*100:.1f}%)")

    # Stage 3: cointegration filter –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    logger.info("üîç –≠–¢–ê–ü 3: –¢–µ—Å—Ç—ã –∫–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏")
    with time_block("cointegration and metrics testing"):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–∞—Ä, –ø—Ä–æ—à–µ–¥—à–∏—Ö tradability —Ñ–∏–ª—å—Ç—Ä
        all_pairs_data = {}
        max_pairs_in_memory = 1000  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –ø–∞–º—è—Ç–∏
        
        if len(tradable_pairs) > max_pairs_in_memory:
            logger.warning(f"–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø–∞—Ä ({len(tradable_pairs)}), –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ {max_pairs_in_memory} –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –ø–∞–º—è—Ç–∏")
            tradable_pairs = tradable_pairs[:max_pairs_in_memory]
            
        for s1, s2 in tradable_pairs:
            pair_data = handler.load_pair_data(s1, s2, start_date, end_date).dropna()
            if not pair_data.empty and len(pair_data.columns) >= 2:
                all_pairs_data[(s1, s2)] = pair_data
        
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è {len(all_pairs_data)} –ø–∞—Ä")
        
        # –°–æ–∑–¥–∞–µ–º DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –≤—Å–µ—Ö –ø–∞—Ä –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ filter_pairs_by_coint_and_half_life
        if not all_pairs_data:
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–π –ø–∞—Ä—ã")
            return []
            
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –≤ –æ–¥–∏–Ω DataFrame
        all_symbols = set()
        for s1, s2 in all_pairs_data.keys():
            all_symbols.add(s1)
            all_symbols.add(s2)
            
        # –°–æ–∑–¥–∞–µ–º DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤
        price_df = pd.DataFrame()
        for pair, data in all_pairs_data.items():
            s1, s2 = pair
            if s1 not in price_df.columns:
                price_df[s1] = data[s1]
            if s2 not in price_df.columns:
                price_df[s2] = data[s2]
                
        logger.info(f"–°–æ–∑–¥–∞–Ω DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è {len(price_df.columns)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        from coint2.pipeline.filters import filter_pairs_by_coint_and_half_life
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é
        final_pairs = filter_pairs_by_coint_and_half_life(
            pairs=tradable_pairs,
            price_df=price_df,
            pvalue_threshold=p_value_threshold,
            min_half_life=cfg.pair_selection.min_half_life_days,
            max_half_life=cfg.pair_selection.max_half_life_days,
            min_mean_crossings=cfg.pair_selection.min_mean_crossings,
            max_hurst_exponent=getattr(cfg.pair_selection, 'max_hurst_exponent', 0.5),
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã commission_pct –∏ slippage_pct —É–¥–∞–ª–µ–Ω—ã - –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è
        )
        success_rate = (len(final_pairs)/len(tradable_pairs)*100) if len(tradable_pairs) > 0 else 0
        logger.info(f"–ü—Ä–æ—à–ª–∏ –≤—Å–µ —Ñ–∏–ª—å—Ç—Ä—ã: {len(final_pairs):,} –∏–∑ {len(tradable_pairs):,} –ø–∞—Ä ({success_rate:.1f}%)")
        logger.info(
            f"–§–∏–ª—å—Ç—Ä—ã: p-value < {p_value_threshold}, half-life = {cfg.pair_selection.min_half_life_days}-{cfg.pair_selection.max_half_life_days}, "
            f"mean_crossings >= {cfg.pair_selection.min_mean_crossings}"
        )

    
    # Summary
    if not final_pairs:
        logger.warning("‚ö†Ô∏è  –ù–ï –ù–ê–ô–î–ï–ù–û –Ω–∏ –æ–¥–Ω–æ–π –∫–æ–∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–∞—Ä—ã. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        logger.warning("  ‚Ä¢ –û—Å–ª–∞–±–∏—Ç—å pvalue_threshold (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–æ 0.10)")
        logger.warning("  ‚Ä¢ –£–≤–µ–ª–∏—á–∏—Ç—å max_half_life_days")
        logger.warning("  ‚Ä¢ –£–º–µ–Ω—å—à–∏—Ç—å min_mean_crossings")
        logger.warning("  ‚Ä¢ –£–≤–µ–ª–∏—á–∏—Ç—å ssd_top_n")
    else:
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø–∞—Ä—ã –ø–æ p-value –∫–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏ –±–µ—Ä–µ–º —Ç–æ–ø-N
        pvalue_top_n = cfg.pair_selection.pvalue_top_n
        final_pairs = sorted(final_pairs, key=lambda x: x[5].get('pvalue', 1.0))[:pvalue_top_n]
        logger.info(
            f"‚úÖ –ò–¢–û–ì–û –Ω–∞–π–¥–µ–Ω–æ {len(final_pairs)} –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–∞—Ä (–¢–æ–ø-{pvalue_top_n} –ø–æ p-value):"
        )
        for i, (s1, s2, beta, mean, std, metrics) in enumerate(final_pairs[:5], 1):
            logger.info(f"  {i}. {s1}-{s2}: beta={beta:.4f}, mean={mean:.4f}, std={std:.4f}, "  
                      f"half_life={metrics['half_life']:.1f}, corr={metrics['correlation']:.2f}, "  
                      f"mean_crossings={metrics['mean_crossings']}")
        if len(final_pairs) > 5:
            logger.info(f"  ... –∏ –µ—â–µ {len(final_pairs) - 5} –ø–∞—Ä")
            
    return final_pairs
