import logging

import dask
import pandas as pd
from dask import delayed
# from statsmodels.tsa.stattools import coint  # –ó–∞–º–µ–Ω–µ–Ω–æ –Ω–∞ fast_coint

from coint2.core import math_utils
from coint2.core.fast_coint import fast_coint
from coint2.utils.config import AppConfig
from coint2.utils.timing_utils import logged_time, time_block, ProgressTracker

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
logger = logging.getLogger(__name__)


@delayed
def _test_pair_for_tradability(
    handler,
    symbol1: str,
    symbol2: str,
    start_date: pd.Timestamp,
    end_date: pd.Timestamp,
    min_half_life: float,
    max_half_life: float,
    min_crossings: int,
) -> tuple[str, str] | None:
    """Lazy tradability filter for a pair."""
    # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º, —á—Ç–æ –¥–∞—Ç—ã –≤ –Ω–∞–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (–±–µ–∑ timezone)
    if start_date.tzinfo is not None:
        start_date = start_date.tz_localize(None)
    if end_date.tzinfo is not None:
        end_date = end_date.tz_localize(None)
    
    logger.debug(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ tradability –¥–ª—è –ø–∞—Ä—ã {symbol1}-{symbol2} ({start_date} - {end_date})")
    
    pair_data = handler.load_pair_data(symbol1, symbol2, start_date, end_date)
    if pair_data.empty or len(pair_data.columns) < 2:
        logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ç–æ–ª–±—Ü–æ–≤")
        return None

    logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: –ø–æ–ª—É—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä–æ–º {len(pair_data)}")
    
    y = pair_data[symbol1]
    x = pair_data[symbol2]
    beta = y.cov(x) / x.var()
    spread = y - beta * x

    spread_np = spread.to_numpy()
    half_life = math_utils.half_life_numba(spread_np)
    logger.debug(
        f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: half_life = {half_life:.2f} "
        f"(—Ç—Ä–µ–±—É–µ—Ç—Å—è {min_half_life:.2f}-{max_half_life:.2f})"
    )
    
    if half_life < min_half_life or half_life > max_half_life:
        logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: –æ—Ç–∫–ª–æ–Ω–µ–Ω–∞ –ø–æ half_life")
        return None

    crossings = math_utils.mean_crossings_numba(spread_np)
    logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: mean_crossings = {crossings} (—Ç—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω. {min_crossings})")
    
    if crossings < min_crossings:
        logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: –æ—Ç–∫–ª–æ–Ω–µ–Ω–∞ –ø–æ mean_crossings")
        return None

    logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: –ø—Ä–æ—à–ª–∞ —Ñ–∏–ª—å—Ç—Ä tradability")
    return symbol1, symbol2


def _coint_test(series1: pd.Series, series2: pd.Series) -> float:
    """Run fast cointegration test and return p-value."""
    _score, pvalue, _ = fast_coint(series1, series2, trend='n')
    return pvalue


@delayed
def _test_pair_for_coint(
    handler,
    symbol1: str,
    symbol2: str,
    start_date: pd.Timestamp,
    end_date: pd.Timestamp,
    p_value_threshold: float,
) -> tuple[str, str, float, float, float] | None:
    """Lazy test for a single pair using provided handler and dates."""
    # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º, —á—Ç–æ –¥–∞—Ç—ã –≤ –Ω–∞–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (–±–µ–∑ timezone)
    if start_date.tzinfo is not None:
        start_date = start_date.tz_localize(None)
    if end_date.tzinfo is not None:
        end_date = end_date.tz_localize(None)
    
    logger.debug(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –¥–ª—è –ø–∞—Ä—ã {symbol1}-{symbol2} ({start_date} - {end_date})")
    
    pair_data = handler.load_pair_data(symbol1, symbol2, start_date, end_date)
    if pair_data.empty or len(pair_data.columns) < 2:
        logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ç–æ–ª–±—Ü–æ–≤ –¥–ª—è –∫–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏")
        return None

    logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: –ø–æ–ª—É—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ä–æ–º {len(pair_data)} –¥–ª—è —Ç–µ—Å—Ç–∞ –∫–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö –∏ –ª–æ–≥–∏—Ä—É–µ–º
    if (
        not pd.api.types.is_numeric_dtype(pair_data[symbol1])
        or not pd.api.types.is_numeric_dtype(pair_data[symbol2])
    ):
        logger.warning(
            "–ü–∞—Ä–∞ %s-%s: –¥–∞–Ω–Ω—ã–µ –Ω–µ —á–∏—Å–ª–æ–≤—ã–µ: %s, %s",
            symbol1,
            symbol2,
            pair_data[symbol1].dtype,
            pair_data[symbol2].dtype,
        )
        return None
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ NaN, –µ—Å–ª–∏ –±–æ–ª–µ–µ 10% - –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º –ø–∞—Ä—É
    nan_ratio1 = pair_data[symbol1].isna().mean()
    nan_ratio2 = pair_data[symbol2].isna().mean()
    if nan_ratio1 > 0.1 or nan_ratio2 > 0.1:
        logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ NaN ({nan_ratio1:.2f}, {nan_ratio2:.2f})")
        return None
    
    # –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ç NaN –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –ø–æ –æ–±–æ–∏–º —Å—Ç–æ–ª–±—Ü–∞–º, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é
    clean_data = pair_data[[symbol1, symbol2]].dropna()
    pvalue = _coint_test(clean_data[symbol1], clean_data[symbol2])
    logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: p-value = {pvalue:.4f} (—Ç—Ä–µ–±—É–µ—Ç—Å—è < {p_value_threshold:.4f})")
    
    if pvalue >= p_value_threshold:
        logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: –æ—Ç–∫–ª–æ–Ω–µ–Ω–∞ –ø–æ p-value –∫–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏")
        return None

    y = pair_data[symbol1]
    x = pair_data[symbol2]
    beta = y.cov(x) / x.var()
    spread = y - beta * x
    mean = spread.mean()
    std = spread.std()
    
    logger.debug(f"–ü–∞—Ä–∞ {symbol1}-{symbol2}: —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—à–ª–∞ —Ñ–∏–ª—å—Ç—Ä –∫–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏")
    return symbol1, symbol2, beta, mean, std


@logged_time("find_cointegrated_pairs")
def find_cointegrated_pairs(
    handler,
    start_date: pd.Timestamp,
    end_date: pd.Timestamp,
    cfg: AppConfig,
) -> list[tuple[str, str, float, float, float]]:
    """Find cointegrated pairs using SSD pre-filtering."""
    
    # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º, —á—Ç–æ –¥–∞—Ç—ã –≤ –Ω–∞–∏–≤–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (–±–µ–∑ timezone)
    if start_date.tzinfo is not None:
        logger.debug(f"–£–¥–∞–ª—è—é timezone –∏–∑ start_date: {start_date}")
        start_date = start_date.tz_localize(None)
    if end_date.tzinfo is not None:
        logger.debug(f"–£–¥–∞–ª—è—é timezone –∏–∑ end_date: {end_date}")
        end_date = end_date.tz_localize(None)

    logger.info(f"–ü–æ–∏—Å–∫ –∫–æ–∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä –≤ –ø–µ—Ä–∏–æ–¥–µ {start_date} - {end_date}")
    p_value_threshold = cfg.pair_selection.coint_pvalue_threshold
    logger.info(
        "–ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: p_value < %s, half_life = %s-%s –¥–Ω–µ–π, min_crossings = %s",
        p_value_threshold,
        cfg.pair_selection.min_half_life_days,
        cfg.pair_selection.max_half_life_days,
        cfg.pair_selection.min_mean_crossings,
    )

    # Stage 1: SSD pre-filter
    logger.info("üîç –≠–¢–ê–ü 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ SSD-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è")
    with time_block("loading and normalizing data for SSD"):
        normalized = handler.load_and_normalize_data(start_date, end_date)
        if normalized.empty or len(normalized.columns) < 2:
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–∏–º–≤–æ–ª–æ–≤")
            return []
        
        logger.info(
            f"–ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è {len(normalized.columns)} —Å–∏–º–≤–æ–ª–æ–≤ –∑–∞ –ø–µ—Ä–∏–æ–¥ "
            f"{normalized.index[0]} - {normalized.index[-1]}"
        )

    # Calculate theoretical number of pairs
    num_symbols = len(normalized.columns)
    theoretical_pairs = (num_symbols * (num_symbols - 1)) // 2
    logger.info(f"–¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ —á–∏—Å–ª–æ –ø–∞—Ä: {theoretical_pairs:,} –∏–∑ {num_symbols} —Å–∏–º–≤–æ–ª–æ–≤")

    with time_block("SSD computation"):
        logger.info(f"–í—ã–ø–æ–ª–Ω—è—é SSD-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –¥–ª—è —Ç–æ–ø-{cfg.pair_selection.ssd_top_n} –ø–∞—Ä")
        ssd = math_utils.calculate_ssd(
            normalized,
            top_k=cfg.pair_selection.ssd_top_n,
        )
        top_pairs = ssd.index.tolist()
        logger.info(f"SSD –æ—Ç–æ–±—Ä–∞–ª {len(top_pairs):,} –∏–∑ {theoretical_pairs:,} –ø–∞—Ä ({len(top_pairs)/theoretical_pairs*100:.1f}%)")

    # Stage 2: tradability filter
    logger.info("üîç –≠–¢–ê–ü 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ tradability (half-life, mean crossings)")
    with time_block("tradability testing"):
        lazy_tradable = []
        for s1, s2 in top_pairs:
            task = _test_pair_for_tradability(
                handler,
                s1,
                s2,
                start_date,
                end_date,
                cfg.pair_selection.min_half_life_days,
                cfg.pair_selection.max_half_life_days,
                cfg.pair_selection.min_mean_crossings,
            )
            lazy_tradable.append(task)

        logger.info(f"–ó–∞–ø—É—Å–∫–∞—é –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É tradability –¥–ª—è {len(lazy_tradable):,} –ø–∞—Ä")
        tradable_results = dask.compute(*lazy_tradable, scheduler="threads")
        tradable_pairs = [p for p in tradable_results if p is not None]
        
        logger.info(f"–ü—Ä–æ—à–ª–∏ —Ñ–∏–ª—å—Ç—Ä tradability: {len(tradable_pairs):,} –∏–∑ {len(top_pairs):,} –ø–∞—Ä ({len(tradable_pairs)/len(top_pairs)*100:.1f}%)")

    # Stage 3: cointegration filter
    logger.info("üîç –≠–¢–ê–ü 3: –¢–µ—Å—Ç—ã –∫–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ (Engle-Granger)")
    with time_block("cointegration testing"):
        lazy_results = []
        for s1, s2 in tradable_pairs:
            task = _test_pair_for_coint(
                handler,
                s1,
                s2,
                start_date,
                end_date,
                p_value_threshold,
            )
            lazy_results.append(task)

        logger.info(f"–ó–∞–ø—É—Å–∫–∞—é –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã –∫–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –¥–ª—è {len(lazy_results):,} –ø–∞—Ä")
        results = dask.compute(*lazy_results, scheduler="threads")
        final_pairs = [r for r in results if r is not None]
        
        success_rate = (len(final_pairs)/len(tradable_pairs)*100) if len(tradable_pairs) > 0 else 0
        logger.info(f"–ü—Ä–æ—à–ª–∏ —Ñ–∏–ª—å—Ç—Ä –∫–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏: {len(final_pairs):,} –∏–∑ {len(tradable_pairs):,} –ø–∞—Ä ({success_rate:.1f}%)")
    
    # Summary
    if not final_pairs:
        logger.warning("‚ö†Ô∏è  –ù–ï –ù–ê–ô–î–ï–ù–û –Ω–∏ –æ–¥–Ω–æ–π –∫–æ–∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–∞—Ä—ã. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        logger.warning("  ‚Ä¢ –û—Å–ª–∞–±–∏—Ç—å pvalue_threshold (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–æ 0.10)")
        logger.warning("  ‚Ä¢ –£–≤–µ–ª–∏—á–∏—Ç—å max_half_life_days")
        logger.warning("  ‚Ä¢ –£–º–µ–Ω—å—à–∏—Ç—å min_mean_crossings")
        logger.warning("  ‚Ä¢ –£–≤–µ–ª–∏—á–∏—Ç—å ssd_top_n")
    else:
        logger.info(f"‚úÖ –ò–¢–û–ì–û –Ω–∞–π–¥–µ–Ω–æ {len(final_pairs)} –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–∞—Ä:")
        for i, (s1, s2, beta, mean, std) in enumerate(final_pairs[:5], 1):
            logger.info(f"  {i}. {s1}-{s2}: beta={beta:.4f}, mean={mean:.4f}, std={std:.4f}")
        if len(final_pairs) > 5:
            logger.info(f"  ... –∏ –µ—â–µ {len(final_pairs) - 5} –ø–∞—Ä")
            
    return final_pairs
