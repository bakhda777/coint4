"""Walk-forward analysis orchestrator."""

from __future__ import annotations

import pandas as pd
from pathlib import Path

from coint2.core import math_utils, performance
# Import directly from the file path rather than the module
from src.coint2.core.data_loader import DataHandler, load_master_dataset
from src.coint2.core.normalization_improvements import preprocess_and_normalize_data
from coint2.engine.backtest_engine import PairBacktester
from coint2.utils.config import AppConfig
from coint2.utils.logging_utils import get_logger
from coint2.utils.timing_utils import logged_time, time_block, ProgressTracker
from coint2.utils.visualization import (
    create_performance_report, 
    format_metrics_summary, 
    calculate_extended_metrics
)


@logged_time("run_walk_forward_analysis")
def run_walk_forward(cfg: AppConfig) -> dict[str, float]:
    """Run walk-forward analysis and return aggregated performance metrics."""
    logger = get_logger("walk_forward")

    # Setup and initial data loading
    with time_block("initializing data handler"):
        handler = DataHandler(cfg)
        handler.clear_cache()

    start_date = pd.to_datetime(cfg.walk_forward.start_date)
    end_date = pd.to_datetime(cfg.walk_forward.end_date)
    # –î–ª—è –Ω–æ–≤–æ–π –ª–æ–≥–∏–∫–∏: –Ω—É–∂–Ω—ã –¥–∞–Ω–Ω—ã–µ –Ω–∞ training_period_days —Ä–∞–Ω—å—à–µ start_date
    full_range_start = start_date - pd.Timedelta(days=cfg.walk_forward.training_period_days)
    
    logger.info(f"üìÖ –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: start_date = –Ω–∞—á–∞–ª–æ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    logger.info(f"üìä –î–∞–Ω–Ω—ã–µ –Ω—É–∂–Ω—ã —Å: {full_range_start.date()} (–¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –ø–µ—Ä–≤–æ–≥–æ —à–∞–≥–∞)")
    
    logger.info(f"üéØ Walk-Forward –∞–Ω–∞–ª–∏–∑: {start_date.date()} ‚Üí {end_date.date()}")
    logger.info(f"Training –ø–µ—Ä–∏–æ–¥: {cfg.walk_forward.training_period_days} –¥–Ω–µ–π")
    logger.info(f"Testing –ø–µ—Ä–∏–æ–¥: {cfg.walk_forward.testing_period_days} –¥–Ω–µ–π")


    # Calculate walk-forward steps
    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: start_date —Ç–µ–ø–µ—Ä—å –Ω–∞—á–∞–ª–æ –¢–ï–°–¢–û–í–û–ì–û –ø–µ—Ä–∏–æ–¥–∞, –∞ –Ω–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ
    current_test_start = start_date
    walk_forward_steps = []
    bar_minutes = getattr(cfg.pair_selection, "bar_minutes", None) or 15
    bar_delta = pd.Timedelta(minutes=bar_minutes)
    while current_test_start < end_date:
        # –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –ø–µ—Ä–∏–æ–¥ –ü–†–ï–î–®–ï–°–¢–í–£–ï–¢ —Ç–µ—Å—Ç–æ–≤–æ–º—É
        training_start = current_test_start - pd.Timedelta(days=cfg.walk_forward.training_period_days)
        # –ó–∞–≤–µ—Ä—à–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –ø–µ—Ä–∏–æ–¥ –∑–∞ –æ–¥–∏–Ω –±–∞—Ä –¥–æ –Ω–∞—á–∞–ª–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ
        training_end = current_test_start - bar_delta
        testing_start = current_test_start
        testing_end = testing_start + pd.Timedelta(days=cfg.walk_forward.testing_period_days)
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–æ–∑–≤–æ–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤–æ–º—É –æ–∫–Ω—É –≤—ã—Ö–æ–¥–∏—Ç—å –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π –∑–∞ end_date
        # –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –¥–∞–Ω–Ω—ã–µ. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤–º–µ—Å—Ç–æ –∂–µ—Å—Ç–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–∞—Ç.
        buffer_days = 5  # –ü–æ–∑–≤–æ–ª—è–µ–º –¥–æ 5 –¥–Ω–µ–π –±—É—Ñ–µ—Ä–∞
        if testing_end > end_date + pd.Timedelta(days=buffer_days):
            logger.info(f"  –¢–µ—Å—Ç–æ–≤–æ–µ –æ–∫–Ω–æ {testing_end.date()} —Å–ª–∏—à–∫–æ–º –¥–∞–ª–µ–∫–æ –∑–∞ end_date {end_date.date()}")
            break
        
            
        logger.info(f"  –®–∞–≥ {len(walk_forward_steps)+1}: —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ {training_start.date()}-{training_end.date()}, —Ç–µ—Å—Ç {testing_start.date()}-{testing_end.date()}")
        walk_forward_steps.append((training_start, training_end, testing_start, testing_end))
        current_test_start = testing_end

    logger.info(f"üìà –ó–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ {len(walk_forward_steps)} walk-forward —à–∞–≥–æ–≤")
    
    # –û–¢–õ–ê–î–û–ß–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø—Ä–æ–±–ª–µ–º
    if len(walk_forward_steps) == 0:
        logger.warning("‚ö†Ô∏è  –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ù–µ—Ç walk-forward —à–∞–≥–æ–≤!")
        logger.warning(f"   start_date (–Ω–∞—á–∞–ª–æ —Ç–µ—Å—Ç–∞): {start_date.date()}")
        logger.warning(f"   end_date (–∫–æ–Ω–µ—Ü –≤—ã–±–æ—Ä–∫–∏): {end_date.date()}")
        logger.warning(f"   training_period_days: {cfg.walk_forward.training_period_days}")
        logger.warning(f"   testing_period_days: {cfg.walk_forward.testing_period_days}")
        
        # –†–∞—Å—Å—á–∏—Ç–∞–µ–º, —á—Ç–æ –¥–æ–ª–∂–Ω–æ –±—ã–ª–æ –±—ã—Ç—å
        would_be_testing_end = start_date + pd.Timedelta(days=cfg.walk_forward.testing_period_days)
        needed_training_start = start_date - pd.Timedelta(days=cfg.walk_forward.training_period_days)
        
        logger.warning(f"   testing_end –±—ã–ª –±—ã: {would_be_testing_end.date()}")
        logger.warning(f"   –Ω—É–∂–Ω—ã –¥–∞–Ω–Ω—ã–µ —Å: {needed_training_start.date()}")
        
        if would_be_testing_end > end_date:
            logger.warning(f"   ‚ùå –ü–†–ò–ß–ò–ù–ê: testing_end ({would_be_testing_end.date()}) > end_date ({end_date.date()})")
            logger.warning(f"   ‚úÖ –†–ï–®–ï–ù–ò–ï: –ü—Ä–æ–¥–ª–∏—Ç–µ end_date –¥–æ {would_be_testing_end.date()} –∏–ª–∏ —Å–æ–∫—Ä–∞—Ç–∏—Ç–µ testing_period_days")
        
    else:
        logger.info(f"‚úÖ –®–∞–≥–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω—ã:")
        for i, (tr_start, tr_end, te_start, te_end) in enumerate(walk_forward_steps, 1):
            logger.info(f"   –®–∞–≥ {i}: —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ {tr_start.date()}-{tr_end.date()}, —Ç–µ—Å—Ç {te_start.date()}-{te_end.date()}")

    # Initialize tracking variables
    aggregated_pnl = pd.Series(dtype=float)
    daily_pnl = []
    equity_data = []
    pair_count_data = []
    trade_stats = []
    equity = cfg.portfolio.initial_capital
    equity_curve = [equity]
    equity_data.append((start_date, equity))

    # Execute walk-forward steps
    step_tracker = ProgressTracker(len(walk_forward_steps), "Walk-forward steps", step=1)
    
    for step_idx, (training_start, training_end, testing_start, testing_end) in enumerate(walk_forward_steps, 1):
        step_tag = f"WF-—à–∞–≥ {step_idx}/{len(walk_forward_steps)}"
        
        with time_block(f"{step_tag}: training {training_start.date()}-{training_end.date()}, testing {testing_start.date()}-{testing_end.date()}"):
            # Load data only for this step
            with time_block("loading step data"):
                step_df_long = load_master_dataset(cfg.data_dir, training_start, testing_end)

            if step_df_long.empty:
                logger.warning(f"  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —à–∞–≥–∞ {step_idx}, –ø—Ä–æ–ø—É—Å–∫.")
                continue

            step_df = step_df_long.pivot_table(index="timestamp", columns="symbol", values="close")
            training_slice = step_df.loc[training_start:training_end]
            logger.info(f"  Training –¥–∞–Ω–Ω—ã–µ: {training_slice.shape[0]:,} —Å—Ç—Ä–æ–∫ √ó {training_slice.shape[1]} —Å–∏–º–≤–æ–ª–æ–≤")
                
            if training_slice.empty or len(training_slice.columns) < 2:
                logger.warning(f"  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤ —à–∞–≥–µ {step_idx}")
                pairs = []
            else:
                # Normalize training data
                with time_block("normalizing training data"):
                    logger.debug(f"  –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {len(training_slice.columns)} —Å–∏–º–≤–æ–ª–æ–≤")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤ –¥–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
                    symbols_before = set(training_slice.columns)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏–º–≤–æ–ª—ã —Å –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–π —Ü–µ–Ω–æ–π (max=min)
                    constant_price_symbols = [col for col in training_slice.columns
                                             if training_slice[col].max() == training_slice[col].min()]
                    if constant_price_symbols:
                        logger.info(f"  –°–∏–º–≤–æ–ª—ã —Å –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–π —Ü–µ–Ω–æ–π: {len(constant_price_symbols)}")
                        logger.debug(f"  –°–ø–∏—Å–æ–∫: {', '.join(sorted(constant_price_symbols))}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏–º–≤–æ–ª—ã —Å NaN –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                    nan_symbols = [col for col in training_slice.columns
                                  if training_slice[col].isna().any()]
                    if nan_symbols:
                        logger.info(f"  –°–∏–º–≤–æ–ª—ã —Å NaN –∑–Ω–∞—á–µ–Ω–∏—è–º–∏: {len(nan_symbols)}")
                        logger.debug(f"  –°–ø–∏—Å–æ–∫: {', '.join(sorted(nan_symbols))}")
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
                    norm_method = getattr(cfg.data_processing, 'normalization_method', 'minmax')
                    fill_method = getattr(cfg.data_processing, 'fill_method', 'ffill')
                    min_history_ratio = getattr(cfg.data_processing, 'min_history_ratio', 0.8)
                    handle_constant = getattr(cfg.data_processing, 'handle_constant', True)
                    
                    logger.info(f"  –ü—Ä–∏–º–µ–Ω—è–µ–º –º–µ—Ç–æ–¥ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {norm_method}")
                    normalized_training, norm_stats = preprocess_and_normalize_data(
                        training_slice,
                        min_history_ratio=min_history_ratio,
                        fill_method=fill_method,
                        norm_method=norm_method,
                        handle_constant=handle_constant
                    )
                    
                    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
                    logger.info(f"  –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:")
                    logger.info(f"    –ò—Å—Ö–æ–¥–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã: {norm_stats['initial_symbols']}")
                    logger.info(f"    –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è: {norm_stats['low_history_ratio']}")
                    logger.info(f"    –ü–æ—Å—Ç–æ—è–Ω–Ω–∞—è —Ü–µ–Ω–∞: {norm_stats['constant_price']}")
                    logger.info(f"    NaN –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {norm_stats['nan_after_norm']}")
                    logger.info(f"    –ò—Ç–æ–≥–æ–≤—ã–µ —Å–∏–º–≤–æ–ª—ã: {norm_stats['final_symbols']} ({norm_stats['final_symbols']/norm_stats['initial_symbols']*100:.1f}%)")
                    
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Ç–µ—Ä—è–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                    symbols_after = set(normalized_training.columns)
                    dropped_symbols = symbols_before - symbols_after
                    logger.info(f"  –ü–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {len(normalized_training.columns)} —Å–∏–º–≤–æ–ª–æ–≤ (–ø–æ—Ç–µ—Ä—è–Ω–æ {len(dropped_symbols)})")
                    if dropped_symbols and len(dropped_symbols) <= 20:
                        logger.info(f"  –û—Ç–±—Ä–æ—à–µ–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã: {', '.join(sorted(dropped_symbols))}")
                    elif dropped_symbols:
                        logger.info(f"  –û—Ç–±—Ä–æ—à–µ–Ω–æ –º–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: {len(dropped_symbols)} –∏–∑ {len(symbols_before)}")
                    
                if len(normalized_training.columns) < 2:
                    logger.warning("  –ü–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Å—Ç–∞–ª–æ—Å—å –º–µ–Ω–µ–µ 2 —Å–∏–º–≤–æ–ª–æ–≤")
                    pairs = []
                else:
                    # SSD computation
                    with time_block("SSD computation"):
                        logger.info("  –†–∞—Å—á–µ—Ç SSD –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä (–±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è)")
                        # –°–Ω–∞—á–∞–ª–∞ —Å—á–∏—Ç–∞–µ–º SSD –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä
                        ssd = math_utils.calculate_ssd(normalized_training, top_k=None)
                        logger.info(f"  SSD —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–≤—Å–µ –ø–∞—Ä—ã): {len(ssd)} –ø–∞—Ä")
                        
                        # –ó–∞—Ç–µ–º –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ top-N –ø–∞—Ä –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                        ssd_top_n = cfg.pair_selection.ssd_top_n
                        if len(ssd) > ssd_top_n:
                            logger.info(f"  –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ top-{ssd_top_n} –ø–∞—Ä –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                            ssd = ssd.sort_values().head(ssd_top_n)
                        
                    # Filter pairs
                    with time_block("filtering pairs by cointegration and half-life"):
                        from coint2.pipeline.filters import filter_pairs_by_coint_and_half_life
                        ssd_pairs = [(s1, s2) for s1, s2 in ssd.index]
                        filtered_pairs = filter_pairs_by_coint_and_half_life(
                            ssd_pairs,
                            training_slice,
                            pvalue_threshold=cfg.pair_selection.coint_pvalue_threshold,
                            min_half_life=cfg.pair_selection.min_half_life_days,
                            max_half_life=cfg.pair_selection.max_half_life_days,
                            min_mean_crossings=cfg.pair_selection.min_mean_crossings,
                            min_correlation=cfg.pair_selection.min_correlation,
                            max_correlation=cfg.pair_selection.max_correlation,
                            min_spread_std=cfg.pair_selection.min_spread_std,
                            max_spread_std=cfg.pair_selection.max_spread_std,
                            max_hurst_exponent=getattr(cfg.pair_selection, 'max_hurst_exponent', 0.5),
                            adaptive_quantiles=cfg.pair_selection.adaptive_quantiles,
                            save_filter_reasons=cfg.pair_selection.save_filter_reasons,
                            save_std_histogram=cfg.pair_selection.save_std_histogram,
                            kpss_pvalue_threshold=cfg.pair_selection.kpss_pvalue_threshold,
                            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã commission_pct –∏ slippage_pct —É–¥–∞–ª–µ–Ω—ã
                        )
                        logger.info(f"  –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: {len(ssd_pairs)} ‚Üí {len(filtered_pairs)} –ø–∞—Ä")
                        pairs = filtered_pairs

        # Select active pairs and run backtests
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–∞—Ä—ã –ø–æ –∫–∞—á–µ—Å—Ç–≤—É (–ø–æ —É–±—ã–≤–∞–Ω–∏—é std, —á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç –±–æ–ª—å—à—É—é –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å —Å–ø—Ä–µ–¥–∞)
        # Alternatively: –º–æ–∂–Ω–æ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –¥—Ä—É–≥–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º –∫–∞—á–µ—Å—Ç–≤–∞
        if pairs:
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–∞—Ä—ã –ø–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—é —Å–ø—Ä–µ–¥–∞ (–±–æ–ª—å—à–µ = –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –±–æ–ª–µ–µ –ø—Ä–∏–±—ã–ª—å–Ω–æ)
            quality_sorted_pairs = sorted(pairs, key=lambda x: abs(x[4]), reverse=True)  # x[4] = std
            active_pairs = quality_sorted_pairs[:cfg.portfolio.max_active_positions]
            logger.info(f"  –¢–æ–ø-3 –ø–∞—Ä—ã –ø–æ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ —Å–ø—Ä–µ–¥–∞:")
            for i, (s1, s2, beta, mean, std, metrics) in enumerate(active_pairs[:3], 1):
                logger.info(f"    {i}. {s1}-{s2}: beta={beta:.4f}, std={std:.4f}")
        else:
            active_pairs = []
        
        num_active_pairs = len(active_pairs)
        logger.info(f"  –ê–∫—Ç–∏–≤–Ω—ã—Ö –ø–∞—Ä –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏: {num_active_pairs}")

        step_pnl = pd.Series(dtype=float)
        total_step_pnl = 0.0

        logger.info(
            f"  –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –∫–∞–ø–∏—Ç–∞–ª–∞: equity={equity}, num_active_pairs={num_active_pairs}"
        )
        if num_active_pairs > 0:
            capital_per_pair = equity * cfg.portfolio.risk_per_position_pct / num_active_pairs
        else:
            capital_per_pair = 0.0

        if capital_per_pair < 0:
            logger.error(
                f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ö–∞–ø–∏—Ç–∞–ª –Ω–∞ –ø–∞—Ä—É —Å—Ç–∞–ª –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º ({capital_per_pair})."
            )
            capital_per_pair = 0.0

        logger.info(f"  –ö–∞–ø–∏—Ç–∞–ª –Ω–∞ –ø–∞—Ä—É: ${capital_per_pair:,.2f}")

        period_label = f"{training_start.strftime('%m/%d')}-{testing_end.strftime('%m/%d')}"
        pair_count_data.append((period_label, len(active_pairs)))

        # Run backtests for active pairs
        if active_pairs:
            pair_tracker = ProgressTracker(len(active_pairs), f"{step_tag} backtests", step=max(1, len(active_pairs)//5))
            
            for pair_idx, (s1, s2, beta, mean, std, metrics) in enumerate(active_pairs, 1):
                pair_data = step_df.loc[testing_start:testing_end, [s1, s2]].dropna()
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: –æ–±–∞ —Ä—è–¥–∞ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å 100
                if not pair_data.empty:
                    pair_data = pair_data / pair_data.iloc[0] * 100

                bt = PairBacktester(
                    pair_data,
                    rolling_window=cfg.backtest.rolling_window,
                    z_threshold=cfg.backtest.zscore_threshold,
                    z_exit=getattr(cfg.backtest, 'zscore_exit', 0.0),
                    commission_pct=getattr(cfg.backtest, 'commission_pct', 0.0),
                    slippage_pct=getattr(cfg.backtest, 'slippage_pct', 0.0),
                    annualizing_factor=getattr(cfg.backtest, 'annualizing_factor', 365),
                    capital_at_risk=capital_per_pair,
                    stop_loss_multiplier=getattr(cfg.backtest, 'stop_loss_multiplier', 2.0),
                    take_profit_multiplier=getattr(cfg.backtest, 'take_profit_multiplier', None),
                    # Convert hours to periods based on 15-minute timeframe
                    cooldown_periods=int(getattr(cfg.backtest, 'cooldown_hours', 0) * 60 / 15),  # 15-–º–∏–Ω—É—Ç–Ω—ã–π —Ç–∞–π–º—Ñ—Ä–µ–π–º
                    wait_for_candle_close=getattr(cfg.backtest, 'wait_for_candle_close', False),
                    max_margin_usage=getattr(cfg.portfolio, 'max_margin_usage', 1.0),
                )
                bt.run()
                results = bt.get_results()
                pnl_series = results["pnl"]
                step_pnl = step_pnl.add(pnl_series, fill_value=0)
                total_step_pnl += pnl_series.sum()
                
                # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å–¥–µ–ª–∫–∞–º –¥–ª—è –¥–∞–Ω–Ω–æ–π –ø–∞—Ä—ã
                if isinstance(results, dict):
                    trades = results.get("trades", pd.Series())
                    positions = results.get("position", pd.Series(dtype=float))
                    costs = results.get("costs", pd.Series())
                else:
                    # –ï—Å–ª–∏ results - DataFrame, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–ª–æ–Ω–∫–∏
                    trades = results.get("trades", results.get("trades", pd.Series()))
                    positions = results.get("position", results.get("position", pd.Series(dtype=float)))
                    costs = results.get("costs", results.get("costs", pd.Series()))

                # –ù–æ–≤–∞—è –ª–æ–≥–∏–∫–∞ –ø–æ–¥—Å—á—ë—Ç–∞ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —Å–¥–µ–ª–æ–∫
                if not positions.empty:
                    positions = positions.fillna(method="ffill")
                    prev_positions = positions.shift(1).fillna(0)
                    is_trade_open_event = (prev_positions == 0) & (positions != 0)
                    actual_trade_count = int(is_trade_open_event.sum())
                else:
                    actual_trade_count = 0
                
                # –í—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –ø–∞—Ä–∞–º, –¥–∞–∂–µ –µ—Å–ª–∏ —Å–¥–µ–ª–æ–∫ –Ω–µ –±—ã–ª–æ
                pair_pnl = pnl_series.sum()
                pair_costs = costs.sum() if not costs.empty else 0

                trade_stats.append({
                    'pair': f'{s1}-{s2}',
                    'period': period_label,
                    'total_pnl': pair_pnl,
                    'total_costs': pair_costs,
                    'trade_count': actual_trade_count,
                    'avg_pnl_per_trade': pair_pnl / max(actual_trade_count, 1),
                    'win_days': (pnl_series > 0).sum(),
                    'lose_days': (pnl_series < 0).sum(),
                    'total_days': len(pnl_series),
                    'max_daily_gain': pnl_series.max(),
                    'max_daily_loss': pnl_series.min()
                })
                
                pair_tracker.update()
            
            pair_tracker.finish()

        # Update equity and daily P&L
        if not step_pnl.empty:
            running_equity = equity
            for date, pnl in step_pnl.items():
                daily_pnl.append((date, pnl))
                running_equity += pnl
                equity_data.append((date, running_equity))

        aggregated_pnl = pd.concat([aggregated_pnl, step_pnl])
        equity += total_step_pnl
        equity_curve.append(equity)
        
        logger.info(f"  –®–∞–≥ P&L: ${total_step_pnl:+,.2f}, –ù–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π: ${equity:,.2f}")
        step_tracker.update()

    step_tracker.finish()

    # Process results and create metrics
    with time_block("processing final results"):
        aggregated_pnl = aggregated_pnl.dropna()
        
        # Create series for analysis
        if daily_pnl:
            dates, pnls = zip(*daily_pnl)
            pnl_series = pd.Series(pnls, index=pd.to_datetime(dates))
            pnl_series = pnl_series.groupby(pnl_series.index.date).sum()
            pnl_series.index = pd.to_datetime(pnl_series.index)
        else:
            pnl_series = pd.Series(dtype=float)
        
        if equity_data:
            eq_dates, eq_values = zip(*equity_data)
            equity_series = pd.Series(eq_values, index=pd.to_datetime(eq_dates))
        else:
            equity_series = pd.Series([cfg.portfolio.initial_capital])

        # Calculate metrics
        if aggregated_pnl.empty:
            base_metrics = {
                "sharpe_ratio_abs": 0.0,
                "sharpe_ratio_on_returns": 0.0,
                "max_drawdown_abs": 0.0,
                "max_drawdown_on_equity": 0.0,
                "total_pnl": 0.0,
            }
        else:
            cumulative = aggregated_pnl.cumsum()
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–ø–∏—Ç–∞–ª –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Ä–∏—Å–∫–∞ –Ω–∞ —Å–¥–µ–ª–∫—É –∏ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
            capital_per_pair = equity * cfg.portfolio.risk_per_position_pct
            if capital_per_pair < 0:
                logger.error(
                    f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ö–∞–ø–∏—Ç–∞–ª –Ω–∞ –ø–∞—Ä—É —Å—Ç–∞–ª –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º ({capital_per_pair})."
                )
                capital_per_pair = 0.0
            sharpe_abs = performance.sharpe_ratio(aggregated_pnl, cfg.backtest.annualizing_factor)
            sharpe_on_returns = performance.sharpe_ratio_on_returns(
                aggregated_pnl, capital_per_pair, cfg.backtest.annualizing_factor
            )
            max_dd_abs = performance.max_drawdown(cumulative)
            max_dd_on_equity = performance.max_drawdown_on_equity(equity_series)
            base_metrics = {
                "sharpe_ratio_abs": sharpe_abs,
                "sharpe_ratio_on_returns": sharpe_on_returns,
                "max_drawdown_abs": max_dd_abs,
                "max_drawdown_on_equity": max_dd_on_equity,
                "total_pnl": cumulative.iloc[-1] if not cumulative.empty else 0.0,
            }
        
        extended_metrics = calculate_extended_metrics(pnl_series, equity_series)
        
        # Trade statistics
        trade_metrics = {}
        if trade_stats:
            trades_df = pd.DataFrame(trade_stats)
            trade_metrics = {
                'total_trades': trades_df['trade_count'].sum(),
                'total_pairs_traded': len(trades_df['pair'].unique()),
                'total_costs': trades_df['total_costs'].sum(),
                'avg_trades_per_pair': trades_df['trade_count'].mean(),
                'win_rate_trades': trades_df['win_days'].sum() / max(trades_df['total_days'].sum(), 1),
                'best_pair_pnl': trades_df['total_pnl'].max(),
                'worst_pair_pnl': trades_df['total_pnl'].min(),
                'avg_pnl_per_pair': trades_df['total_pnl'].mean(),
            }
        
        all_metrics = {**base_metrics, **extended_metrics, **trade_metrics}
    
    # Create reports
    with time_block("generating reports"):
        results_dir = Path(cfg.results_dir)
        
        try:
            create_performance_report(
                equity_curve=equity_series,
                pnl_series=pnl_series,
                metrics=all_metrics,
                pair_counts=pair_count_data,
                results_dir=results_dir,
                strategy_name="CointegrationStrategy"
            )
            
            summary = format_metrics_summary(all_metrics)
            print(summary)
            
            # Save data
            metrics_df = pd.DataFrame([all_metrics])
            metrics_df.to_csv(results_dir / "strategy_metrics.csv", index=False)
            logger.info(f"üìã –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_dir / 'strategy_metrics.csv'}")
            
            if not pnl_series.empty:
                pnl_series.to_csv(results_dir / "daily_pnl.csv", header=['PnL'])
                logger.info(f"üìà –î–Ω–µ–≤–Ω—ã–µ P&L —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_dir / 'daily_pnl.csv'}")
            
            if not equity_series.empty:
                equity_series.to_csv(results_dir / "equity_curve.csv", header=['Equity'])
                logger.info(f"üí∞ –ö—Ä–∏–≤–∞—è –∫–∞–ø–∏—Ç–∞–ª–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {results_dir / 'equity_curve.csv'}")
            
            if trade_stats:
                trades_df = pd.DataFrame(trade_stats)
                trades_df.to_csv(results_dir / "trade_statistics.csv", index=False)
                logger.info(f"üîÑ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–¥–µ–ª–∫–∞–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {results_dir / 'trade_statistics.csv'}")
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –æ—Ç—á–µ—Ç–æ–≤: {e}")
    
    return base_metrics
