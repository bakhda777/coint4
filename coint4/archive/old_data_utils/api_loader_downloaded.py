#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥—É–ª—å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
try:
    from chunk_intervals import group_timestamps_into_chunks
except ImportError:
    # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –¥—Ä—É–≥–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    try:
        from temporary_useful_files.chunk_intervals import group_timestamps_into_chunks
    except ImportError:
        def group_timestamps_into_chunks(timestamps, max_gap=15*60*1000, max_chunk_size=7*24*60*60*1000):
            """–†–µ–∑–µ—Ä–≤–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è, –µ—Å–ª–∏ –º–æ–¥—É–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω"""
            if not timestamps:
                return []
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º timestamps
            timestamps.sort()
            
            # –ü—Ä–æ—Å—Ç–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–Ω—è–º
            chunks = []
            date_groups = {}
            
            for ts in timestamps:
                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –¥–Ω—è–º
                dt = datetime.fromtimestamp(ts/1000)
                day_key = dt.strftime('%Y-%m-%d')
                if day_key not in date_groups:
                    date_groups[day_key] = []
                date_groups[day_key].append(ts)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≥—Ä—É–ø–ø—ã –¥–Ω–µ–π –≤ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
            for day, day_timestamps in date_groups.items():
                if day_timestamps:
                    min_ts = min(day_timestamps)
                    max_ts = max(day_timestamps)
                    
                    start_dt = datetime.fromtimestamp(min_ts/1000) - timedelta(minutes=15)
                    end_dt = datetime.fromtimestamp(max_ts/1000) + timedelta(minutes=15)
                    
                    chunks.append((start_dt, end_dt))
            
            return chunks

# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
import json, logging, os, configparser, time, random
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import ast
import multiprocessing
from functools import partial, lru_cache
from itertools import cycle
import numpy as np
import pandas as pd
from typing import List, Tuple, Dict, Any, Optional, Union, Set
from pathlib import Path
import shutil
import psutil
import gc
from logging.handlers import RotatingFileHandler
from threading import Lock, RLock
import signal
import sys
import argparse

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥—É–ª–∏ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏
try:
    from file_lock_manager import FileLock, file_lock_manager
    from parquet_duplicates_checker import check_and_fix_duplicates
    modules_imported = True
except ImportError:
    modules_imported = False
    # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏ FileLock –¥–ª—è —Å–ª—É—á–∞—è, –∫–æ–≥–¥–∞ –º–æ–¥—É–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
    class FileLock:
        def __init__(self, file_path, timeout=None):
            self.lock = Lock()
            self.file_path = file_path
            
        def __enter__(self):
            self.lock.acquire()
            return self
            
        def __exit__(self, exc_type, exc_val, exc_tb):
            self.lock.release()

# Make line_profiler optional
try:
    from line_profiler import LineProfiler
except ImportError:
    line_profiler_missing = True
    class LineProfiler:
        def __init__(self, *args, **kwargs):
            pass
        def add_function(self, func):
            return self
        def enable_by_count(self):
            pass
        def disable_by_count(self):
            pass
        def print_stats(self, stream=None):
            print("LineProfiler not available")

# Make pybit optional
try:
    from pybit.unified_trading import HTTP, WebSocket
    from pybit.exceptions import FailedRequestError, InvalidRequestError
except ImportError:
    pybit_missing = True
    class HTTP:
        def __init__(self, *args, **kwargs):
            pass
    class WebSocket:
        def __init__(self, *args, **kwargs):
            pass
    class FailedRequestError(Exception):
        pass
    class InvalidRequestError(Exception):
        pass

# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏
import pyarrow as pa
import pyarrow.parquet as pq
import pyarrow.dataset as ds

# ================== –ö–û–ù–°–¢–ê–ù–¢–´ –ò –ù–ê–°–¢–†–û–ô–ö–ò ==================

# –û—Å–Ω–æ–≤–Ω—ã–µ –ø—É—Ç–∏
DATA_DIR = Path("data_downloaded")  # –ò–∑–º–µ–Ω–µ–Ω–æ –Ω–∞ data_downloaded
MARKETS_FILE = "Markets.txt"

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å API
API_RATE_LIMIT = 120  # –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
MIN_REQUEST_INTERVAL = 1.0 / API_RATE_LIMIT  # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
REQUEST_WINDOW = 1.0  # –æ–∫–Ω–æ –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ RPS

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫
MAX_RETRIES = 3
BASE_DELAY = 1.0
MAX_DELAY = 5.0
CHUNK_SIZE = 200  # –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è API –∑–∞–ø—Ä–æ—Å–æ–≤

# ================== –õ–û–ì–ò–†–û–í–ê–ù–ò–ï ==================

def setup_logging():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è."""
    logger = logging.getLogger()
    
    # –†–æ—Ç–∞—Ü–∏—è –ª–æ–≥–æ–≤
    file_handler = RotatingFileHandler(
        'data_loader.log',
        maxBytes=10*1024*1024,  # 10MB
        backupCount=5
    )
    file_handler.setFormatter(JsonFormatter())
    
    # –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –≤—ã–≤–æ–¥
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    logger.setLevel(logging.INFO)
    
    return logger

class JsonFormatter(logging.Formatter):
    def format(self, record):
        log_data = {
            'time': self.formatTime(record),
            'level': record.levelname,
            'message': record.getMessage(),
            'function': record.funcName,
            'line': record.lineno
        }
        return json.dumps(log_data)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞
logger = setup_logging()

if 'line_profiler_missing' in globals():
    logger.warning("line_profiler not installed. Profiling functionality will be limited.")

if 'pybit_missing' in globals():
    logger.warning("pybit not installed. API functionality will be limited.")

# ================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø API ==================

# –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
MAINNET_API_KEYS = ['dummy_key1', 'dummy_key2']
MAINNET_API_SECRETS = ['dummy_secret1', 'dummy_secret2']

if os.path.exists('config.ini'):
    try:
        config = configparser.ConfigParser()
        config.read('config.ini')
        
        if 'API' in config and 'MAINNET_API_KEY1' in config['API'] and 'MAINNET_API_KEY2' in config['API']:
            MAINNET_API_KEYS = [
                config['API']['MAINNET_API_KEY1'],
                config['API']['MAINNET_API_KEY2']
            ]
        if 'API' in config and 'MAINNET_API_SECRET1' in config['API'] and 'MAINNET_API_SECRET2' in config['API']:
            MAINNET_API_SECRETS = [
                config['API']['MAINNET_API_SECRET1'],
                config['API']['MAINNET_API_SECRET2']
            ]
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {str(e)}. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")
else:
    logger.warning("–§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ config.ini –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")

def validate_api_keys() -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ API –∫–ª—é—á–µ–π"""
    if not MAINNET_API_KEYS or not MAINNET_API_SECRETS:
        logger.error("API –∫–ª—é—á–∏ –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")
        return False
        
    if len(MAINNET_API_KEYS) != len(MAINNET_API_SECRETS):
        logger.error("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ API –∫–ª—é—á–µ–π –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–µ–∫—Ä–µ—Ç–æ–≤")
        return False
        
    return True

# ================== –§–£–ù–ö–¶–ò–ò –ó–ê–ì–†–£–ó–ö–ò –°–ò–ú–í–û–õ–û–í ==================

def fetch_bybit_markets(category: str = "spot", save_to_file: bool = True) -> List[str]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä —Å Bybit API.
    
    Args:
        category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ä—ã–Ω–∫–∞ ("spot", "linear", "inverse")
        save_to_file: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–∞–π–ª Markets.txt
        
    Returns:
        List[str]: –°–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤/–ø–∞—Ä
    """
    logger.info(f"üîç –ó–∞–ø—Ä–æ—Å —Å–ø–∏—Å–∫–∞ –ø–∞—Ä Bybit –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category}")
    
    try:
        # –°–æ–∑–¥–∞–µ–º HTTP –∫–ª–∏–µ–Ω—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º API –∫–ª—é—á–µ–π
        session = HTTP(
            testnet=False,
            api_key=MAINNET_API_KEYS[0] if MAINNET_API_KEYS else None,
            api_secret=MAINNET_API_SECRETS[0] if MAINNET_API_SECRETS else None
        )
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ç–æ—Ä–≥–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
        response = session.get_instruments_info(
            category=category
        )
        
        if response['retCode'] != 0:
            logger.error(f"‚ùå API –æ—à–∏–±–∫–∞: {response['retMsg']}")
            return []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤
        instruments = response['result']['list']
        symbols = [item['symbol'] for item in instruments if 'symbol' in item]
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ - –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã)
        symbols = sorted(symbols)
        
        logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(symbols)} –ø–∞—Ä —Å Bybit API")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª, –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
        if save_to_file:
            with open(MARKETS_FILE, 'w', encoding='utf-8') as f:
                f.write(', '.join(symbols))
            logger.info(f"‚úÖ –°–ø–∏—Å–æ–∫ –ø–∞—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {MARKETS_FILE}")
            
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ JSON
        markets_info = []
        for item in instruments:
            if 'symbol' in item:
                markets_info.append({
                    'symbol': item.get('symbol', ''),
                    'status': item.get('status', ''),
                    'baseCoin': item.get('baseCoin', ''),
                    'quoteCoin': item.get('quoteCoin', ''),
                    'innovation': item.get('innovation', '0'),
                    'marginTrading': item.get('marginTrading', ''),
                })
        
        markets_json_file = 'Markets_extended.json'
        with open(markets_json_file, 'w', encoding='utf-8') as f:
            json.dump(markets_info, f, indent=2)
        logger.info(f"‚úÖ –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {markets_json_file}")
        
        return symbols
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –ø–∞—Ä: {e}")
        return []

def load_symbols_from_markets(markets_file: str) -> List[str]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞ Markets.txt.
    
    Args:
        markets_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Å–∏–º–≤–æ–ª–∞–º–∏
        
    Returns:
        List[str]: –°–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤
    """
    try:
        with open(markets_file, 'r', encoding='utf-8') as f:
            content = f.read().strip()
            
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–∞–ø—è—Ç—ã–º –∏ –æ—á–∏—â–∞–µ–º
        symbols = [s.strip() for s in content.split(',') if s.strip()]
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        unique_symbols = []
        seen = set()
        for symbol in symbols:
            if symbol not in seen:
                unique_symbols.append(symbol)
                seen.add(symbol)
                
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(unique_symbols)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ {markets_file}")
        return unique_symbols
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {markets_file}: {str(e)}")
        return []
# ================== –ó–ê–ì–†–£–ó–ö–ê –ò –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–• ==================

class RequestTracker:
    def __init__(self):
        self.requests = []
        self.total_requests = 0
        self.start_time = time.time()
    
    def add_request(self):
        current_time = time.time()
        self.requests.append(current_time)
        self.total_requests += 1
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        while self.requests and current_time - self.requests[0] > REQUEST_WINDOW:
            self.requests.pop(0)
    
    def get_current_rps(self) -> float:
        return len(self.requests) / REQUEST_WINDOW
    
    def get_average_rps(self) -> float:
        elapsed = time.time() - self.start_time
        return self.total_requests / elapsed if elapsed > 0 else 0

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Ç—Ä–µ–∫–µ—Ä –∑–∞–ø—Ä–æ—Å–æ–≤
request_tracker = RequestTracker()
session_last_request = {}

def get_delay_time(session: HTTP) -> float:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è –ª–∏–º–∏—Ç–æ–≤."""
    current_time = time.time()
    last_request_time = session_last_request.get(id(session), 0)
    time_since_last_request = current_time - last_request_time
    if time_since_last_request < MIN_REQUEST_INTERVAL:
        delay = MIN_REQUEST_INTERVAL - time_since_last_request
        return delay
    return 0

def fetch_symbol_data(
    session: HTTP,
    symbol: str,
    start_time: datetime,
    end_time: datetime
) -> Optional[pd.DataFrame]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–∏–º–≤–æ–ª–∞ –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥.
    
    Args:
        session: HTTP —Å–µ—Å—Å–∏—è
        symbol: –°–∏–º–≤–æ–ª –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        start_time: –ù–∞—á–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è
        end_time: –ö–æ–Ω–µ—á–Ω–æ–µ –≤—Ä–µ–º—è
        
    Returns:
        Optional[pd.DataFrame]: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–ª–∏ None
    """
    logger.info(f"üîÑ {symbol}: –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö {start_time.strftime('%Y-%m-%d')} - {end_time.strftime('%Y-%m-%d')}")
    
    try:
        start_ts = int(start_time.timestamp() * 1000)
        end_ts = int(end_time.timestamp() * 1000)
    
        if start_ts >= end_ts:
            logger.error(f"‚ùå {symbol}: –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç")
            return None
    
        # –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è —Å–æ–±–ª—é–¥–µ–Ω–∏—è –ª–∏–º–∏—Ç–æ–≤
        delay = get_delay_time(session)
        if delay > 0:
            time.sleep(delay)
    
        all_data = []
        current_start = start_ts
            
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        while current_start < end_ts:
            current_end = min(current_start + (CHUNK_SIZE * 15 * 60 * 1000), end_ts)
                
            try:
                response = session.get_kline(
                    category="spot",
                    symbol=symbol,
                    interval="15",  # 15 –º–∏–Ω—É—Ç
                    start=current_start,
                    end=current_end,
                    limit=CHUNK_SIZE
                )
            
                if response['retCode'] != 0:
                    logger.error(f"‚ùå {symbol}: API –æ—à–∏–±–∫–∞ - {response['retMsg']}")
                    return None
                    
                data = response['result']['list']
                if data:
                    all_data.extend(data)
                    
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑–∞–ø—Ä–æ—Å–æ–≤
                request_tracker.add_request()
                current_rps = request_tracker.get_current_rps()
                
                # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                if current_rps > API_RATE_LIMIT:
                    excess_rps = current_rps - API_RATE_LIMIT
                    delay_time = excess_rps / (API_RATE_LIMIT * 10)
                    if delay_time > 0:
                        time.sleep(delay_time)
                
                session_last_request[id(session)] = time.time()
                current_start = current_end
            
            except Exception as e:
                logger.error(f"‚ùå {symbol}: –æ—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ —á–∞–Ω–∫–∞: {e}")
                return None
        
        if not all_data:
            logger.warning(f"‚ö†Ô∏è {symbol}: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–µ—Ä–∏–æ–¥")
            return None
        
        # –°–æ–∑–¥–∞–µ–º DataFrame
        df = pd.DataFrame(all_data, columns=[
            'timestamp', 'open', 'high', 'low', 'close', 'volume', 'turnover'
        ])
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–∏–ø—ã
        df['ts_ms'] = pd.to_numeric(df['timestamp'], errors='coerce')
        df['timestamp'] = pd.to_numeric(df['timestamp'], errors='coerce')
        df['open'] = pd.to_numeric(df['open'], errors='coerce')
        df['high'] = pd.to_numeric(df['high'], errors='coerce')
        df['low'] = pd.to_numeric(df['low'], errors='coerce')
        df['close'] = pd.to_numeric(df['close'], errors='coerce')
        df['volume'] = pd.to_numeric(df['volume'], errors='coerce')
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–º–≤–æ–ª –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∞ data_optimized
        df['symbol'] = symbol
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        df = df.sort_values('timestamp')
        
        logger.info(f"‚úÖ {symbol}: –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤")
        return df
        
    except Exception as e:
        logger.error(f"‚ùå {symbol}: –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return None

# –°–ª–æ–≤–∞—Ä—å –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ –º–µ—Å—è—Ü–∞–º
monthly_data_cache: Dict[str, Dict[str, pd.DataFrame]] = {}

# –ü–∞–ø–∫–∞ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
TEMP_DIR = Path("data_temp")

def get_month_year_key(dt: datetime) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–ª—é—á –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ –º–µ—Å—è—Ü–∞–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ 'YYYY-MM'."""
    return f"{dt.year}-{dt.month:02d}"


def save_to_temp_file(symbol: str, df: pd.DataFrame, max_retries: int = 3) -> List[str]:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ –º–µ—Å—è—Ü).
    
    Args:
        symbol: –°–∏–º–≤–æ–ª –¥–∞–Ω–Ω—ã—Ö
        df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
        max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ
        
    Returns:
        List[str]: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Å–æ–∑–¥–∞–Ω–Ω—ã–º –≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º
    """
    if df.empty:
        logger.warning(f"‚ö†Ô∏è {symbol}: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        return []
        
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    TEMP_DIR.mkdir(parents=True, exist_ok=True)
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –º–µ—Å—è—Ü–∞–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
    temp_files = []
    
    try:
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –∫–ª—é—á–æ–º year-month –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
        df['year_month'] = df['timestamp'].apply(lambda ts: 
                                               datetime.fromtimestamp(ts/1000).strftime("%Y-%m"))
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –º–µ—Å—è—Ü–∞–º
        for month_key, month_df in df.groupby('year_month'):
            year, month = month_key.split('-')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
            if month_df['timestamp'].duplicated().any():
                logger.warning(f"‚ö†Ô∏è {symbol}: –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã timestamp –∑–∞ {month_key}, —É–¥–∞–ª—è–µ–º")
                month_df = month_df.drop_duplicates(subset=['timestamp'])
                
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–ª–æ–Ω–∫—É –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
            if 'year_month' in month_df.columns:
                month_df = month_df.drop('year_month', axis=1)
                
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            temp_file = TEMP_DIR / f"{symbol}_{year}_{month}.parquet"
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            for attempt in range(max_retries):
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º PyArrow –Ω–∞–ø—Ä—è–º—É—é –¥–ª—è –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                    table = pa.Table.from_pandas(month_df)
                    pq.write_table(table, temp_file)
                    temp_files.append(str(temp_file))
                    logger.debug(f"‚úÖ {symbol}: –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {temp_file}")
                    break
                except Exception as e:
                    if attempt < max_retries - 1:
                        wait_time = 0.5 * (attempt + 1)
                        logger.warning(f"‚ö†Ô∏è {symbol}: –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/{max_retries}): {e}")
                        time.sleep(wait_time)
                    else:
                        logger.error(f"‚ùå {symbol}: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {e}")
                        
    except Exception as e:
        logger.error(f"‚ùå {symbol}: –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {e}")
        
    return temp_files


def merge_temp_files_to_monthly(year: str, month: str, max_retries: int = 3) -> bool:
    """
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –æ–¥–Ω–æ–≥–æ –º–µ—Å—è—Ü–∞ –≤ –µ–¥–∏–Ω—ã–π —Ñ–∞–π–ª –¥–∞–Ω–Ω—ã—Ö.
    –ï—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º.
    
    Args:
        year: –ì–æ–¥ –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY
        month: –ú–µ—Å—è—Ü –≤ —Ñ–æ—Ä–º–∞—Ç–µ MM
        max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ
        
    Returns:
        bool: True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–æ
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –º–µ—Å—è—Ü–∞
        pattern = f"*_{year}_{month}.parquet"
        temp_files = list(TEMP_DIR.glob(pattern))
        
        if not temp_files:
            logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∑–∞ {year}-{month}")
            return False
            
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–ª—è –∏—Ç–æ–≥–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
        month_dir = DATA_DIR / f"year={year}" / f"month={month}"
        month_dir.mkdir(parents=True, exist_ok=True)
        
        # –ò—Ç–æ–≥–æ–≤—ã–π —Ñ–∞–π–ª –¥–∞–Ω–Ω—ã—Ö
        output_file = month_dir / f"data_part_{month}.parquet"
        
        # –ß–∏—Ç–∞–µ–º –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        all_dfs = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –∑–∞ —ç—Ç–æ—Ç –º–µ—Å—è—Ü
        if output_file.exists():
            try:
                logger.info(f"üì• –§–∞–π–ª {output_file} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –¥–æ–±–∞–≤–ª—è–µ–º –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –¥–∞–Ω–Ω—ã–º")
                existing_df = pd.read_parquet(output_file)
                if not existing_df.empty:
                    all_dfs.append(existing_df)
                    logger.info(f"‚úÖ –ü—Ä–æ—á–∏—Ç–∞–Ω—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ {output_file}, {len(existing_df)} —Å—Ç—Ä–æ–∫")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞ {output_file}: {e}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        for temp_file in temp_files:
            try:
                # –ß–∏—Ç–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                df = pd.read_parquet(temp_file)
                if not df.empty:
                    all_dfs.append(df)
                    logger.debug(f"‚úÖ –ü—Ä–æ—á–∏—Ç–∞–Ω —Ñ–∞–π–ª {temp_file}, {len(df)} —Å—Ç—Ä–æ–∫")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ {temp_file}: {e}")
        
        if not all_dfs:
            logger.error(f"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –∑–∞ {year}-{month}")
            return False
            
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        merged_df = pd.concat(all_dfs, ignore_index=True)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ timestamp –∏ symbol
        if merged_df.duplicated(subset=['timestamp', 'symbol']).any():
            dups_count = merged_df.duplicated(subset=['timestamp', 'symbol']).sum()
            logger.warning(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞ {year}-{month}, —É–¥–∞–ª—è–µ–º {dups_count} –∑–∞–ø–∏—Å–µ–π")
            merged_df = merged_df.drop_duplicates(subset=['timestamp', 'symbol'])
            logger.info(f"‚úÖ –£–¥–∞–ª–µ–Ω–æ {dups_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π –∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
        for attempt in range(max_retries):
            try:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
                with FileLock(str(output_file) + ".lock"):
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º PyArrow –Ω–∞–ø—Ä—è–º—É—é –¥–ª—è –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                    table = pa.Table.from_pandas(merged_df)
                    pq.write_table(table, output_file)
                logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(merged_df)} —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö –≤ {output_file}")
                
                # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                for temp_file in temp_files:
                    try:
                        temp_file.unlink()
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª {temp_file}: {e}")
                        
                return True
            except Exception as e:
                if attempt < max_retries - 1:
                    wait_time = 0.5 * (attempt + 1)
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/{max_retries}): {e}")
                    time.sleep(wait_time)
                else:
                    logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {e}")
                    return False
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∑–∞ {year}-{month}: {e}")
        return False
    if df.empty:
        logger.warning(f"‚ö†Ô∏è {symbol}: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        return []
        
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    TEMP_DIR.mkdir(parents=True, exist_ok=True)
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –º–µ—Å—è—Ü–∞–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
    temp_files = []
    
    try:
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –∫–ª—é—á–æ–º year-month –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
        df['year_month'] = df['timestamp'].apply(lambda ts: 
                                               datetime.fromtimestamp(ts/1000).strftime("%Y-%m"))
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –º–µ—Å—è—Ü–∞–º
        for month_key, month_df in df.groupby('year_month'):
            year, month = month_key.split('-')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
            if month_df['timestamp'].duplicated().any():
                logger.warning(f"‚ö†Ô∏è {symbol}: –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã timestamp –∑–∞ {month_key}, —É–¥–∞–ª—è–µ–º")
                month_df = month_df.drop_duplicates(subset=['timestamp'])
                
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–ª–æ–Ω–∫—É –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
            if 'year_month' in month_df.columns:
                month_df = month_df.drop('year_month', axis=1)
                
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            temp_file = TEMP_DIR / f"{symbol}_{year}_{month}.parquet"
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            for attempt in range(max_retries):
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º PyArrow –Ω–∞–ø—Ä—è–º—É—é –¥–ª—è –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                    table = pa.Table.from_pandas(month_df)
                    pq.write_table(table, temp_file)
                    temp_files.append(str(temp_file))
                    logger.debug(f"‚úÖ {symbol}: –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {temp_file}")
                    break
                except Exception as e:
                    if attempt < max_retries - 1:
                        wait_time = 0.5 * (attempt + 1)
                        logger.warning(f"‚ö†Ô∏è {symbol}: –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/{max_retries}): {e}")
                        time.sleep(wait_time)
                    else:
                        logger.error(f"‚ùå {symbol}: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {e}")
                        
    except Exception as e:
        logger.error(f"‚ùå {symbol}: –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {e}")
        
    return temp_files
        
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    TEMP_DIR.mkdir(parents=True, exist_ok=True)
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –º–µ—Å—è—Ü–∞–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
    temp_files = []
    
    try:
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –∫–ª—é—á–æ–º year-month –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
        df['year_month'] = df['timestamp'].apply(lambda ts: 
                                               datetime.fromtimestamp(ts/1000).strftime("%Y-%m"))
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –º–µ—Å—è—Ü–∞–º
        for month_key, month_df in df.groupby('year_month'):
            year, month = month_key.split('-')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
            if month_df['timestamp'].duplicated().any():
                logger.warning(f"‚ö†Ô∏è {symbol}: –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã timestamp –∑–∞ {month_key}, —É–¥–∞–ª—è–µ–º")
                month_df = month_df.drop_duplicates(subset=['timestamp'])
                
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–ª–æ–Ω–∫—É –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
            if 'year_month' in month_df.columns:
                month_df = month_df.drop('year_month', axis=1)
                
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            temp_file = TEMP_DIR / f"{symbol}_{year}_{month}.parquet"
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            for attempt in range(max_retries):
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º PyArrow –Ω–∞–ø—Ä—è–º—É—é –¥–ª—è –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                    table = pa.Table.from_pandas(month_df)
                    pq.write_table(table, temp_file)
                    temp_files.append(str(temp_file))
                    logger.debug(f"‚úÖ {symbol}: –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {temp_file}")
                    break
                except Exception as e:
                    if attempt < max_retries - 1:
                        wait_time = 0.5 * (attempt + 1)
                        logger.warning(f"‚ö†Ô∏è {symbol}: –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/{max_retries}): {e}")
                        time.sleep(wait_time)
                    else:
                        logger.error(f"‚ùå {symbol}: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {e}")
                        
    except Exception as e:
        logger.error(f"‚ùå {symbol}: –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {e}")
        
    return temp_files

def save_data_by_month_optimized(symbol: str, df: pd.DataFrame, max_retries: int = 3) -> bool:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–æ –º–µ—Å—è—Ü–∞–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ —Ñ–∞–π–ª–æ–≤.
    –§–æ—Ä–º–∞—Ç: /data_downloaded/year=YYYY/month=MM/data_part_MM.parquet
    
    Args:
        symbol: –°–∏–º–≤–æ–ª –¥–∞–Ω–Ω—ã—Ö
        df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
        max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ
        
    Returns:
        bool: True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ
    """
    if df.empty:
        logger.warning(f"‚ö†Ô∏è {symbol}: –Ω–µ—á–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å (–ø—É—Å—Ç–æ–π DataFrame)")
        return False
    
    try:
        # –î–æ–±–∞–≤–ª—è–µ–º datetime –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
        df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –≥–æ–¥—É –∏ –º–µ—Å—è—Ü—É
        groups = df.groupby([df['datetime'].dt.year, df['datetime'].dt.month])
        
        for (year, month), group_df in groups:
            # –£–¥–∞–ª—è–µ–º datetime –∫–æ–ª–æ–Ω–∫—É –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
            group_df = group_df.drop(columns=['datetime'])
            
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –≥–æ–¥–∞ –∏ –º–µ—Å—è—Ü–∞
            month_str = f"{month:02d}"
            target_dir = DATA_DIR / f"year={year}" / f"month={month_str}"
            target_dir.mkdir(parents=True, exist_ok=True)
            
            # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–∞–Ω–Ω—ã—Ö
            target_file = target_dir / f"data_part_{month_str}.parquet"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
            for attempt in range(max_retries):
                try:
                    if target_file.exists():
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ —á—Ç–µ–Ω–∏—è
                        with FileLock(str(target_file), timeout=30):
                            try:
                                existing_df = pd.read_parquet(target_file)
                                
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                                combined_df = pd.concat([existing_df, group_df], ignore_index=True)
                                combined_df = combined_df.drop_duplicates(
                                    subset=['timestamp', 'symbol'], 
                                    keep='last'
                                ).reset_index(drop=True)
                                
                                # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                                temp_file = target_file.with_name(f"{target_file.stem}_temp.parquet")
                                
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                                combined_df.to_parquet(temp_file, index=False)
                                
                                # –ê—Ç–æ–º–∞—Ä–Ω–æ –∑–∞–º–µ–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã–π
                                shutil.move(str(temp_file), str(target_file))
                                
                                logger.info(f"‚úÖ {symbol}: –æ–±–Ω–æ–≤–ª–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –∑–∞ {year}-{month_str}, –≤—Å–µ–≥–æ {len(combined_df)} —Å—Ç—Ä–æ–∫")
                            except Exception as e:
                                logger.warning(f"‚ö†Ô∏è {symbol}: –ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/{max_retries} –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∑–∞ {year}-{month_str} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                                if attempt == max_retries - 1:
                                    logger.error(f"‚ùå {symbol}: –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∑–∞ {year}-{month_str}: {e}")
                                    return False
                                time.sleep(1)  # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
                                continue
                    else:
                        # –î–ª—è –Ω–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                        parent_lock_file = str(target_dir)
                        with FileLock(parent_lock_file, timeout=30):
                            # –î–≤–æ–π–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞, –º–æ–≥ –ª–∏ —Ñ–∞–π–ª –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω –¥—Ä—É–≥–∏–º –ø—Ä–æ—Ü–µ—Å—Å–æ–º
                            if not target_file.exists():
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ—Ä–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –∞—Ç–æ–º–∞—Ä–Ω–æ—Å—Ç–∏
                                temp_file = target_file.with_name(f"{target_file.stem}_temp.parquet")
                                group_df.to_parquet(temp_file, index=False)
                                shutil.move(str(temp_file), str(target_file))
                                logger.info(f"‚úÖ {symbol}: —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞ {year}-{month_str}, –≤—Å–µ–≥–æ {len(group_df)} —Å—Ç—Ä–æ–∫")
                            else:
                                # –ï—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å–æ–∑–¥–∞–Ω, –ø–æ–≤—Ç–æ—Ä—è–µ–º –ø–æ–ø—ã—Ç–∫—É —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º
                                continue
                    
                    # –ï—Å–ª–∏ –¥–æ—à–ª–∏ —Å—é–¥–∞, –∑–Ω–∞—á–∏—Ç –≤—Å—ë —É—Å–ø–µ—à–Ω–æ
                    break
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è {symbol}: –ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/{max_retries} –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∑–∞ {year}-{month_str} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                    if attempt == max_retries - 1:
                        logger.error(f"‚ùå {symbol}: –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∑–∞ {year}-{month_str}: {e}")
                        return False
                    time.sleep(1)  # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå {symbol}: –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return False

def process_symbol_optimized(
    symbol: str, 
    sessions: List[HTTP], 
    start_date: datetime, 
    end_date: datetime,
    missing_timestamps: List[int] = None
) -> List[str]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —Å–∏–º–≤–æ–ª: –∑–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã.
    
    Args:
        symbol: –°–∏–º–≤–æ–ª –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        sessions: –°–ø–∏—Å–æ–∫ HTTP —Å–µ—Å—Å–∏–π
        start_date: –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞
        end_date: –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞
        missing_timestamps: –°–ø–∏—Å–æ–∫ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö timestamp –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
        
    Returns:
        List[str]: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Å–æ–∑–¥–∞–Ω–Ω—ã–º –≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ñ–∞–π–ª–∞–º
    """
    session_index = random.randint(0, len(sessions)-1)
    session = sessions[session_index]
    
    try:
        # –ï—Å–ª–∏ —É –Ω–∞—Å —Ä–µ–∂–∏–º –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
        if missing_timestamps is not None and len(missing_timestamps) > 0:
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –º–µ—Ç–∫–∏ –∏ –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –∏—Ö –≤ –ø–µ—Ä–∏–æ–¥—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
            missing_timestamps.sort()
            
            # –ï—Å–ª–∏ –ø—Ä–æ–ø—É—Å–∫–æ–≤ —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ, –Ω–µ –∏–º–µ–µ—Ç —Å–º—ã—Å–ª–∞ —Ä–∞–∑–±–∏–≤–∞—Ç—å –Ω–∞ –ø–µ—Ä–∏–æ–¥—ã
            if len(missing_timestamps) < 100:
                logger.info(f"üîπ {symbol}: –¢–æ—á–µ—á–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ {len(missing_timestamps)} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤")
                
                # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π DataFrame –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
                all_data = pd.DataFrame()
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º timestamps –≤ –¥–∞—Ç—ã –¥–ª—è –¥–µ–±–∞–≥–∞
                first_date = datetime.fromtimestamp(missing_timestamps[0]/1000)
                last_date = datetime.fromtimestamp(missing_timestamps[-1]/1000)
                logger.info(f"üìÖ {symbol}: –ò–Ω—Ç–µ—Ä–≤–∞–ª –∑–∞–≥—Ä—É–∑–∫–∏ —Å {first_date.strftime('%Y-%m-%d')} –ø–æ {last_date.strftime('%Y-%m-%d')}")
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ timestamp
                chunks = [missing_timestamps[i:i+20] for i in range(0, len(missing_timestamps), 20)]
                for i, chunk in enumerate(chunks):
                    chunk_start = datetime.fromtimestamp(chunk[0]/1000)
                    chunk_end = datetime.fromtimestamp(chunk[-1]/1000)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–π –∑–∞–ø–∞—Å –∫ –∏–Ω—Ç–µ—Ä–≤–∞–ª—É
                    chunk_start = chunk_start - timedelta(minutes=15)
                    chunk_end = chunk_end + timedelta(minutes=15)
                    
                    logger.info(f"üîÑ {symbol}: –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö {chunk_start.strftime('%Y-%m-%d %H:%M')} - {chunk_end.strftime('%Y-%m-%d %H:%M')} (—á–∞–Ω–∫ {i+1}/{len(chunks)})")
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞ –∫–æ—Ä–æ—Ç–∫–∏–π –ø–µ—Ä–∏–æ–¥
                    chunk_df = fetch_symbol_data(session, symbol, chunk_start, chunk_end)
                    
                    if chunk_df is not None and not chunk_df.empty:
                        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–æ –Ω—É–∂–Ω—ã–º timestamp
                        chunk_ts_set = set(chunk)
                        chunk_df = chunk_df[chunk_df['timestamp'].isin(chunk_ts_set)]
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –∫ –æ–±—â–µ–º—É DataFrame
                        all_data = pd.concat([all_data, chunk_df])
                
                # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –¥–∞–Ω–Ω—ã–µ, —Å–æ—Ö—Ä–∞–Ω—è–µ–º
                if not all_data.empty:
                    df = all_data
                else:
                    logger.warning(f"‚ö†Ô∏è {symbol}: –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫")
                    return []
            else:
                # –ü—Ä–∏ –±–æ–ª—å—à–æ–º —á–∏—Å–ª–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ —Ä–∞–∑–±–∏–≤–∞–µ–º –∏—Ö –Ω–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã (—á–∞–Ω–∫–∏)
                logger.info(f"üîç {symbol}: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ {len(missing_timestamps)} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏")
                
                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤ —Å–º–µ–∂–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
                chunks = group_timestamps_into_chunks(missing_timestamps)
                logger.info(f"üìä {symbol}: –°–æ–∑–¥–∞–Ω–æ {len(chunks)} –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
                
                # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π DataFrame –¥–ª—è –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
                all_data = pd.DataFrame()
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞
                for i, (chunk_start, chunk_end) in enumerate(chunks):
                    logger.info(f"üîÑ {symbol}: –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö {chunk_start.strftime('%Y-%m-%d %H:%M')} - {chunk_end.strftime('%Y-%m-%d %H:%M')} (—á–∞–Ω–∫ {i+1}/{len(chunks)})")
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª
                    chunk_df = fetch_symbol_data(session, symbol, chunk_start, chunk_end)
                    
                    if chunk_df is not None and not chunk_df.empty:
                        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–æ –Ω—É–∂–Ω—ã–º timestamp
                        missing_ts_set = set(missing_timestamps)
                        filtered_df = chunk_df[chunk_df['timestamp'].isin(missing_ts_set)]
                        
                        if not filtered_df.empty:
                            # –î–æ–±–∞–≤–ª—è–µ–º –∫ –æ–±—â–µ–º—É DataFrame
                            all_data = pd.concat([all_data, filtered_df])
                            logger.info(f"‚úÖ {symbol}: –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(filtered_df)} –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –∏–∑ —á–∞–Ω–∫–∞ {i+1}")
                        else:
                            logger.info(f"‚ÑπÔ∏è {symbol}: –í —á–∞–Ω–∫–µ {i+1} –Ω–µ—Ç —Ç—Ä–µ–±—É–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
                
                # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –¥–∞–Ω–Ω—ã–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
                if not all_data.empty:
                    df = all_data
                    logger.info(f"üîÑ {symbol}: –í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –∏–∑ {len(missing_timestamps)} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö")
                else:
                    logger.warning(f"‚ö†Ô∏è {symbol}: –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–∏–Ω –∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤")
                    return []
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞ –≤–µ—Å—å –ø–µ—Ä–∏–æ–¥
            logger.info(f"üîç {symbol}: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å {start_date.strftime('%Y-%m-%d')} –ø–æ {end_date.strftime('%Y-%m-%d')}")
            logger.info(f"üîÑ {symbol}: –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö {start_date.strftime('%Y-%m-%d')} - {end_date.strftime('%Y-%m-%d')}")
            
            df = fetch_symbol_data(session, symbol, start_date, end_date)
            
            if df is None or df.empty:
                logger.warning(f"‚ö†Ô∏è {symbol}: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–µ—Ä–∏–æ–¥")
                return []  # –°—á–∏—Ç–∞–µ–º —É—Å–ø–µ—à–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π (just no data)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –ø–æ –º–µ—Å—è—Ü–∞–º
        temp_files = save_to_temp_file(symbol, df)
        
        if temp_files:
            logger.info(f"‚úÖ {symbol}: –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤")
            return temp_files
        else:
            logger.error(f"‚ùå {symbol}: –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
            return []
            
    except Exception as e:
        logger.error(f"‚ùå {symbol}: –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
        return []

def main_optimized(start_date_str: str = "2022-01-01", end_date_str: str = "2025-07-01", symbols_limit: int = None, api_key: str = None, api_secret: str = None, incremental: bool = False):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ —Å –¥–≤—É—Ö—Ñ–∞–∑–Ω–æ–π —Å—Ö–µ–º–æ–π.
    
    Args:
        start_date_str: –°—Ç—Ä–æ–∫–∞ —Å –Ω–∞—á–∞–ª—å–Ω–æ–π –¥–∞—Ç–æ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM-DD
        end_date_str: –°—Ç—Ä–æ–∫–∞ —Å –∫–æ–Ω–µ—á–Ω–æ–π –¥–∞—Ç–æ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM-DD
        symbols_limit: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤–∞–ª—é—Ç–Ω—ã—Ö –ø–∞—Ä –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        api_key: API –∫–ª—é—á Bybit
        api_secret: API —Å–µ–∫—Ä–µ—Ç Bybit
    """
    try:
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º API –∫–ª—é—á–∏
        if not validate_api_keys():
            logger.error("‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ API –∫–ª—é—á–µ–π")
            return
            
        # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        start_time = time.time()
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        DATA_DIR.mkdir(parents=True, exist_ok=True)
            
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø–∞—Ä —Å API
        logger.info("üìã –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø–∞—Ä –Ω–∞–ø—Ä—è–º—É—é —Å Bybit API")
        symbols = fetch_bybit_markets(category="spot", save_to_file=False)
            
        if not symbols:
            logger.error("‚ùå –°–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤ –ø—É—Å—Ç, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å")
            return
            
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤–∞–ª—é—Ç –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
        if symbols_limit is not None and symbols_limit > 0:
            logger.info(f"üîç –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: –∑–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–ª—å–∫–æ {symbols_limit} –≤–∞–ª—é—Ç–Ω—ã—Ö –ø–∞—Ä –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
            symbols = symbols[:symbols_limit]
            logger.info(f"‚úÖ –°–ø–∏—Å–æ–∫ –≤–∞–ª—é—Ç–Ω—ã—Ö –ø–∞—Ä –æ–≥—Ä–∞–Ω–∏—á–µ–Ω –¥–æ {len(symbols)} –ø–∞—Ä")
            
        # –†–µ–∂–∏–º –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        if incremental:
            logger.info("üîÑ –†–µ–∂–∏–º –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏: –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ")
            
            # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –æ–∂–∏–¥–∞–µ–º—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ (15-–º–∏–Ω—É—Ç–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã)
            start_date = datetime.strptime(start_date_str, "%Y-%m-%d")
            end_date = datetime.strptime(end_date_str, "%Y-%m-%d")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥ –∏—Å–ø–æ–ª—å–∑—É—è –±—ã—Å—Ç—Ä—ã–π PyArrow –º–µ—Ç–æ–¥
            logger.info(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ —Å {start_date.strftime('%Y-%m-%d')} –ø–æ {end_date.strftime('%Y-%m-%d')}")
            missing_data_map = analyze_missing_data_fast(symbols, start_date, end_date)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –≤–∞–ª—é—Ç, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, –≥–¥–µ –µ—Å—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            symbols_with_missing_data = [symbol for symbol in symbols if missing_data_map.get(symbol, [])]
            
            if not symbols_with_missing_data:
                logger.info("‚úÖ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã, –Ω–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤")
                return
            
            logger.info(f"üîÑ –ù–∞–π–¥–µ–Ω–æ {len(symbols_with_missing_data)} –≤–∞–ª—é—Ç–Ω—ã—Ö –ø–∞—Ä —Å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
            symbols = symbols_with_missing_data
            
        # –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á–∏ –∏–∑ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        global MAINNET_API_KEYS, MAINNET_API_SECRETS
        
        # –°–æ–∑–¥–∞–µ–º –ø—É–ª HTTP —Å–µ—Å—Å–∏–π –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏
        sessions = []
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ API –∫–ª—é—á–∏
        for key, secret in zip(MAINNET_API_KEYS, MAINNET_API_SECRETS):
            try:
                session = HTTP(
                    testnet=False,
                    api_key=key,
                    api_secret=secret
                )
                sessions.append(session)
                logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ HTTP-—Å–µ—Å—Å–∏—è {len(sessions)}")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–µ—Å—Å–∏–∏: {e}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–µ—Å—Å–∏–∏ —Å–æ–∑–¥–∞–Ω—ã
        if not sessions:
            logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–π HTTP —Å–µ—Å—Å–∏–∏")
            return

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        DATA_DIR.mkdir(parents=True, exist_ok=True)
        TEMP_DIR.mkdir(parents=True, exist_ok=True)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä–∏–æ–¥ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ - —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω (01.01.2022 - 01.07.2025)
        try:
            start_date = datetime.strptime(start_date_str, "%Y-%m-%d")
            end_date = datetime.strptime(end_date_str, "%Y-%m-%d")
        except ValueError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–∞—Ç: {e}")
            logger.info("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É—é —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω: 01.01.2022 - 01.07.2025")
            start_date = datetime.strptime("2022-01-01", "%Y-%m-%d")
            end_date = datetime.strptime("2025-07-01", "%Y-%m-%d")
        
        logger.info(f"üìÖ –ü–µ—Ä–∏–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏: {start_date.strftime('%Y-%m-%d')} - {end_date.strftime('%Y-%m-%d')}")
        logger.info(f"üî¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏: {len(symbols)}")
        
        # –§–∞–∑–∞ 1: –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        logger.info("üìä –§–ê–ó–ê 1: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã")
        temp_files_by_month = {}  # –°–ª–æ–≤–∞—Ä—å –º–µ—Å—è—Ü -> —Å–ø–∏—Å–æ–∫ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—á–µ—Ç—á–∏–∫–æ–≤ –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
        temp_files_by_month = {}  # –°–ª–æ–≤–∞—Ä—å –º–µ—Å—è—Ü -> —Å–ø–∏—Å–æ–∫ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        processed_count = 0       # –°—á–µ—Ç—á–∏–∫ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –≤–∞–ª—é—Ç
        batch_size = 10           # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
        success_count = 0
        error_count = 0
        processed_files = set()   # –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        
        # –°–æ–∑–¥–∞–µ–º –ø—É–ª –º—É–ª—å—Ç–∏–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞
        with ThreadPoolExecutor(max_workers=multiprocessing.cpu_count()) as executor:
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ—Ç–æ–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
            futures = {}
            for symbol in symbols:
                # –í —Ä–µ–∂–∏–º–µ –¥–æ–∑–∞–≥—Ä—É–∑–∫–∏ –ø–µ—Ä–µ–¥–∞—ë–º —Å–ø–∏—Å–æ–∫ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö timestamp
                if incremental and symbol in missing_data_map:
                    future = executor.submit(
                        process_symbol_optimized,
                        symbol=symbol,
                        sessions=sessions,
                        start_date=start_date,
                        end_date=end_date,
                        missing_timestamps=missing_data_map[symbol]
                    )
                else:
                    future = executor.submit(
                        process_symbol_optimized,
                        symbol=symbol,
                        sessions=sessions,
                        start_date=start_date,
                        end_date=end_date
                    )
                futures[future] = symbol
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ–º
            for future, symbol in futures.items():
                try:
                    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –æ—Ç –∑–∞–¥–∞—á–∏
                    temp_files = future.result()
                    
                    if temp_files:
                        success_count += 1
                        
                        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ –º–µ—Å—è—Ü–∞–º
                        for temp_file in temp_files:
                            path = Path(temp_file)
                            filename = path.name
                            parts = filename.split('_')
                            if len(parts) >= 3:
                                year, month = parts[-2], parts[-1].split('.')[0]
                                month_key = f"{year}-{month}"
                                
                                if month_key not in temp_files_by_month:
                                    temp_files_by_month[month_key] = []
                                    
                                temp_files_by_month[month_key].append(temp_file)
                        
                        processed_count += 1
                        
                        # –ö–∞–∂–¥—ã–µ batch_size —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –≤–∞–ª—é—Ç –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                        if processed_count % batch_size == 0:
                            logger.info(f"üìä –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ {processed_count} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –≤–∞–ª—é—Ç")
                            
                            # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
                            success_merge = 0
                            error_merge = 0
                            processed_month_keys = []
                            
                            for month_key, files in temp_files_by_month.items():
                                # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –µ—â–µ –Ω–µ –±—ã–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã
                                new_files = [f for f in files if f not in processed_files]
                                if not new_files:
                                    continue
                                    
                                year, month = month_key.split('-')
                                logger.info(f"üìÖ –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞ {year}-{month}: {len(new_files)} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
                                
                                if merge_temp_files_to_monthly(year, month):
                                    success_merge += 1
                                    # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–æ
                                    processed_files.update(new_files)
                                    processed_month_keys.append(month_key)
                                else:
                                    error_merge += 1
                            
                            logger.info(f"‚úÖ –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {success_merge} –º–µ—Å—è—Ü–µ–≤ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ")
                    else:
                        error_count += 1
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {symbol}: {e}")
                    error_count += 1
            
            logger.info(f"‚úÖ –§–∞–∑–∞ 1 –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {success_count} —É—Å–ø–µ—à–Ω–æ, {error_count} —Å –æ—à–∏–±–∫–∞–º–∏")
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Ñ–∞–∑–∞: –æ–±—ä–µ–¥–∏–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            remaining_files = False
            for month_key, files in temp_files_by_month.items():
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –æ—Å—Ç–∞–ª–∏—Å—å –ª–∏ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                new_files = [f for f in files if f not in processed_files]
                if new_files:
                    remaining_files = True
                    break
                    
            if remaining_files:
                logger.info("üìä –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤")
                
                success_merge = 0
                error_merge = 0
                
                for month_key, files in temp_files_by_month.items():
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –µ—â–µ –Ω–µ –±—ã–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã
                    if any(f not in processed_files for f in files):
                        year, month = month_key.split('-')
                        logger.info(f"üìÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞ {year}-{month}")
                        
                        if merge_temp_files_to_monthly(year, month):
                            success_merge += 1
                        else:
                            error_merge += 1
                
                logger.info(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {success_merge} –º–µ—Å—è—Ü–µ–≤ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ, {error_merge} —Å –æ—à–∏–±–∫–∞–º–∏")
            else:
                logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≤ –∏—Ç–æ–≥–æ–≤—ã–µ —Ñ–∞–π–ª—ã")

            end_time = time.time()
            elapsed = end_time - start_time
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed:.2f} —Å–µ–∫—É–Ω–¥")
            logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —É—Å–ø–µ—à–Ω–æ {success_count}, —Å –æ—à–∏–±–∫–∞–º–∏ {error_count}")
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ –∫–æ–Ω—Ü–µ –∑–∞–≥—Ä—É–∑–∫–∏
            if 'modules_imported' in globals() and modules_imported:
                logger.info("üîç –ó–∞–ø—É—Å–∫ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã...")
                try:
                    stats = check_and_fix_duplicates(str(DATA_DIR), fix=True)
                    logger.info(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
                    logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏: –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ {stats['total_files']} —Ñ–∞–π–ª–æ–≤, –Ω–∞–π–¥–µ–Ω–æ {stats['files_with_duplicates']} —Ñ–∞–π–ª–æ–≤ —Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏, —É–¥–∞–ª–µ–Ω–æ {stats['duplicates_removed']} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑ {stats['total_rows']} —Å—Ç—Ä–æ–∫")
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {e}")
            else:
                logger.warning("‚ö†Ô∏è –ú–æ–¥—É–ª—å –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")

        logger.info("‚ú® –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")

def signal_handler(signum, frame):
    logger.info("üõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
    sys.exit(0)


def analyze_missing_data_fast(symbols: List[str], start_date: datetime, end_date: datetime) -> Dict[str, List[int]]:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ –¥–∞—Ç, –∏—Å–ø–æ–ª—å–∑—É—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Å PyArrow.
    
    Args:
        symbols: –°–ø–∏—Å–æ–∫ –≤–∞–ª—é—Ç–Ω—ã—Ö –ø–∞—Ä
        start_date: –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ –ø–µ—Ä–∏–æ–¥–∞
        end_date: –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ –ø–µ—Ä–∏–æ–¥–∞
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å, –≥–¥–µ –∫–ª—é—á - —Å–∏–º–≤–æ–ª, –∑–Ω–∞—á–µ–Ω–∏–µ - —Å–ø–∏—Å–æ–∫ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö timestamp
    """
    import pyarrow.dataset as ds
    import pyarrow.compute as pc
    
    logger.info(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ —Å {start_date.strftime('%Y-%m-%d')} –ø–æ {end_date.strftime('%Y-%m-%d')}")
    
    start_ts = int(start_date.timestamp() * 1000)
    end_ts = int(end_date.timestamp() * 1000)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–∞–Ω–Ω—ã—Ö
    if not DATA_DIR.exists():
        # –ï—Å–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –Ω–µ—Ç, –≤—Å–µ –º–µ—Ç–∫–∏ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤
        logger.warning("‚ö†Ô∏è –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–∞–Ω–Ω—ã—Ö –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –±—É–¥–µ–º –∑–∞–≥—Ä—É–∂–∞—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ")
        expected_timestamps = list(range(start_ts, end_ts + 1, 15 * 60 * 1000))
        return {symbol: expected_timestamps for symbol in symbols}
    
    try:
        # 1Ô∏è‚É£ –õ–µ–Ω–∏–≤–∞—è ¬´—Å–∫–ª–µ–π–∫–∞¬ª –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
        logger.info("üîç –°–æ–∑–¥–∞–Ω–∏–µ dataset –∏–∑ parquet-—Ñ–∞–π–ª–æ–≤ (—Ç–æ–ª—å–∫–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)...")
        dataset = ds.dataset(str(DATA_DIR), format="parquet")  # —á–∏—Ç–∞–µ—Ç –¢–û–õ–¨–ö–û –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        
        # 2Ô∏è‚É£ –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ä–∞–∑—É –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–∫–∞–Ω–∞
        logger.info("üîç –°–æ–∑–¥–∞–Ω–∏–µ —Å–∫–∞–Ω–µ—Ä–∞ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏...")
        scanner = dataset.scanner(
            columns=["timestamp", "symbol"],
            filter=(
                (pc.field("timestamp") >= start_ts) &
                (pc.field("timestamp") <= end_ts)
            )
        )
        
        # 3Ô∏è‚É£ –û–¥–Ω–∏–º –≤—ã–∑–æ–≤–æ–º –ø–æ–ª—É—á–∞–µ–º Arrow-—Ç–∞–±–ª–∏—Ü—É
        logger.info("üìä –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ parquet-—Ñ–∞–π–ª–æ–≤ –Ω–∞ –Ω–∏–∑–∫–æ–º —É—Ä–æ–≤–Ω–µ —á–µ—Ä–µ–∑ PyArrow...")
        tbl = scanner.to_table()  # —á–∏—Ç–∞–µ—Ç —Ä–æ–≤–Ω–æ –¥–≤–∞ —Å—Ç–æ–ª–±—Ü–∞, –º–∏–Ω—É—è pandas
        logger.info(f"‚úÖ –ü—Ä–æ—á–∏—Ç–∞–Ω–æ {tbl.num_rows} –∑–∞–ø–∏—Å–µ–π –∏–∑ –≤—Å–µ—Ö parquet-—Ñ–∞–π–ª–æ–≤")
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤–æ–æ–±—â–µ
        if tbl.num_rows == 0:
            logger.warning("‚ö†Ô∏è –í —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–∞—Ö –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥")
            expected_timestamps = list(range(start_ts, end_ts + 1, 15 * 60 * 1000))
            return {symbol: expected_timestamps for symbol in symbols}
        
        have = {}
        try:
            # 4Ô∏è‚É£ –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å C-–∞–≥—Ä–µ–≥–∞—Ü–∏—é hash_set
            logger.info("üîÑ –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º hash_set –∞–≥—Ä–µ–≥–∞—Ü–∏–∏...")
            groups = (
                tbl.group_by("symbol")
                   .aggregate([("timestamp", "hash_set")])
                   .to_pydict()
            )
            have = {sym: set(ts) for sym, ts in zip(groups["symbol"], groups["timestamp_hash_set"])}
        except Exception as e:
            # –ï—Å–ª–∏ hash_set –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º pandas
            logger.warning(f"‚ö†Ô∏è –ê–≥—Ä–µ–≥–∞—Ü–∏—è hash_set –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è ({str(e)}), –∏—Å–ø–æ–ª—å–∑—É–µ–º pandas")
            logger.info("üîÑ –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ pandas...")
            
            df = tbl.to_pandas()
            for symbol, group in df.groupby('symbol'):
                have[symbol] = set(group['timestamp'].unique())
        
        # 5Ô∏è‚É£ –î–ª—è –∫–∞–∂–¥–æ–π –≤–∞–ª—é—Ç—ã –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∞–ª—å–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω —Ç–æ—Ä–≥–æ–≤–ª–∏
        logger.info("üìÖ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤ —Ç–æ—Ä–≥–æ–≤–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –≤–∞–ª—é—Ç—ã...")
        
        # –î–ª—è –∫–∞–∂–¥–æ–π –≤–∞–ª—é—Ç—ã –Ω–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤—ã–π –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π timestamp
        real_ranges = {}
        
        # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –≤–∞–ª—é—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ –¥–∞–Ω–Ω—ã—Ö
        for sym in have.keys():
            if sym in symbols and have[sym]:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
                sym_min = min(have[sym])
                sym_max = max(have[sym])
                
                # –§–∏–∫—Å–∏—Ä—É–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –≥–ª–æ–±–∞–ª—å–Ω—ã–º–∏ —Ä–∞–º–∫–∞–º–∏
                actual_start = max(sym_min, start_ts)
                actual_end = min(sym_max, end_ts)
                
                real_ranges[sym] = (actual_start, actual_end)
                logger.info(f"üìÜ {sym}: –ø–µ—Ä–∏–æ–¥ —Ç–æ—Ä–≥–æ–≤–ª–∏ —Å {datetime.fromtimestamp(sym_min/1000).strftime('%Y-%m-%d')} –ø–æ {datetime.fromtimestamp(sym_max/1000).strftime('%Y-%m-%d')}")
        
        # 6Ô∏è‚É£ –°–ø–∏—Å–æ–∫ –æ–∂–∏–¥–∞–Ω–∏–π –¥–µ–ª–∞–µ–º –æ–¥–∏–Ω —Ä–∞–∑ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
        logger.info("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ —Å —É—á–µ—Ç–æ–º —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤ —Ç–æ—Ä–≥–æ–≤–ª–∏...")
        
        expected = {}
        for sym in symbols:
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω - –∏—â–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤–Ω—É—Ç—Ä–∏ –Ω–µ–≥–æ
            if sym in real_ranges:
                actual_start, actual_end = real_ranges[sym]
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–∏ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
                missing = [t for t in range(actual_start, actual_end + 1, 15 * 60 * 1000)
                          if t not in have.get(sym, set())]
                
                if missing:
                    expected[sym] = missing
            # –ï—Å–ª–∏ –≤–æ–æ–±—â–µ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ —ç—Ç–æ–π –≤–∞–ª—é—Ç–µ - —Å–∫–∞—á–∏–≤–∞–µ–º –∑–∞ –≤–µ—Å—å –ø–µ—Ä–∏–æ–¥
            elif sym not in have or not have[sym]:
                expected[sym] = list(range(start_ts, end_ts + 1, 15 * 60 * 1000))
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–∏–º–≤–æ–ª—ã, —É –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –º–µ—Ç–∫–∏
        result = {k: v for k, v in expected.items() if v}
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        for sym, missing in result.items():
            all_expected = list(range(start_ts, end_ts + 1, 15 * 60 * 1000))
            logger.info(f"‚ö†Ô∏è {sym}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç {len(missing)} –∏–∑ {len(all_expected)} –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ ({len(missing)/len(all_expected)*100:.1f}%)")
            
        return result
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ PyArrow: {str(e)}")
        logger.warning("‚ö†Ô∏è –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ (–º–µ–¥–ª–µ–Ω–Ω–µ–µ)")
        return analyze_missing_data(symbols, start_date, end_date)

def analyze_missing_data(symbols: List[str], start_date: datetime, end_date: datetime) -> Dict[str, List[int]]:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ –¥–∞—Ç.
    
    Args:
        symbols: –°–ø–∏—Å–æ–∫ –≤–∞–ª—é—Ç–Ω—ã—Ö –ø–∞—Ä
        start_date: –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ –ø–µ—Ä–∏–æ–¥–∞
        end_date: –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ –ø–µ—Ä–∏–æ–¥–∞
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å, –≥–¥–µ –∫–ª—é—á - —Å–∏–º–≤–æ–ª, –∑–Ω–∞—á–µ–Ω–∏–µ - —Å–ø–∏—Å–æ–∫ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö timestamp
    """
    logger.info(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ —Å {start_date.strftime('%Y-%m-%d')} –ø–æ {end_date.strftime('%Y-%m-%d')}")
    
    # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ –≤ 15-–º–∏–Ω—É—Ç–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞—Ö
    expected_timestamps = []
    current_date = start_date
    while current_date <= end_date:
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–Ω—è —Å–æ–∑–¥–∞–µ–º 15-–º–∏–Ω—É—Ç–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã (96 –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –≤ –¥–µ–Ω—å)
        for hour in range(24):
            for minute in range(0, 60, 15):
                dt = current_date.replace(hour=hour, minute=minute, second=0, microsecond=0)
                ts = int(dt.timestamp() * 1000)  # –í –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö –∫–∞–∫ –≤ –ë–∏–±–∏—Ç
                expected_timestamps.append(ts)
        current_date += timedelta(days=1)
    
    logger.info(f"üìÜ –°–æ–∑–¥–∞–Ω–æ {len(expected_timestamps)} –æ–∂–∏–¥–∞–µ–º—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫")
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –≤ data_downloaded
    result = {}
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ä–∫–µ—Ç-—Ñ–∞–π–ª—ã
    parquet_files = []
    for year_dir in DATA_DIR.glob("year=*"):
        for month_dir in year_dir.glob("month=*"):
            for parquet_file in month_dir.glob("*.parquet"):
                parquet_files.append(parquet_file)
    
    if not parquet_files:
        # –ï—Å–ª–∏ —Ñ–∞–π–ª–æ–≤ –Ω–µ—Ç, –≤—Å–µ –º–µ—Ç–∫–∏ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤
        logger.warning("‚ö†Ô∏è –ù–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö, –±—É–¥–µ–º –∑–∞–≥—Ä—É–∂–∞—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ")
        for symbol in symbols:
            result[symbol] = expected_timestamps
        return result
    
    logger.info(f"üóÉÔ∏è –ù–∞–π–¥–µ–Ω–æ {len(parquet_files)} parquet-—Ñ–∞–π–ª–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º —Å–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π (timestamp, symbol)
    existing_data = set()
    
    # –ß–∏—Ç–∞–µ–º –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
    for i, parquet_file in enumerate(parquet_files):
        try:
            if i % 10 == 0:
                logger.info(f"üìà –ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{len(parquet_files)} ({i/len(parquet_files)*100:.1f}%)")
                
            df = pd.read_parquet(parquet_file)
            if df.empty:
                continue
                
            # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ timestamp-symbol –≤ —Å–µ—Ç
            for _, row in df.iterrows():
                existing_data.add((row['timestamp'], row['symbol']))
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {parquet_file}: {e}")
    
    logger.info(f"üìä –ò–∑ parquet-—Ñ–∞–π–ª–æ–≤ –ø–æ–ª—É—á–µ–Ω–æ {len(existing_data)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π (timestamp, symbol)")
    
    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
    for symbol in symbols:
        # –ù–∞—Ö–æ–¥–∏–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è —ç—Ç–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
        missing_timestamps = [
            ts for ts in expected_timestamps 
            if (ts, symbol) not in existing_data
        ]
        
        if missing_timestamps:
            result[symbol] = missing_timestamps
            logger.info(f"‚ö†Ô∏è {symbol}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç {len(missing_timestamps)} –∏–∑ {len(expected_timestamps)} –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ ({len(missing_timestamps)/len(expected_timestamps)*100:.1f}%)")
    
    return result
    return result

logger.info("‚ú® –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        
def parse_args():
    """–ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    parser = argparse.ArgumentParser(description="–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö Bybit API")
    parser.add_argument(
        "--start-date",
        type=str,
        default="2022-01-01",
        help="–ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM-DD (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 2022-01-01)"
    )
    parser.add_argument(
        "--end-date",
        type=str,
        default="2025-07-01",
        help="–ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYY-MM-DD (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 2025-07-01)"
    )
    parser.add_argument(
        "--markets-only",
        action="store_true",
        help="–¢–æ–ª—å–∫–æ –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Ä—ã–Ω–∫–æ–≤ –±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö"
    )
    parser.add_argument(
        "--incremental",
        action="store_true",
        help="–†–µ–∂–∏–º –¥–æ–∑–∞–≥—Ä—É–∑–∫–∏: –∑–∞–≥—Ä—É–∂–∞—Ç—å —Ç–æ–ª—å–∫–æ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ (15-–º–∏–Ω—É—Ç–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã)"
    )
    parser.add_argument(
        "--symbols-limit",
        type=int,
        default=None,
        help="–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –≤–∞–ª—é—Ç–Ω—ã—Ö –ø–∞—Ä –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)"
    )
    parser.add_argument(
        "--api-key",
        type=str,
        default=None,
        help="API –∫–ª—é—á Bybit (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)"
    )
    parser.add_argument(
        "--api-secret",
        type=str,
        default=None,
        help="API —Å–µ–∫—Ä–µ—Ç Bybit (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)"
    )
    
    return parser.parse_args()

# ================== –¢–û–ß–ö–ê –í–•–û–î–ê ==================
if __name__ == "__main__":
    # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    args = parse_args()
    
    if args.markets_only:
        # –†–µ–∂–∏–º —Ç–æ–ª—å–∫–æ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –ø–∞—Ä
        logger.info("üìã –ó–∞–ø—É—Å–∫ –≤ —Ä–µ–∂–∏–º–µ –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–∫–∞ –ø–∞—Ä")
        symbols = fetch_bybit_markets(category="spot", save_to_file=True)
        logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(symbols)} –ø–∞—Ä")
    else:
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–µ–∂–∏–º –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        main_optimized(
            start_date_str=args.start_date,
            end_date_str=args.end_date,
            symbols_limit=args.symbols_limit,
            api_key=args.api_key,
            api_secret=args.api_secret,
            incremental=args.incremental
        )
