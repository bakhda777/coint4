#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–ú–æ–¥—É–ª—å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ parquet-—Ñ–∞–π–ª–∞—Ö.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –º–∞—Å—Å–æ–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏.
"""

import os
import logging
from pathlib import Path
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from typing import List, Set, Tuple, Dict
import time

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
logger = logging.getLogger()

class ParquetDuplicatesChecker:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ parquet-—Ñ–∞–π–ª–∞—Ö."""
    
    def __init__(self, data_dir: str = "data_downloaded"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–µ—Ä—â–∏–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.
        
        Args:
            data_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        """
        self.data_dir = Path(data_dir)
        
    def scan_parquet_files(self) -> List[Path]:
        """
        –°–∫–∞–Ω–∏—Ä—É–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö parquet-—Ñ–∞–π–ª–æ–≤.
        
        Returns:
            List[Path]: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ parquet-—Ñ–∞–π–ª–∞–º
        """
        logger.info(f"üîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ parquet-—Ñ–∞–π–ª–æ–≤ –≤ {self.data_dir}")
        parquet_files = list(self.data_dir.glob("**/data_part_*.parquet"))
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(parquet_files)} parquet-—Ñ–∞–π–ª–æ–≤")
        return parquet_files
    
    def check_file_for_duplicates(self, file_path: Path) -> Tuple[bool, int, int]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–¥–∏–Ω —Ñ–∞–π–ª –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.
        
        Args:
            file_path: –ü—É—Ç—å –∫ parquet-—Ñ–∞–π–ª—É
            
        Returns:
            Tuple[bool, int, int]: (–µ—Å—Ç—å –ª–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–æ –æ—á–∏—Å—Ç–∫–∏, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)
        """
        try:
            # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
            df = pd.read_parquet(file_path)
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–æ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            initial_size = len(df)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            if 'timestamp' not in df.columns or 'symbol' not in df.columns:
                logger.warning(f"‚ö†Ô∏è {file_path}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏")
                return False, initial_size, 0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
            duplicates = df.duplicated(subset=['timestamp', 'symbol'])
            duplicate_count = duplicates.sum()
            
            # –ï—Å–ª–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if duplicate_count == 0:
                return False, initial_size, 0
            
            return True, initial_size, duplicate_count
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return False, 0, 0
    
    def remove_duplicates_from_file(self, file_path: Path) -> bool:
        """
        –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ —Ñ–∞–π–ª–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
        
        Args:
            file_path: –ü—É—Ç—å –∫ parquet-—Ñ–∞–π–ª—É
            
        Returns:
            bool: True –µ—Å–ª–∏ –æ–ø–µ—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞
        """
        try:
            # –°–æ–∑–¥–∞–µ–º –∏–º—è –¥–ª—è —Ñ–∞–π–ª–∞ –±—ç–∫–∞–ø–∞
            backup_path = file_path.with_name(f"{file_path.stem}_backup.parquet")
            
            # –°–æ–∑–¥–∞–µ–º –±—ç–∫–∞–ø —Ç–µ–∫—É—â–µ–≥–æ —Ñ–∞–π–ª–∞
            if not os.path.exists(backup_path):
                os.rename(file_path, backup_path)
            
            # –ß–∏—Ç–∞–µ–º –∏–∑ –±—ç–∫–∞–ø–∞
            df = pd.read_parquet(backup_path)
            initial_size = len(df)
            
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            df = df.drop_duplicates(subset=['timestamp', 'symbol'], keep='last').reset_index(drop=True)
            final_size = len(df)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ
            df.to_parquet(file_path, index=False)
            
            logger.info(f"‚úÖ {file_path}: —É–¥–∞–ª–µ–Ω–æ {initial_size - final_size} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, –æ—Å—Ç–∞–ª–æ—Å—å {final_size} —Å—Ç—Ä–æ–∫")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑ {file_path}: {e}")
            if os.path.exists(backup_path) and not os.path.exists(file_path):
                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–∑ –±—ç–∫–∞–ø–∞ –µ—Å–ª–∏ —Ñ–∞–π–ª –ø–æ–≤—Ä–µ–∂–¥–µ–Ω
                os.rename(backup_path, file_path)
            return False
    
    def check_all_files(self, fix_duplicates: bool = True) -> Dict[str, int]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —É–¥–∞–ª—è–µ—Ç –∏—Ö.
        
        Args:
            fix_duplicates: –£–¥–∞–ª—è—Ç—å –ª–∏ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
            
        Returns:
            Dict[str, int]: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ {–≤—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤, —Ñ–∞–π–ª–æ–≤ —Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏, –≤—Å–µ–≥–æ —Å—Ç—Ä–æ–∫, —É–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤}
        """
        start_time = time.time()
        logger.info("üîç –ù–∞—á–∞–ª–æ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–π–ª–æ–≤ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã")
        
        parquet_files = self.scan_parquet_files()
        files_with_duplicates = 0
        total_rows = 0
        total_duplicates = 0
        
        for i, file_path in enumerate(parquet_files):
            # –í—ã–≤–æ–¥–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            if i % 10 == 0:
                logger.info(f"üìä –ü—Ä–æ–≥—Ä–µ—Å—Å: {i}/{len(parquet_files)} ({i/len(parquet_files)*100:.1f}%)")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª
            has_duplicates, row_count, duplicate_count = self.check_file_for_duplicates(file_path)
            total_rows += row_count
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –Ω—É–∂–Ω–æ –∏—Ö —É–¥–∞–ª–∏—Ç—å
            if has_duplicates and fix_duplicates:
                files_with_duplicates += 1
                total_duplicates += duplicate_count
                success = self.remove_duplicates_from_file(file_path)
                if not success:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ {file_path}")
            elif has_duplicates:
                files_with_duplicates += 1
                total_duplicates += duplicate_count
                logger.info(f"‚ö†Ô∏è {file_path}: –Ω–∞–π–¥–µ–Ω–æ {duplicate_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        
        elapsed_time = time.time() - start_time
        logger.info(f"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed_time:.1f} —Å–µ–∫—É–Ω–¥")
        logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ {len(parquet_files)} —Ñ–∞–π–ª–æ–≤, –Ω–∞–π–¥–µ–Ω–æ {files_with_duplicates} —Ñ–∞–π–ª–æ–≤ —Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏")
        logger.info(f"üìä –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {total_rows}, —É–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {total_duplicates}")
        
        return {
            "total_files": len(parquet_files),
            "files_with_duplicates": files_with_duplicates,
            "total_rows": total_rows,
            "duplicates_removed": total_duplicates
        }
        
def check_and_fix_duplicates(data_dir: str = "data_downloaded", fix: bool = True) -> Dict[str, int]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏ —É–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –≤–æ –≤—Å–µ—Ö parquet-—Ñ–∞–π–ª–∞—Ö –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.
    
    Args:
        data_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏
        fix: –£–¥–∞–ª—è—Ç—å –ª–∏ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
        
    Returns:
        Dict[str, int]: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏
    """
    checker = ParquetDuplicatesChecker(data_dir)
    return checker.check_all_files(fix_duplicates=fix)

if __name__ == "__main__":
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å–∫—Ä–∏–ø—Ç–∞ –Ω–∞–ø—Ä—è–º—É—é
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s"
    )
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É
    check_and_fix_duplicates()
