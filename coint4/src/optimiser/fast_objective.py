"""
–ë—ã—Å—Ç—Ä–∞—è —Ü–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è Optuna.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–µ –ø–∞—Ä—ã –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.
"""

import optuna
import pandas as pd
import numpy as np
from pathlib import Path
import yaml
import logging
import os
import math
import threading
import multiprocessing

from coint2.utils.config import load_config
from coint2.core.data_loader import DataHandler, load_master_dataset
from coint2.engine.numba_engine import NumbaPairBacktester
from coint2.engine.optimized_backtest_engine import OptimizedPairBacktester
from coint2.engine.numba_backtest_engine_full import FullNumbaPairBacktester
# –£–°–ö–û–†–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ü–û–õ–ù–û–°–¢–¨–Æ Numba-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–≤–∏–∂–æ–∫ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è
PairBacktester = FullNumbaPairBacktester
from coint2.core.portfolio import Portfolio
from coint2.core.math_utils import calculate_ssd
from coint2.pipeline.filters import filter_pairs_by_coint_and_half_life
from coint2.utils.pairs_loader import load_pair_tuples
from coint2.core.normalization_improvements import preprocess_and_normalize_data, compute_normalization_params, apply_normalization_with_params
from coint2.utils.logging_utils import get_logger
from coint2.utils.time_utils import ensure_datetime_index
from .metric_utils import extract_sharpe, normalize_params, validate_params
from .lookahead_validator import LookaheadValidator, create_temporal_validator
from .components.universe_manager import UniverseManager
# –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª–æ–≤—ã–π –∫—ç—à —Å filelock
from .components.file_cache_cross import CrossPlatformFileCache as FileCache, DummyLock
from .sharpe_validator import SharpeValidator, create_sharpe_validator
from .annualization import get_annualization_factor, calculate_sharpe_ratio

# –£–°–ö–û–†–ï–ù–ò–ï: –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫—ç—à–∞ rolling-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫
from coint2.core.global_rolling_cache import initialize_global_rolling_cache, cleanup_global_rolling_cache
from coint2.core.memory_optimization import initialize_global_price_data, determine_required_windows

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏–∑ –µ–¥–∏–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
from .constants import PENALTY, PENALTY_SOFT, PENALTY_HARD, MIN_TRADES_THRESHOLD, MAX_DRAWDOWN_SOFT_THRESHOLD, MAX_DRAWDOWN_HARD_THRESHOLD, \
    WIN_RATE_BONUS_THRESHOLD, WIN_RATE_PENALTY_THRESHOLD, DD_PENALTY_SOFT_MULTIPLIER, DD_PENALTY_HARD_MULTIPLIER, \
    WIN_RATE_BONUS_MULTIPLIER, WIN_RATE_PENALTY_MULTIPLIER, INTERMEDIATE_REPORT_INTERVAL

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
logger = logging.getLogger(__name__)

def convert_hours_to_periods(hours: float, bar_minutes: int) -> int:
    """
    Convert hours to number of periods based on bar timeframe.
    –ò—Å–ø–æ–ª—å–∑—É–µ–º ceil –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –æ–∫—Ä—É–≥–ª–µ–Ω–∏—è –≤–≤–µ—Ä—Ö.
    """
    if hours <= 0:
        return 0
    return int(math.ceil(hours * 60 / bar_minutes))

def _coerce_float(value, default: float) -> float:
    """Safely coerce config values to float for test/mocked configs."""
    if isinstance(value, (int, float, np.integer, np.floating)):
        return float(value)
    return default

def _coerce_int(value, default: int) -> int:
    """Safely coerce config values to int for test/mocked configs."""
    if isinstance(value, (int, np.integer)):
        return int(value)
    if isinstance(value, float) and value.is_integer():
        return int(value)
    return default

def _coerce_bool(value, default: bool) -> bool:
    """Safely coerce config values to bool for test/mocked configs."""
    if isinstance(value, (bool, np.bool_)):
        return bool(value)
    return default

def _coerce_str(value, default: str) -> str:
    """Safely coerce config values to str for test/mocked configs."""
    return value if isinstance(value, str) else default


def _clean_step_dataframe(
    step_df: pd.DataFrame,
    base_config,
    *,
    drop_columns: bool = True,
    fill_missing: bool = True,
) -> pd.DataFrame:
    """Normalize step dataframe index/order and apply light missing-data cleanup."""
    cleaned = ensure_datetime_index(step_df)
    if cleaned.index.has_duplicates:
        cleaned = cleaned[~cleaned.index.duplicated(keep="last")]

    if fill_missing:
        fill_limit_pct = getattr(getattr(base_config, "backtest", None), "fill_limit_pct", None)
        if fill_limit_pct is not None:
            try:
                fill_limit = max(1, int(len(cleaned) * float(fill_limit_pct)))
            except (TypeError, ValueError):
                fill_limit = 0
            if fill_limit > 0:
                limit = min(fill_limit, 5)
                cleaned = cleaned.ffill(limit=limit)

    if drop_columns:
        nan_threshold = getattr(getattr(base_config, "data_processing", None), "nan_threshold", None)
        if nan_threshold is None:
            nan_threshold = 0.5
        try:
            drop_threshold = int(len(cleaned) * (1 - float(nan_threshold)))
        except (TypeError, ValueError):
            drop_threshold = 0
        if drop_threshold > 0:
            cleaned = cleaned.dropna(axis=1, thresh=drop_threshold)

    return cleaned


def _pairs_df_to_tuples(step_pairs: pd.DataFrame) -> list[tuple[str, str]]:
    """Convert pair dataframe into list of (s1, s2) tuples for universe checks."""
    if step_pairs is None or step_pairs.empty:
        return []
    return list(step_pairs[["s1", "s2"]].itertuples(index=False, name=None))


def _resolve_step_size_days(cfg) -> int:
    """Resolve step size from config, with refit_frequency fallback."""
    step_size_days = getattr(cfg.walk_forward, "step_size_days", None)
    if step_size_days is None or step_size_days <= 0:
        refit_frequency = getattr(cfg.walk_forward, "refit_frequency", None)
        refit_map = {
            "daily": 1,
            "weekly": 7,
            "monthly": 30,
        }
        key = str(refit_frequency).lower() if refit_frequency is not None else ""
        step_size_days = refit_map.get(key, cfg.walk_forward.testing_period_days)
    try:
        step_size_days = int(step_size_days)
    except (TypeError, ValueError):
        step_size_days = int(cfg.walk_forward.testing_period_days)
    return step_size_days

class FastWalkForwardObjective:
    """
    –ë—ã—Å—Ç—Ä–∞—è —Ü–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    –Ω–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞—Ö.
    """
    
    def __init__(self, base_config_path: str, search_space_path: str):
        self.base_config = load_config(base_config_path)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞
        with open(search_space_path, 'r') as f:
            self.search_space = yaml.safe_load(f)

        # –£–°–ö–û–†–ï–ù–ò–ï: –ö—ç—à –¥–ª—è –æ—Ç–±–æ—Ä–∞ –ø–∞—Ä –ø–æ –ø–µ—Ä–∏–æ–¥–∞–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
        self.pair_selection_cache = {}

        # –ü–û–¢–û–ö–û–ë–ï–ó–û–ü–ê–°–ù–û–°–¢–¨: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∞–π–ª–æ–≤—ã–π –∫—ç—à –≤–º–µ—Å—Ç–æ Manager
        # –§–∞–π–ª–æ–≤—ã–π –∫—ç—à –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –º–µ–∂–ø—Ä–æ—Ü–µ—Å—Å–Ω—É—é —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã
        optuna_cfg = getattr(self.base_config, "optuna", None)
        if isinstance(optuna_cfg, dict):
            n_jobs = optuna_cfg.get("n_jobs", 1)
        elif optuna_cfg is not None:
            n_jobs = getattr(optuna_cfg, "n_jobs", 1)
        else:
            n_jobs = 1
        try:
            n_jobs = int(n_jobs)
        except (TypeError, ValueError):
            n_jobs = 1
        
        if n_jobs > 1:
            # –ú–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–Ω—ã–π —Ä–µ–∂–∏–º - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∞–π–ª–æ–≤—ã–π –∫—ç—à
            print(f"üîÑ –ú–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–Ω—ã–π —Ä–µ–∂–∏–º ({n_jobs} jobs) - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∞–π–ª–æ–≤—ã–π –∫—ç—à")
            self.pair_selection_cache = FileCache(".cache/optuna/pairs")
            self.data_cache = FileCache(".cache/optuna/data")
            # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –Ω–µ –Ω—É–∂–Ω—ã - FileCache –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é
            self._cache_lock = DummyLock()
            self.data_cache_lock = DummyLock()
        else:
            # –û–¥–Ω–æ–ø—Ä–æ—Ü–µ—Å—Å–Ω—ã–π —Ä–µ–∂–∏–º - –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏ —Å threading
            print("üîÑ –û–¥–Ω–æ–ø—Ä–æ—Ü–µ—Å—Å–Ω—ã–π —Ä–µ–∂–∏–º - –∏—Å–ø–æ–ª—å–∑—É–µ–º in-memory –∫—ç—à")
            self.pair_selection_cache = {}
            self.data_cache = {}
            self._cache_lock = threading.Lock()
            self.data_cache_lock = threading.Lock()
            
        self.max_cache_size = 100  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞

        # –£–°–ö–û–†–ï–ù–ò–ï: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫—ç—à–∞ rolling-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫
        self.global_cache_initialized = self._initialize_global_rolling_cache()
        if self.global_cache_initialized:
            print("‚úÖ –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤ FastWalkForwardObjective")
        else:
            print("‚ùå –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à –ù–ï –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤ FastWalkForwardObjective")
            
        # –ö–†–ò–¢–ò–ß–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞ lookahead bias
        try:
            self.lookahead_validator = create_temporal_validator(self.base_config)
            print("üîç –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤–∞–ª–∏–¥–∞—Ç–æ—Ä lookahead bias")
        except (AttributeError, ImportError, TypeError) as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å lookahead validator: {e}")
            self.lookahead_validator = None
        
        # –§–ò–ö–°–ê–¶–ò–Ø UNIVERSE: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ universe
        self.universe_manager = UniverseManager()
        self._universe_fixed = False
        print("üåç –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –º–µ–Ω–µ–¥–∂–µ—Ä universe –ø–∞—Ä")
        
        # –í–ê–õ–ò–î–ê–¶–ò–Ø SHARPE: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞
        self.sharpe_validator = create_sharpe_validator(self.base_config)
        print("üìä –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤–∞–ª–∏–¥–∞—Ç–æ—Ä Sharpe ratio")
    
    def convert_hours_to_periods(self, hours: float, bar_minutes: int) -> int:
        """Convert hours to number of periods based on bar timeframe."""
        return convert_hours_to_periods(hours, bar_minutes)
    
    def _validate_params(self, params):
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏—Å–ø–æ–ª—å–∑—É—è —Ñ—É–Ω–∫—Ü–∏—é validate_params –∏–∑ metric_utils."""
        return validate_params(params)

        # –£–±–∏—Ä–∞–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø–∞—Ä
        # –¢–µ–ø–µ—Ä—å –ø–∞—Ä—ã –æ—Ç–±–∏—Ä–∞—é—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ walk-forward —à–∞–≥–∞

        logger.info("‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FastWalkForwardObjective —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –æ—Ç–±–æ—Ä–æ–º –ø–∞—Ä –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º")
        
        # –ù–æ–≤–æ–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º walk-forward –∞–Ω–∞–ª–∏–∑–µ
        logger.info(
            "üîÑ –ò–°–ü–†–ê–í–õ–ï–ù LOOKAHEAD BIAS: –ü–∞—Ä—ã —Ç–µ–ø–µ—Ä—å –æ—Ç–±–∏—Ä–∞—é—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ "
            "–¥–ª—è –∫–∞–∂–¥–æ–≥–æ walk-forward —à–∞–≥–∞ –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —ç—Ç–æ–≥–æ —à–∞–≥–∞. "
            "–≠—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π walk-forward –∞–Ω–∞–ª–∏–∑ –±–µ–∑ lookahead bias."
        )

        if 'filters' in self.search_space:
            raise ValueError(
                "–í fast-—Ä–µ–∂–∏–º–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã 'filters' –≤ search_space –Ω–µ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è. "
                "–ü–∞—Ä—ã —É–∂–µ –ø—Ä–µ–¥–æ—Ç–æ–±—Ä–∞–Ω—ã –∏–∑ outputs/preselected_pairs.csv. "
                "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ search_space_fast.yaml –∏–ª–∏ –ø–µ—Ä–µ–Ω–µ—Å–∏—Ç–µ –æ—Ç–±–æ—Ä –ø–∞—Ä –≤ objective."
            )

        # –î–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç –∑–∞–≥—Ä—É–∂–∞—Ç—å—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –±—ç–∫—Ç–µ—Å—Ç–µ

    # –ø—Ä–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–º –æ—Ç–±–æ—Ä–µ –ø–∞—Ä –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞

    # –ø—Ä–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–º –æ—Ç–±–æ—Ä–µ –ø–∞—Ä –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞

    def _initialize_global_rolling_cache(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à rolling-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."""
        try:
            print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫—ç—à–∞ rolling-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫...")

            start_date = pd.to_datetime(self.base_config.walk_forward.start_date) - pd.Timedelta(days=self.base_config.walk_forward.training_period_days)
            end_date = pd.to_datetime(self.base_config.walk_forward.end_date)

            print(f"üìÖ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫—ç—à–∞: {start_date.date()} -> {end_date.date()}")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞—Å—Ç–µ—Ä-–¥–∞—Ç–∞—Å–µ—Ç –Ω–∞–ø—Ä—è–º—É—é
            all_raw_data = load_master_dataset(self.base_config.data_dir, start_date, end_date)
            if all_raw_data.empty:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫—ç—à–∞. –ö—ç—à –Ω–µ –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω.")
                return False

            # –ü–∏–≤–æ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ —à–∏—Ä–æ–∫–∏–π —Ñ–æ—Ä–º–∞—Ç
            all_data = all_raw_data.pivot_table(index="timestamp", columns="symbol", values="close", observed=False)
            # –ü—Ä–æ—Å—Ç–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã –∫—ç—à–∞
            all_data = all_data.ffill().bfill()

            print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö: {all_data.shape[0]} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–æ—á–µ–∫, {all_data.shape[1]} —Å–∏–º–≤–æ–ª–æ–≤")

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ —Ü–µ–Ω–∞—Ö
            from coint2.core.memory_optimization import initialize_global_price_data_from_dataframe
            print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ —Ü–µ–Ω–∞—Ö...")
            success = initialize_global_price_data_from_dataframe(all_data)
            if not success:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ —Ü–µ–Ω–∞—Ö")
                return False
            print("‚úÖ –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ —Ü–µ–Ω–∞—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")

            from coint2.core.memory_optimization import determine_required_windows
            print("üîÑ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç—Ä–µ–±—É–µ–º—ã—Ö rolling windows...")

            # –ü–µ—Ä–µ–¥–∞–µ–º –ø–æ–ª–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤–º–µ—Å—Ç–æ —Ç–æ–ª—å–∫–æ search_space
            full_config = self.base_config.model_dump() if hasattr(self.base_config, 'model_dump') else self.base_config.__dict__
            required_windows = determine_required_windows(full_config)

            # –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º rolling_window –∏–∑ search_space –µ—Å–ª–∏ –µ—Å—Ç—å
            if 'signals' in self.search_space and 'rolling_window' in self.search_space['signals']:
                rolling_window_values = self.search_space['signals']['rolling_window']
                if isinstance(rolling_window_values, list):
                    required_windows.update(rolling_window_values)
                elif isinstance(rolling_window_values, dict):
                    if 'choices' in rolling_window_values:
                        required_windows.update(rolling_window_values['choices'])
                    elif 'low' in rolling_window_values and 'high' in rolling_window_values:
                        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
                        low = rolling_window_values['low']
                        high = rolling_window_values['high']
                        step = rolling_window_values.get('step', 1)
                        range_values = list(range(low, high + 1, step))
                        required_windows.update(range_values)
                        print(f"üìä –î–æ–±–∞–≤–ª–µ–Ω—ã rolling windows –∏–∑ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ {low}-{high} (step={step}): {range_values}")
            elif 'rolling_window' in self.search_space:
                # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                rolling_window_values = self.search_space['rolling_window']
                if isinstance(rolling_window_values, list):
                    required_windows.update(rolling_window_values)
                elif isinstance(rolling_window_values, dict) and 'choices' in rolling_window_values:
                    required_windows.update(rolling_window_values['choices'])

            print(f"üìä –ù–∞–π–¥–µ–Ω—ã rolling windows: {sorted(required_windows)}")

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à rolling-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫
            cache_config = {
                'search_space': self.search_space,
                'required_windows': required_windows,
                'backtest': full_config.get('backtest', {}),
                'portfolio': full_config.get('portfolio', {})
            }

            from coint2.core.global_rolling_cache import initialize_global_rolling_cache
            print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫—ç—à–∞ rolling-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫...")
            success = initialize_global_rolling_cache(cache_config)
            if success:
                print("‚úÖ –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à rolling-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
                return True
            else:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à rolling-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫")
                return False

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫—ç—à–∞: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _load_data_for_step(self, training_start, training_end, testing_start, testing_end):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ walk-forward —à–∞–≥–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º
        –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –∏ —Ç–µ—Å—Ç–æ–≤—ã–π –ø–µ—Ä–∏–æ–¥—ã –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è lookahead bias.
        """

        print(f"üìà –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è walk-forward —à–∞–≥–∞:")
        print(f"   –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞: {training_start.date()} -> {training_end.date()}")
        print(f"   –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: {testing_start.date()} -> {testing_end.date()}")

        try:

            raw_data = load_master_dataset(
                data_path=self.base_config.data_dir,
                start_date=training_start,
                end_date=testing_end
            )

            if raw_data.empty:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –±—ç–∫—Ç–µ—Å—Ç–∏–Ω–≥–∞ —Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ
            step_df = raw_data.pivot_table(index="timestamp", columns="symbol", values="close", observed=False)

            # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º DatetimeIndex
            if not isinstance(step_df.index, pd.DatetimeIndex):
                step_df.index = pd.to_datetime(step_df.index, errors="coerce")
                if getattr(step_df.index, "tz", None) is not None:
                    step_df.index = step_df.index.tz_localize(None)
                step_df = step_df.sort_index()

            step_df = _clean_step_dataframe(
                step_df,
                self.base_config,
                drop_columns=False,
                fill_missing=False,
            )

            # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï LOOKAHEAD BIAS: –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ
            training_slice = step_df.loc[training_start:training_end]
            testing_slice = step_df.loc[testing_start:testing_end]

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö (–∑–∞—â–∏—Ç–∞ –æ—Ç lookahead bias)
            if not training_slice.empty and not testing_slice.empty:
                if training_slice.index.max() >= testing_slice.index.min():
                    raise ValueError(
                        f"–û–ë–ù–ê–†–£–ñ–ï–ù–û –ü–ï–†–ï–ö–†–´–¢–ò–ï –î–ê–ù–ù–´–•! "
                        f"–ü–æ—Å–ª–µ–¥–Ω—è—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è –º–µ—Ç–∫–∞: {training_slice.index.max()}, "
                        f"–ü–µ—Ä–≤–∞—è —Ç–µ—Å—Ç–æ–≤–∞—è –º–µ—Ç–∫–∞: {testing_slice.index.min()}. "
                        f"–≠—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ lookahead bias!"
                    )
                
                # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É –ø–µ—Ä–∏–æ–¥–∞–º–∏
                gap = testing_slice.index.min() - training_slice.index.max()
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º gap_minutes –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 15 –º–∏–Ω—É—Ç = 1 –±–∞—Ä)
                gap_minutes = getattr(self.base_config.walk_forward, 'gap_minutes', 15)
                min_gap = pd.Timedelta(minutes=gap_minutes)
                if gap < min_gap:
                    raise ValueError(
                        f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–æ–π –∏ —Ç–µ—Å—Ç–æ–º: {gap}. "
                        f"–¢—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º {min_gap} (gap_minutes={gap_minutes}) –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è data leakage."
                    )

            # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: –ò—Å–ø–æ–ª—å–∑—É–µ–º lookahead validator –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
            message = "Lookahead validator –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
            if self.lookahead_validator is not None:
                try:
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º gap_minutes –≤ –¥—Ä–æ–±–Ω—ã–µ –¥–Ω–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞
                    gap_minutes = getattr(self.base_config.walk_forward, 'gap_minutes', 15)
                    gap_days = gap_minutes / (24 * 60)  # –ú–∏–Ω—É—Ç—ã –≤ –¥–Ω–∏
                    is_valid, message = self.lookahead_validator.validate_data_split(
                        training_slice, testing_slice, gap_days
                    )
                    if not is_valid:
                        raise ValueError(f"Lookahead validator: {message}")
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ lookahead: {e}")
                    message = f"–í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞: {e}"
            
            # –û—á–∏—â–∞–µ–º training –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º —Ç–µ –∂–µ –∫–æ–ª–æ–Ω–∫–∏ –∫ —Ç–µ—Å—Ç—É, –±–µ–∑ —É—Ç–µ—á–µ–∫ –∏–∑ –±—É–¥—É—â–µ–≥–æ
            training_slice = _clean_step_dataframe(
                training_slice,
                self.base_config,
                drop_columns=True,
                fill_missing=True,
            )
            if not training_slice.empty:
                testing_slice = testing_slice.loc[:, training_slice.columns.intersection(testing_slice.columns)]
            else:
                testing_slice = testing_slice.iloc[0:0]
            testing_slice = _clean_step_dataframe(
                testing_slice,
                self.base_config,
                drop_columns=False,
                fill_missing=True,
            )

            print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã:")
            print(f"   –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π —Å—Ä–µ–∑: {training_slice.shape}")
            print(f"   –¢–µ—Å—Ç–æ–≤—ã–π —Å—Ä–µ–∑: {testing_slice.shape}")
            print(f"   –í—Ä–µ–º–µ–Ω–Ω–æ–π —Ä–∞–∑—Ä—ã–≤: {testing_start - training_end}")
            print(f"   üîç –í–∞–ª–∏–¥–∞—Ü–∏—è: {message}")

            return {
                'full_data': step_df,
                'training_data': training_slice,
                'testing_data': testing_slice,
                'training_start': training_start,
                'training_end': training_end,
                'testing_start': testing_start,
                'testing_end': testing_end
            }

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            raise
    
    def _suggest_parameters(self, trial: optuna.Trial):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Optuna trial –Ω–∞ –æ—Å–Ω–æ–≤–µ search_space.
        
        Args:
            trial: Optuna trial –æ–±—ä–µ–∫—Ç
            
        Returns:
            dict: –°–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤–∫–ª—é—á–∞—è trial_number
        """
        params = {}
        
        # –ì—Ä—É–ø–ø–∞ 1: –§–∏–ª—å—Ç—Ä—ã –æ—Ç–±–æ—Ä–∞ –ø–∞—Ä
        if 'filters' in self.search_space:
            filters = self.search_space['filters']
            if 'ssd_top_n' in filters:
                cfg = filters['ssd_top_n']

                if cfg.get('step'):
                    params['ssd_top_n'] = trial.suggest_int(
                        "ssd_top_n",
                        cfg['low'],
                        cfg['high'],
                        step=cfg['step']
                    )
                else:

                    params['ssd_top_n'] = trial.suggest_int(
                        "ssd_top_n",
                        cfg['low'],
                        cfg['high'],
                        log=True
                    )
            if 'kpss_pvalue_threshold' in filters:
                params['kpss_pvalue_threshold'] = trial.suggest_float(
                    "kpss_pvalue_threshold",
                    filters['kpss_pvalue_threshold']['low'],
                    filters['kpss_pvalue_threshold']['high']
                )
            if 'coint_pvalue_threshold' in filters:
                params['coint_pvalue_threshold'] = trial.suggest_float(
                    "coint_pvalue_threshold",
                    filters['coint_pvalue_threshold']['low'],
                    filters['coint_pvalue_threshold']['high']
                )

            if 'min_half_life_days' in filters:

                params['min_half_life_days'] = trial.suggest_float(
                    "min_half_life_days",
                    filters['min_half_life_days']['low'],
                    filters['min_half_life_days']['high']
                )

            if 'max_half_life_days' in filters:
                min_half_life = params.get('min_half_life_days', filters['max_half_life_days']['low'])
                # max_half_life –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å >= min_half_life
                effective_low = max(filters['max_half_life_days']['low'], min_half_life + 0.1)

                if effective_low <= filters['max_half_life_days']['high']:

                    params['max_half_life_days'] = trial.suggest_float(
                        "max_half_life_days",
                        effective_low,
                        filters['max_half_life_days']['high']
                    )
                else:
                    # –ï—Å–ª–∏ –¥–∏–∞–ø–∞–∑–æ–Ω –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º pruning
                    raise optuna.TrialPruned(f"–ù–µ–≤–æ–∑–º–æ–∂–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω max_half_life –¥–ª—è min_half_life={min_half_life}")
            if 'min_mean_crossings' in filters:
                params['min_mean_crossings'] = trial.suggest_int(
                    "min_mean_crossings",
                    filters['min_mean_crossings']['low'],
                    filters['min_mean_crossings']['high']
                )
        
        # –ì—Ä—É–ø–ø–∞ 2: –°–∏–≥–Ω–∞–ª—ã –∏ —Ç–∞–π–º–∏–Ω–≥ - —É—Å–ª–æ–≤–Ω—ã–π sampling –¥–ª—è –∑–∞–≤–∏—Å–∏–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if 'signals' in self.search_space:
            signals = self.search_space['signals']

            # –°–Ω–∞—á–∞–ª–∞ —Å–µ–º–ø–ª–∏–º zscore_threshold
            if 'zscore_threshold' in signals:
                params['zscore_threshold'] = trial.suggest_float(
                    "zscore_threshold",
                    signals['zscore_threshold']['low'],
                    signals['zscore_threshold']['high']
                )

            # –ó–∞—Ç–µ–º —Å–µ–º–ø–ª–∏–º zscore_exit —Å —É—á–µ—Ç–æ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
            if 'zscore_exit' in signals and 'zscore_threshold' in params:
                threshold = params['zscore_threshold']
                # zscore_exit –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –±–ª–∏–∂–µ –∫ 0, —á–µ–º threshold
                max_exit = min(signals['zscore_exit']['high'], threshold - 0.1)
                min_exit = max(signals['zscore_exit']['low'], -threshold + 0.1)

                if min_exit <= max_exit:
                    zscore_exit = trial.suggest_float(
                        "zscore_exit",
                        min_exit,
                        max_exit
                    )
                    params['zscore_exit'] = zscore_exit

                    # BEST PRACTICE: –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω—Ç–∏-—á—É—Ä–Ω –ø—Ä–æ–≤–µ—Ä–∫–∏
                    gap = threshold - zscore_exit
                    if gap < 0.05:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π gap –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —á–∞—Å—Ç—ã—Ö —Å–¥–µ–ª–æ–∫
                        raise optuna.TrialPruned(f"–°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π gap –º–µ–∂–¥—É threshold –∏ exit: {gap:.3f} < 0.05")

                    # –õ–æ–≥–∏—Ä—É–µ–º hysteresis –¥–ª—è –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏
                    trial.set_user_attr("hysteresis", gap)
                else:
                    # –ï—Å–ª–∏ –¥–∏–∞–ø–∞–∑–æ–Ω –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º pruning
                    raise optuna.TrialPruned(f"–ù–µ–≤–æ–∑–º–æ–∂–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω zscore_exit –¥–ª—è threshold={threshold}")
            elif 'zscore_exit' in signals:
                # Fallback –µ—Å–ª–∏ threshold –Ω–µ –∑–∞–¥–∞–Ω
                params['zscore_exit'] = trial.suggest_float(
                    "zscore_exit",
                    signals['zscore_exit']['low'],
                    signals['zscore_exit']['high']
                )

            if 'rolling_window' in signals:
                cfg = signals['rolling_window']
                if 'step' in cfg:
                    params['rolling_window'] = trial.suggest_int("rolling_window", cfg['low'], cfg['high'], step=cfg['step'])
                else:
                    params['rolling_window'] = trial.suggest_int("rolling_window", cfg['low'], cfg['high'])
                
                # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: rolling_window –¥–æ–ª–∂–µ–Ω —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
                # –ï—Å–ª–∏ –µ—Å—Ç—å zscore_lookback_hours, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
                if hasattr(self.base_config.backtest, 'zscore_lookback_hours'):
                    bar_minutes = getattr(self.base_config.pair_selection, 'bar_minutes', 15)
                    expected_window = convert_hours_to_periods(
                        self.base_config.backtest.zscore_lookback_hours, 
                        bar_minutes
                    )
                    if abs(params['rolling_window'] - expected_window) > 10:
                        logger.warning(
                            f"WARNING: rolling_window={params['rolling_window']} –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç "
                            f"–æ–∂–∏–¥–∞–µ–º–æ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ zscore_lookback_hours: {expected_window}"
                        )
        
        # –ì—Ä—É–ø–ø–∞ 3: –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–∞–º–∏
        if 'risk_management' in self.search_space:
            risk = self.search_space['risk_management']
            if 'stop_loss_multiplier' in risk:
                params['stop_loss_multiplier'] = trial.suggest_float(
                    "stop_loss_multiplier",
                    risk['stop_loss_multiplier']['low'],
                    risk['stop_loss_multiplier']['high']
                )
            if 'time_stop_multiplier' in risk:
                params['time_stop_multiplier'] = trial.suggest_float(
                    "time_stop_multiplier",
                    risk['time_stop_multiplier']['low'],
                    risk['time_stop_multiplier']['high']
                )
            if 'cooldown_hours' in risk:
                cfg = risk['cooldown_hours']
                if 'step' in cfg:
                    params['cooldown_hours'] = trial.suggest_int("cooldown_hours", cfg['low'], cfg['high'], step=cfg['step'])
                else:
                    params['cooldown_hours'] = trial.suggest_int("cooldown_hours", cfg['low'], cfg['high'])
        
        # –ì—Ä—É–ø–ø–∞ 4: –ü–æ—Ä—Ç—Ñ–µ–ª—å
        if 'portfolio' in self.search_space:
            portfolio = self.search_space['portfolio']
            if 'risk_per_position_pct' in portfolio:
                params['risk_per_position_pct'] = trial.suggest_float(
                    "risk_per_position_pct",
                    portfolio['risk_per_position_pct']['low'],
                    portfolio['risk_per_position_pct']['high']
                )
            if 'max_position_size_pct' in portfolio:
                params['max_position_size_pct'] = trial.suggest_float(
                    "max_position_size_pct",
                    portfolio['max_position_size_pct']['low'],
                    portfolio['max_position_size_pct']['high']
                )
            if 'max_active_positions' in portfolio:
                cfg = portfolio['max_active_positions']
                params['max_active_positions'] = trial.suggest_int(
                    "max_active_positions",
                    cfg['low'],
                    cfg['high'],
                    step=cfg.get('step', 1)
                )
        
        # –ì—Ä—É–ø–ø–∞ 5: –ò–∑–¥–µ—Ä–∂–∫–∏
        if 'costs' in self.search_space:
            costs = self.search_space['costs']
            if 'commission_pct' in costs:
                if isinstance(costs['commission_pct'], dict):
                    # –î–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π
                    params['commission_pct'] = trial.suggest_float(
                        "commission_pct",
                        costs['commission_pct']['low'],
                        costs['commission_pct']['high']
                    )
                else:
                    # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                    params['commission_pct'] = costs['commission_pct']
            if 'slippage_pct' in costs:
                if isinstance(costs['slippage_pct'], dict):
                    # –î–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π
                    params['slippage_pct'] = trial.suggest_float(
                        "slippage_pct",
                        costs['slippage_pct']['low'],
                        costs['slippage_pct']['high']
                    )
                else:
                    # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                    params['slippage_pct'] = costs['slippage_pct']
        
        # –ì—Ä—É–ø–ø–∞ 6: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        if 'normalization' in self.search_space:
            norm = self.search_space['normalization']
            if 'normalization_method' in norm:
                params['normalization_method'] = trial.suggest_categorical(
                    "normalization_method",
                    norm['normalization_method']
                )
            if 'min_history_ratio' in norm:
                params['min_history_ratio'] = trial.suggest_float(
                    "min_history_ratio",
                    norm['min_history_ratio']['low'],
                    norm['min_history_ratio']['high']
                )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–º–µ—Ä trial –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        params['trial_number'] = trial.number
        
        return params

    def _select_pairs_for_step(self, cfg, training_data, step_idx):
        """
        –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –û—Ç–±–∏—Ä–∞–µ—Ç –ø–∞—Ä—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ walk-forward —à–∞–≥–∞
        –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —ç—Ç–æ–≥–æ —à–∞–≥–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è lookahead bias.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä—ã –ò —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏.
        """
        logger = get_logger(f"pair_selection_step_{step_idx}")
        
        print(f"   üîç –û—Ç–±–æ—Ä –ø–∞—Ä –¥–ª—è —à–∞–≥–∞ {step_idx + 1} –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {training_data.shape}")
        
        # –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏
        if len(training_data) < 2880:  # –ú–∏–Ω–∏–º—É–º 30 –¥–Ω–µ–π –¥–ª—è 15-–º–∏–Ω –±–∞—Ä–æ–≤
            logger.warning(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–±–æ—Ä–∞ –ø–∞—Ä: {len(training_data)} < 2880 (30 –¥–Ω–µ–π)")
            print(f"   ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {len(training_data)} –±–∞—Ä–æ–≤ < 2880 –º–∏–Ω–∏–º—É–º")
            # –ù–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –∏–º–µ—é—â–∏–º–∏—Å—è –¥–∞–Ω–Ω—ã–º–∏, –Ω–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–∞–µ–º
        
        try:
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–±–æ—Ä–∞ –ø–∞—Ä
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π (—É—á–∏—Ç—ã–≤–∞–µ–º mock-–∫–æ–Ω—Ñ–∏–≥–∏)
            data_processing = getattr(cfg, "data_processing", None)
            pair_selection = getattr(cfg, "pair_selection", None)
            backtest_cfg = getattr(cfg, "backtest", None)

            min_history_ratio = _coerce_float(
                getattr(data_processing, "min_history_ratio", None),
                _coerce_float(getattr(pair_selection, "min_history_ratio", None), 0.8),
            )
            fill_method = _coerce_str(getattr(data_processing, "fill_method", None), "ffill")
            norm_method = _coerce_str(getattr(data_processing, "normalization_method", None), "rolling_zscore")
            handle_constant = _coerce_bool(getattr(data_processing, "handle_constant", None), True)
            rolling_window = _coerce_int(getattr(backtest_cfg, "rolling_window", None), 25)
            
            # –í–ê–ñ–ù–û: –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –≤–æ–∑–≤—Ä–∞—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏
            normalized_training, norm_stats = preprocess_and_normalize_data(
                training_data,
                min_history_ratio=min_history_ratio,
                fill_method=fill_method,
                norm_method=norm_method,
                handle_constant=handle_constant,
                rolling_window=rolling_window,
                return_stats=True  # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            )
            
            if normalized_training.empty or len(normalized_training.columns) < 2:
                print(f"   ‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–±–æ—Ä–∞ –ø–∞—Ä –≤ —à–∞–≥–µ {step_idx + 1}")
                return pd.DataFrame(), norm_stats.get('normalization_stats', {})
            
            pairs_file = getattr(getattr(cfg, "walk_forward", None), "pairs_file", None)
            if pairs_file:
                fixed_pairs = load_pair_tuples(pairs_file)
                if not fixed_pairs:
                    print(f"   ‚ùå –§–∞–π–ª pairs_file –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–∞—Ä: {pairs_file}")
                    return pd.DataFrame(), norm_stats.get('normalization_stats', {})

                available_symbols = set(normalized_training.columns)
                pairs_for_filter = [
                    (s1, s2)
                    for s1, s2 in fixed_pairs
                    if s1 in available_symbols and s2 in available_symbols
                ]
                dropped = len(fixed_pairs) - len(pairs_for_filter)
                print(
                    f"   üîí –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π universe: {len(pairs_for_filter)} –ø–∞—Ä "
                    f"(–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {dropped} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö)"
                )
            else:
                # –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä
                ssd = calculate_ssd(normalized_training, top_k=None)

                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–æ—Ç–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤–∞–ª—é—Ç–µ (*USDT)
                usdt_ssd = ssd[ssd.index.map(lambda x: x[0].endswith('USDT') and x[1].endswith('USDT'))]

                # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ top-N –ø–∞—Ä –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                ssd_top_n = cfg.pair_selection.ssd_top_n
                if len(usdt_ssd) > ssd_top_n:
                    usdt_ssd = usdt_ssd.sort_values().head(ssd_top_n)

                pairs_for_filter = [(s1, s2) for s1, s2 in usdt_ssd.index]

            if not pairs_for_filter:
                print(f"   ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–∞—Ä –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –≤ —à–∞–≥–µ {step_idx + 1}")
                return pd.DataFrame(), norm_stats.get('normalization_stats', {})

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–∞—Ä –ø–æ –∫–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏ –¥—Ä—É–≥–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º
            filtered_pairs = filter_pairs_by_coint_and_half_life(
                pairs_for_filter,
                training_data,
                min_half_life=getattr(cfg.pair_selection, 'min_half_life_days', 1.0),
                max_half_life=getattr(cfg.pair_selection, 'max_half_life_days', 30.0),
                pvalue_threshold=getattr(cfg.pair_selection, 'coint_pvalue_threshold', 0.05),
                min_beta=getattr(cfg.pair_selection, 'min_beta', 0.001),
                max_beta=getattr(cfg.pair_selection, 'max_beta', 100.0),
                max_hurst_exponent=getattr(cfg.pair_selection, 'max_hurst_exponent', 0.7),
                min_mean_crossings=getattr(cfg.pair_selection, 'min_mean_crossings', 10),
                min_correlation=getattr(cfg.pair_selection, 'min_correlation', 0.5),  # –ù–û–í–´–ô –ø–∞—Ä–∞–º–µ—Ç—Ä
                kpss_pvalue_threshold=getattr(cfg.pair_selection, 'kpss_pvalue_threshold', 0.05),
            )
            
            if not filtered_pairs:
                print(f"   ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–∞—Ä –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –≤ —à–∞–≥–µ {step_idx + 1}")
                return pd.DataFrame(), norm_stats.get('normalization_stats', {})
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–∞—Ä—ã –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
            quality_sorted_pairs = sorted(filtered_pairs, key=lambda x: abs(x[4]), reverse=True)
            
            # –¢–æ–ø-M –æ—Ç–±–æ—Ä –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è churn –∏ –∫–æ–º–∏—Å—Å–∏–π
            max_pairs_for_trading = getattr(cfg.pair_selection, 'max_pairs_for_trading', 50)
            active_pairs = quality_sorted_pairs[:max_pairs_for_trading]
            
            # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø–∞—Ä –≤ —Ñ–æ—Ä–º–∞—Ç–µ DataFrame
            pairs_list = []
            for s1, s2, beta, mean, std, metrics in active_pairs:
                pairs_list.append({
                    's1': s1,
                    's2': s2,
                    'beta': beta,
                    'mean': mean,
                    'std': std,
                    'half_life': metrics.get('half_life', 0),
                    'pvalue': metrics.get('pvalue', 0),
                    'hurst': 0,
                    'mean_crossings': metrics.get('mean_crossings', 0)
                })
            
            step_pairs_df = pd.DataFrame(pairs_list)
            
            print(
                f"   ‚úÖ –®–∞–≥ {step_idx + 1}: –æ—Ç–æ–±—Ä–∞–Ω–æ {len(step_pairs_df)} –ø–∞—Ä "
                f"–∏–∑ {len(pairs_for_filter)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤"
            )
            
            # –í–ê–ñ–ù–û: –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–∞—Ä—ã –ò —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            normalization_stats = norm_stats.get('normalization_stats', {})
            return step_pairs_df, normalization_stats
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –æ—Ç–±–æ—Ä–∞ –ø–∞—Ä –¥–ª—è —à–∞–≥–∞ {step_idx + 1}: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame(), {}
    
    def _process_single_walk_forward_step(self, cfg, step_data, step_idx):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω walk-forward —à–∞–≥ —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –æ—Ç–±–æ—Ä–æ–º –ø–∞—Ä."""
        testing_start = step_data['testing_start']
        testing_end = step_data['testing_end']
        training_data = step_data['training_data']
        step_df = step_data['full_data']
        
        # –ö–†–ò–¢–ò–ß–ù–û: –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≥—Ä–∞–Ω–∏—Ü –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è lookahead bias
        training_start = step_data.get('training_start')
        training_end = step_data.get('training_end')
        
        if training_end and testing_start:
            if testing_start < training_end:
                raise ValueError(
                    f"‚ùå LOOKAHEAD BIAS DETECTED! Testing period starts before training ends: "
                    f"training_end={training_end}, testing_start={testing_start}"
                )
            # –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ–∫ –º–µ–∂–¥—É training –∏ testing
            # –°–æ–≥–ª–∞—Å–Ω–æ CLAUDE.md, –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–µ–∂—É—Ç–æ–∫ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 15 –º–∏–Ω—É—Ç (1 –±–∞—Ä)
            gap = testing_start - training_end
            gap_minutes = gap.total_seconds() / 60
            if gap_minutes != 15:  # –î–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–æ–≤–Ω–æ 15 –º–∏–Ω—É—Ç
                logger.debug(
                    f"–ü—Ä–æ–º–µ–∂—É—Ç–æ–∫ –º–µ–∂–¥—É training –∏ testing: {gap_minutes:.0f} –º–∏–Ω—É—Ç "
                    f"(–æ–∂–∏–¥–∞–µ—Ç—Å—è 15 –º–∏–Ω—É—Ç –¥–ª—è 15-–º–∏–Ω—É—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö)"
                )

        print(f"   üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —à–∞–≥–∞ {step_idx + 1}: {testing_start.strftime('%Y-%m-%d')} -> {testing_end.strftime('%Y-%m-%d')}")

        # –£–°–ö–û–†–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∫—ç—à –¥–ª—è –æ—Ç–±–æ—Ä–∞ –ø–∞—Ä
        if not training_start:
            training_start = step_data['training_start']
        if not training_end:
            training_end = step_data['training_end']

        # —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø–∞—Ä –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        data_processing = getattr(cfg, "data_processing", None)
        pair_selection = getattr(cfg, "pair_selection", None)
        backtest_cfg = getattr(cfg, "backtest", None)

        norm_method = _coerce_str(getattr(data_processing, "normalization_method", None), "rolling_zscore")
        min_history_ratio = _coerce_float(
            getattr(data_processing, "min_history_ratio", None),
            _coerce_float(getattr(pair_selection, "min_history_ratio", None), 0.8),
        )
        fill_method = _coerce_str(getattr(data_processing, "fill_method", None), "ffill")
        handle_constant = _coerce_bool(getattr(data_processing, "handle_constant", None), True)
        rolling_window = _coerce_int(getattr(backtest_cfg, "rolling_window", None), 25)
        filter_params = (
            f"ssd{getattr(cfg.pair_selection, 'ssd_top_n', 10000)}_"
            f"pval{getattr(cfg.pair_selection, 'coint_pvalue_threshold', 0.05)}_"
            f"hl{getattr(cfg.pair_selection, 'min_half_life_days', 1)}-{getattr(cfg.pair_selection, 'max_half_life_days', 30)}_"
            f"kpss{getattr(cfg.pair_selection, 'kpss_pvalue_threshold', 0.05)}_"
            f"norm{norm_method}_hist{min_history_ratio:.4f}_fill{fill_method}_"
            f"roll{rolling_window}_const{int(bool(handle_constant))}"
        )
        cache_key = f"{training_start.strftime('%Y-%m-%d')}_{training_end.strftime('%Y-%m-%d')}_{filter_params}"

        # 1. –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞ –±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
        if cache_key in self.pair_selection_cache:
            print(f"   üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä—ã –¥–ª—è –ø–µ—Ä–∏–æ–¥–∞ {cache_key}")
            cached_data = self.pair_selection_cache[cache_key]
            if isinstance(cached_data, tuple):
                step_pairs, normalization_stats = cached_data
            else:
                # –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å–æ —Å—Ç–∞—Ä—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º –∫—ç—à–∞
                step_pairs = cached_data
                normalization_stats = {}

            pair_tuples = _pairs_df_to_tuples(step_pairs)
            if pair_tuples and self._universe_fixed:
                try:
                    self.universe_manager.validate_pairs(pair_tuples, raise_on_mismatch=False)
                except ValueError as e:
                    print(f"   ‚ö†Ô∏è Universe –∏–∑–º–µ–Ω–∏–ª—Å—è: {e}")
        else:
            # 2. –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–æ—Ä–æ–≥–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏
            with self._cache_lock:
                # 3. –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞ –í–ù–£–¢–†–ò –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
                if cache_key in self.pair_selection_cache:
                    print(f"   üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä—ã –¥–ª—è –ø–µ—Ä–∏–æ–¥–∞ {cache_key} (–ø–æ–ª—É—á–µ–Ω—ã –≤–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è)")
                    cached_data = self.pair_selection_cache[cache_key]
                    if isinstance(cached_data, tuple):
                        step_pairs, normalization_stats = cached_data
                    else:
                        step_pairs = cached_data
                        normalization_stats = {}
                else:
                    print(f"   üîç –û—Ç–±–∏—Ä–∞–µ–º –Ω–æ–≤—ã–µ –ø–∞—Ä—ã –¥–ª—è –ø–µ—Ä–∏–æ–¥–∞ {cache_key}")
                    step_pairs, normalization_stats = self._select_pairs_for_step(cfg, training_data, step_idx)

                    pair_tuples = _pairs_df_to_tuples(step_pairs)
                    
                    # –§–ò–ö–°–ê–¶–ò–Ø UNIVERSE: –§–∏–∫—Å–∏—Ä—É–µ–º universe –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –æ—Ç–±–æ—Ä–µ
                    if pair_tuples and not self._universe_fixed:
                        study_name = getattr(cfg, 'study_name', 'default_study')
                        self.universe_manager.fix_universe(pair_tuples, study_name)
                        self._universe_fixed = True
                        print(f"   üîí Universe –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω: {len(pair_tuples)} –ø–∞—Ä")
                    
                    # –í–∞–ª–∏–¥–∞—Ü–∏—è universe –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö —à–∞–≥–æ–≤
                    elif pair_tuples and self._universe_fixed:
                        try:
                            self.universe_manager.validate_pairs(pair_tuples, raise_on_mismatch=False)
                        except ValueError as e:
                            print(f"   ‚ö†Ô∏è Universe –∏–∑–º–µ–Ω–∏–ª—Å—è: {e}")
                    
                    # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –∫—ç—à
                    if step_pairs is not None and len(step_pairs) > 0:
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ä—ã –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤–º–µ—Å—Ç–µ
                        self.pair_selection_cache[cache_key] = (step_pairs, normalization_stats)
                        print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–∏–ª–∏ {len(step_pairs)} –ø–∞—Ä –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤ –∫—ç—à –¥–ª—è –ø–µ—Ä–∏–æ–¥–∞ {cache_key}")

        if step_pairs is None or len(step_pairs) == 0:
            print(f"   ‚ùå –ù–µ—Ç –ø–∞—Ä –¥–ª—è —à–∞–≥–∞ {step_idx + 1}")
            return {
                'pnls': [],
                'trades': 0,
                'pairs_checked': 0,
                'pairs_with_data': 0
            }

        step_pnls = []
        step_trades = 0
        pairs_processed = 0
        pairs_with_data = 0

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–¥–µ–ª–∫–∞—Ö –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫
        all_trade_pnls = []
        
        for _, pair_row in step_pairs.iterrows():
            try:

                backtest_output = self._backtest_single_pair(
                    pair_row,
                    cfg,
                    training_data=training_data,
                    testing_data=step_data.get('testing_data'),
                    step_df=step_df,
                    normalization_stats=normalization_stats
                )
                if backtest_output is None:
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—É, –µ—Å–ª–∏ –±—ç–∫—Ç–µ—Å—Ç –Ω–µ —É–¥–∞–ª—Å—è
                pair_result, pair_trades = backtest_output

                if pair_result is not None and len(pair_result) > 0:
                    # pair_result —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –∑–∞ —Ç–µ—Å—Ç–æ–≤—ã–π –ø–µ—Ä–∏–æ–¥
                    # —Ç–∞–∫ –∫–∞–∫ –º—ã –ø–µ—Ä–µ–¥–∞–ª–∏ –≤ –±—ç–∫—Ç–µ—Å—Ç–µ—Ä —Ç–æ–ª—å–∫–æ testing_pair_data
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º numpy array –≤ pandas Series –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                    if isinstance(pair_result, np.ndarray):
                        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π Series –±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
                        # —Ç–∞–∫ –∫–∞–∫ –Ω–∞–º –≤–∞–∂–Ω—ã —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏—è PnL
                        step_result = pd.Series(pair_result)
                    else:
                        step_result = pair_result
                    
                    if len(step_result) > 0:
                        step_pnls.append(step_result)
                        step_trades += pair_trades
                        pairs_with_data += 1
                        
                        # –ù–û–í–û–ï: –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–¥–µ–ª–∫–∞—Ö –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫
                        # –ù—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—ç–∫—Ç–µ—Å—Ç–∞ —Å –ø–æ–∑–∏—Ü–∏—è–º–∏
                        # –í —Ç–µ–∫—É—â–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ pair_result —ç—Ç–æ —Ç–æ–ª—å–∫–æ PnL —Å–µ—Ä–∏—è
                        # –î–æ–±–∞–≤–∏–º —Å–±–æ—Ä PnL –ø–æ —Å–¥–µ–ª–∫–∞–º –≤ —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏

                pairs_processed += 1

            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–∞—Ä—ã –≤ —à–∞–≥–µ {step_idx + 1}: {e}")
                continue

        print(f"   üìä –®–∞–≥ {step_idx + 1}: {pairs_with_data}/{pairs_processed} –ø–∞—Ä, {step_trades} —Å–¥–µ–ª–æ–∫")

        return {
            'pnls': step_pnls,
            'trades': step_trades,
            'pairs_checked': pairs_processed,
            'pairs_with_data': pairs_with_data
        }

    def _run_fast_backtest(self, params):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –±—ã—Å—Ç—Ä—ã–π –±—ç–∫—Ç–µ—Å—Ç –¢–û–ß–ù–û –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ."""

        print(f"\nüîç –î–ï–¢–ê–õ–¨–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ë–≠–ö–¢–ï–°–¢–ê")
        print(f"üìä –í—Ö–æ–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
        for key, value in params.items():
            print(f"   {key}: {value}")

        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
        try:
            validated_params = validate_params(params)
            print(f"‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ")
        except ValueError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {e}")
            return {"sharpe_ratio_abs": None, "total_trades": 0, "error_type": "validation_error", "error_message": str(e)}

        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        cfg = self.base_config.model_copy(deep=True)
        
        # –ì—Ä—É–ø–ø–∞ 1: –§–∏–ª—å—Ç—Ä—ã –æ—Ç–±–æ—Ä–∞ –ø–∞—Ä
        if 'ssd_top_n' in validated_params:
            cfg.pair_selection.ssd_top_n = validated_params['ssd_top_n']
        if 'kpss_pvalue_threshold' in validated_params:
            cfg.pair_selection.kpss_pvalue_threshold = validated_params['kpss_pvalue_threshold']
        if 'coint_pvalue_threshold' in validated_params:
            cfg.pair_selection.coint_pvalue_threshold = validated_params['coint_pvalue_threshold']
        if 'min_half_life_days' in validated_params:
            cfg.pair_selection.min_half_life_days = validated_params['min_half_life_days']
        if 'max_half_life_days' in validated_params:
            cfg.pair_selection.max_half_life_days = validated_params['max_half_life_days']
        if 'min_mean_crossings' in validated_params:
            cfg.pair_selection.min_mean_crossings = validated_params['min_mean_crossings']
        
        # –ì—Ä—É–ø–ø–∞ 2: –°–∏–≥–Ω–∞–ª—ã –∏ —Ç–∞–π–º–∏–Ω–≥
        cfg.backtest.zscore_threshold = validated_params.get('zscore_threshold', 2.0)
        cfg.backtest.zscore_entry_threshold = cfg.backtest.zscore_threshold  # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Å –æ—Å–Ω–æ–≤–Ω—ã–º –ø–∞–π–ø–ª–∞–π–Ω–æ–º
        cfg.backtest.zscore_exit = validated_params.get('zscore_exit', 0.0)
        if 'rolling_window' in validated_params:
            cfg.backtest.rolling_window = validated_params['rolling_window']
        
        # –ì—Ä—É–ø–ø–∞ 3: –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∏—Å–∫–∞–º–∏
        cfg.backtest.stop_loss_multiplier = validated_params.get('stop_loss_multiplier', 3.0)
        cfg.backtest.time_stop_multiplier = validated_params.get('time_stop_multiplier', 2.0)
        if 'cooldown_hours' in validated_params:
            cfg.backtest.cooldown_hours = validated_params['cooldown_hours']
        
        # –ì—Ä—É–ø–ø–∞ 4: –ü–æ—Ä—Ç—Ñ–µ–ª—å
        if hasattr(cfg, 'portfolio'):
            cfg.portfolio.risk_per_position_pct = validated_params.get('risk_per_position_pct', 0.015)
            if hasattr(cfg.portfolio, 'max_position_size_pct'):
                cfg.portfolio.max_position_size_pct = validated_params.get('max_position_size_pct', 0.1)
            cfg.portfolio.max_active_positions = int(validated_params.get('max_active_positions', 15))
        
        # –ì—Ä—É–ø–ø–∞ 5: –ò–∑–¥–µ—Ä–∂–∫–∏
        if 'commission_pct' in validated_params:
            cfg.backtest.commission_pct = validated_params['commission_pct']
        if 'slippage_pct' in validated_params:
            cfg.backtest.slippage_pct = validated_params['slippage_pct']
        
        # –ì—Ä—É–ø–ø–∞ 6: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        if 'normalization_method' in validated_params:
            if hasattr(cfg, 'data_processing'):
                cfg.data_processing.normalization_method = validated_params['normalization_method']
        if 'min_history_ratio' in validated_params:
            if hasattr(cfg, 'data_processing'):
                cfg.data_processing.min_history_ratio = validated_params['min_history_ratio']

        start_date = pd.to_datetime(cfg.walk_forward.start_date)
        end_date = pd.to_datetime(getattr(cfg.walk_forward, 'end_date', start_date + pd.Timedelta(days=cfg.walk_forward.testing_period_days)))
        step_size_days = _resolve_step_size_days(cfg)
        bar_minutes = getattr(cfg.pair_selection, "bar_minutes", None) or 15
        bar_delta = pd.Timedelta(minutes=bar_minutes)

        # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ step_size_days –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è lookahead bias
        if step_size_days < cfg.walk_forward.testing_period_days:
            logger.warning(
                f"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: step_size_days ({step_size_days}) < testing_period_days ({cfg.walk_forward.testing_period_days}). "
                f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º step_size_days = testing_period_days –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤."
            )
            step_size_days = cfg.walk_forward.testing_period_days

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Å–µ walk-forward —à–∞–≥–∏
        walk_forward_steps = []
        current_test_start = start_date

        while current_test_start < end_date:
            training_start = current_test_start - pd.Timedelta(days=cfg.walk_forward.training_period_days)
            training_end = current_test_start - bar_delta
            testing_start = current_test_start
            testing_end = min(
                testing_start + pd.Timedelta(days=cfg.walk_forward.testing_period_days),
                end_date
            )

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ç–µ—Å—Ç–æ–≤—ã–π –ø–µ—Ä–∏–æ–¥ –Ω–µ –ø—É—Å—Ç–æ–π
            if testing_end > testing_start:
                walk_forward_steps.append({
                    'training_start': training_start,
                    'training_end': training_end,
                    'testing_start': testing_start,
                    'testing_end': testing_end
                })

            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —à–∞–≥—É
            current_test_start += pd.Timedelta(days=step_size_days)

        max_steps = getattr(cfg.walk_forward, 'max_steps', None)
        if max_steps is not None:
            try:
                max_steps = int(max_steps)
            except (TypeError, ValueError):
                max_steps = None

        if max_steps and max_steps > 0:
            walk_forward_steps = walk_forward_steps[:max_steps]

        print(f"üóìÔ∏è  –ú–ù–û–ñ–ï–°–¢–í–ï–ù–ù–´–ï WALK-FORWARD –®–ê–ì–ò ({len(walk_forward_steps)} —à–∞–≥–æ–≤):")
        for i, step in enumerate(walk_forward_steps):
            print(f"   –®–∞–≥ {i+1}: –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ {step['training_start'].strftime('%Y-%m-%d')} -> {step['training_end'].strftime('%Y-%m-%d')}, "
                  f"–¢–µ—Å—Ç {step['testing_start'].strftime('%Y-%m-%d')} -> {step['testing_end'].strftime('%Y-%m-%d')}")

        # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π —à–∞–≥
        if not walk_forward_steps:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ walk-forward —à–∞–≥–∞")

        first_step = walk_forward_steps[0]
        training_start = first_step['training_start']
        training_end = first_step['training_end']
        testing_start = first_step['testing_start']
        testing_end = first_step['testing_end']

        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ –≤—Å–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ - Timestamp
        testing_start = pd.to_datetime(testing_start)
        testing_end = pd.to_datetime(testing_end)

        all_step_results = []

        for step_idx, step in enumerate(walk_forward_steps):
            print(f"\nüîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ walk-forward —à–∞–≥–∞ {step_idx + 1}/{len(walk_forward_steps)}")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç—Ç–æ–≥–æ —à–∞–≥–∞
            step_data = self._load_data_for_step(
                step['training_start'], step['training_end'],
                step['testing_start'], step['testing_end']
            )
            step_df = step_data['full_data']

            if step_df is None:
                print(f"   ‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —à–∞–≥–∞ {step_idx + 1}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —ç—Ç–æ—Ç —à–∞–≥
            step_result = self._process_single_walk_forward_step(
                cfg, step_data, step_idx
            )

            if step_result is not None and step_result['pnls']:
                all_step_results.append(step_result)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if not all_step_results:
            print("‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ walk-forward —à–∞–≥–∞")
            return {"sharpe_ratio_abs": None, "total_trades": 0, "error_type": "no_wf_steps", "error_message": "No valid walk-forward steps"}

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö —à–∞–≥–æ–≤
        all_pnls = []
        total_trades = 0

        for step_result in all_step_results:
            all_pnls.extend(step_result['pnls'])
            total_trades += step_result['trades']

        print(f"\nüìä –ê–ì–†–ï–ì–ò–†–û–í–ê–ù–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–°–ï–• {len(all_step_results)} –®–ê–ì–û–í:")
        print(f"   üìà –í—Å–µ–≥–æ PnL —Å–µ—Ä–∏–π: {len(all_pnls)}")
        print(f"   üîÑ –í—Å–µ–≥–æ —Å–¥–µ–ª–æ–∫: {total_trades}")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Ä—Ç—Ñ–µ–ª—å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        portfolio = Portfolio(
            initial_capital=cfg.portfolio.initial_capital,
            max_active_positions=cfg.portfolio.max_active_positions,
            config=cfg.portfolio,
        )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫
        if not all_pnls:
            print(f"üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ù–ï–¢ PnL –î–ê–ù–ù–´–• - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
            return {"sharpe_ratio_abs": None, "total_trades": total_trades, "error_type": "no_pnl_data"}

        # –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è —Å–∏–º—É–ª—è—Ü–∏—è –ø–æ—Ä—Ç—Ñ–µ–ª—è —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –ø–æ–∑–∏—Ü–∏—è–º–∏
        try:
            if len(all_pnls) == 1:
                combined_pnl = all_pnls[0].fillna(0)
            else:
                # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é —Å–∏–º—É–ª—è—Ü–∏—é –ø–æ—Ä—Ç—Ñ–µ–ª—è
                combined_pnl = self._simulate_realistic_portfolio(all_pnls, cfg)

                print(f"üìä –†–ï–ê–õ–ò–°–¢–ò–ß–ù–ê–Ø –°–ò–ú–£–õ–Ø–¶–ò–Ø –ü–û–†–¢–§–ï–õ–Ø:")
                print(f"   ‚Ä¢ –û–±—â–∏–π PnL: ${combined_pnl.sum():.2f}")
                print(f"   ‚Ä¢ –ú–∞–∫—Å. –¥–Ω–µ–≤–Ω–æ–π PnL: ${combined_pnl.max():.2f}")
                print(f"   ‚Ä¢ –ú–∏–Ω. –¥–Ω–µ–≤–Ω–æ–π PnL: ${combined_pnl.min():.2f}")
                print(f"   ‚Ä¢ –õ–∏–º–∏—Ç –ø–æ–∑–∏—Ü–∏–π: {cfg.portfolio.max_active_positions}")

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ PnL: {e}")
            return {"sharpe_ratio_abs": None, "total_trades": total_trades, "error_type": "aggregation_error", "error_message": str(e)}
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º equity curve
        equity_curve = cfg.portfolio.initial_capital + combined_pnl.cumsum()
        daily_returns = equity_curve.ffill().pct_change(fill_method=None).dropna()
        
        if len(daily_returns) == 0 or daily_returns.std() == 0:
            return {"sharpe_ratio_abs": None, "total_trades": total_trades, "error_type": "insufficient_data_for_sharpe"}
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º Sharpe ratio —Å –µ–¥–∏–Ω—ã–º annualization –¥–ª—è –∫—Ä–∏–ø—Ç–æ
        # –î–ª—è –∫—Ä–∏–ø—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º 365 –¥–Ω–µ–π, –∞ –Ω–µ 252 (–±–∏—Ä–∂–µ–≤—ã–µ –¥–Ω–∏)
        ann_factor = get_annualization_factor("1d", "sharpe")  # sqrt(365) ‚âà 19.1
        sharpe = daily_returns.mean() / daily_returns.std() * ann_factor
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –ø—Ä–æ—Å–∞–¥–∫—É
        max_dd = ((equity_curve.cummax() - equity_curve) / equity_curve.cummax()).max()

        avg_trade_size = combined_pnl.abs().mean() if len(combined_pnl) > 0 else 0
        commission_to_pnl_ratio = 0  # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è, –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å
        avg_hold_time = len(combined_pnl) / max(total_trades, 1)  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        micro_trades_pct = 0  # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è

        win_rate = 0.0
        avg_trade_size = 0.0
        avg_hold_time = 0.0

        if total_trades > 0 and len(all_pnls) > 0:
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—ç–∫—Ç–µ—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–¥–µ–ª–æ–∫
            all_trade_pnls = []
            all_hold_times = []

            # –î–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –µ—ë —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for pnl_series in all_pnls:
                if len(pnl_series) == 0:
                    continue

                # –°–æ–∑–¥–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—É—é –ø–æ–∑–∏—Ü–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ PnL (—É–ø—Ä–æ—â–µ–Ω–∏–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ä–∞—Å—á–µ—Ç–∞)
                # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ –ø–æ–ª—É—á–∞—Ç—å –ø–æ–∑–∏—Ü–∏—é –∏–∑ –±—ç–∫—Ç–µ—Å—Ç–µ—Ä–∞
                position = (pnl_series != 0).astype(int)

                # –ù–∞—Ö–æ–¥–∏–º —Å–¥–µ–ª–∫–∏ –ø–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º –ø–æ–∑–∏—Ü–∏–∏
                trade_start = (position.shift(fill_value=0) == 0) & (position != 0)
                trade_id = trade_start.cumsum()
                trade_id = trade_id.where(position != 0, None)

                if trade_id.notna().any():
                    # PnL –ø–æ —Å–¥–µ–ª–∫–∞–º
                    trade_pnls = pnl_series.groupby(trade_id).sum().dropna()
                    all_trade_pnls.extend(trade_pnls.tolist())

                    # –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–¥–µ–ª–æ–∫ –≤ –±–∞—Ä–∞—Ö
                    hold_bars = position.groupby(trade_id).sum().dropna()
                    all_hold_times.extend(hold_bars.tolist())

            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –≤—Å–µ–º —Å–¥–µ–ª–∫–∞–º
            if all_trade_pnls:
                win_rate = float(sum(1 for pnl in all_trade_pnls if pnl > 0) / len(all_trade_pnls))
                avg_trade_size = float(sum(abs(pnl) for pnl in all_trade_pnls) / len(all_trade_pnls))

            if all_hold_times:
                avg_hold_time = float(sum(all_hold_times) / len(all_hold_times))
        else:
            # Fallback –¥–ª—è —Å–ª—É—á–∞—è –∫–æ–≥–¥–∞ –Ω–µ—Ç —Å–¥–µ–ª–æ–∫ - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            if len(combined_pnl) > 0:
                winning_bars = sum(1 for pnl in combined_pnl if pnl > 0)
                win_rate = winning_bars / len(combined_pnl)
                avg_trade_size = combined_pnl.abs().mean()
                avg_hold_time = len(combined_pnl) / max(total_trades, 1)
        
        print(f"üìä –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
        print(f"   ‚Ä¢ –í—Å–µ–≥–æ –ø–∞—Ä –≤ —Ç–æ—Ä–≥–æ–≤–ª–µ: {len(all_pnls)}")
        print(f"   ‚Ä¢ –í—Å–µ–≥–æ —Å–¥–µ–ª–æ–∫: {total_trades}")
        print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —Å–¥–µ–ª–∫–∏: ${avg_trade_size:.2f}")
        print(f"   ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π hold-time: {avg_hold_time:.1f} –±–∞—Ä–æ–≤")
        print(f"   ‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞: {max_dd:.2%}")
        print(f"   ‚Ä¢ –û–±—â–∏–π P&L: ${combined_pnl.sum():.2f}")
        
        return {
            "sharpe_ratio_abs": float(sharpe),
            "total_trades": total_trades,
            "max_drawdown": float(max_dd),
            "max_drawdown_on_equity": float(max_dd),  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å objective.py
            "total_pnl": float(combined_pnl.sum()),
            "total_return_pct": float(combined_pnl.sum() / cfg.portfolio.initial_capital),
            "win_rate": float(win_rate),
            "avg_trade_size": float(avg_trade_size),
            "avg_hold_time": float(avg_hold_time)
        }

    def _extract_trades_from_results(self, results):
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–¥–µ–ª–∫–∞—Ö –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±—ç–∫—Ç–µ—Å—Ç–∞.
        
        Returns:
            Tuple[int, List[float]]: (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫, —Å–ø–∏—Å–æ–∫ PnL –ø–æ —Å–¥–µ–ª–∫–∞–º)
        """
        trade_count = 0
        trade_pnls = []
        
        if isinstance(results, dict):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–∑–∏—Ü–∏–∏ –∏ PnL
            positions = results.get('position', pd.Series())
            pnl_series = results.get('pnl', pd.Series())
            
            if not positions.empty and not pnl_series.empty:
                # –ù–∞—Ö–æ–¥–∏–º –º–æ–º–µ–Ω—Ç—ã –æ—Ç–∫—Ä—ã—Ç–∏—è –∏ –∑–∞–∫—Ä—ã—Ç–∏—è –ø–æ–∑–∏—Ü–∏–π
                position_changes = positions.diff().fillna(positions.iloc[0] if len(positions) > 0 else 0)
                
                # –û—Ç–∫—Ä—ã—Ç–∏–µ –ø–æ–∑–∏—Ü–∏–∏: –ø–µ—Ä–µ—Ö–æ–¥ –∏–∑ 0 –≤ –Ω–µ-0
                trade_starts = (positions.shift(1, fill_value=0) == 0) & (positions != 0)
                # –ó–∞–∫—Ä—ã—Ç–∏–µ –ø–æ–∑–∏—Ü–∏–∏: –ø–µ—Ä–µ—Ö–æ–¥ –∏–∑ –Ω–µ-0 –≤ 0
                trade_ends = (positions.shift(1, fill_value=0) != 0) & (positions == 0)
                
                # –ü–æ–¥—Å—á–µ—Ç —Å–¥–µ–ª–æ–∫
                trade_count = trade_starts.sum()
                
                # –†–∞—Å—á–µ—Ç PnL –ø–æ —Å–¥–µ–ª–∫–∞–º
                if trade_count > 0:
                    # –ú–∞—Ä–∫–∏—Ä—É–µ–º —Å–¥–µ–ª–∫–∏
                    trade_id = trade_starts.cumsum()
                    trade_id = trade_id.where(positions != 0, 0)
                    
                    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º PnL –ø–æ —Å–¥–µ–ª–∫–∞–º
                    for tid in range(1, trade_count + 1):
                        trade_mask = (trade_id == tid)
                        if trade_mask.any():
                            trade_pnl = pnl_series[trade_mask].sum()
                            trade_pnls.append(trade_pnl)
        
        return trade_count, trade_pnls

    def _backtest_single_pair(self, pair_row, cfg, step_df=None, normalization_stats=None, training_data=None, testing_data=None):
        """–ë—ç–∫—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–π –ø–∞—Ä—ã - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞–º–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏."""
        try:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ Series –∏–ª–∏ dict
            if hasattr(pair_row, 'to_dict'):
                # –ï—Å–ª–∏ —ç—Ç–æ pandas Series, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ dict
                pair_dict = pair_row.to_dict()
            else:
                # –ï—Å–ª–∏ —É–∂–µ dict
                pair_dict = pair_row
            
            s1, s2 = pair_dict['s1'], pair_dict['s2']
            beta, mean, std = pair_dict['beta'], pair_dict['mean'], pair_dict['std']
            
            # –û—Ç–ª–∞–¥–∫–∞
            # print(f"DEBUG _backtest_single_pair: s1={s1}, s2={s2}, beta={beta}, std={std}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ None/NaN –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞—Ö
            if beta is None or pd.isna(beta) or std is None or pd.isna(std) or std <= 0:
                print(f"DEBUG: –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—É {s1}/{s2} - beta={beta}, std={std}")
                return None, 0

            if training_data is None or testing_data is None:
                # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò:
                # –ï—Å–ª–∏ step_df –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω, –∑–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —à–∞–≥–∞ –∫–∞–∫ fallback
                if step_df is None:
                    print(f"‚ö†Ô∏è FALLBACK: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–∞—Ä—ã {s1}-{s2} (step_df –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω)")
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä–∏–æ–¥—ã —Ç–æ—á–Ω–æ –∫–∞–∫ –≤ _run_fast_backtest
                    start_date = pd.to_datetime(cfg.walk_forward.start_date)
                    bar_minutes = getattr(cfg.pair_selection, "bar_minutes", None) or 15
                    bar_delta = pd.Timedelta(minutes=bar_minutes)

                    current_test_start = start_date
                    training_start = current_test_start - pd.Timedelta(days=cfg.walk_forward.training_period_days)
                    training_end = current_test_start - bar_delta
                    testing_start = current_test_start
                    testing_end = testing_start + pd.Timedelta(days=cfg.walk_forward.testing_period_days)

                    testing_start = pd.to_datetime(testing_start)
                    testing_end = pd.to_datetime(testing_end)

                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã
                    step_data = self._load_data_for_step(training_start, training_end, testing_start, testing_end)
                    step_df = step_data['full_data']
                    training_data = step_data['training_data']
                    testing_data = step_data['testing_data']
                else:
                    # Fallback: —Ä–∞–∑–¥–µ–ª—è–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π step_df –ø–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Ç–µ–∫—É—â–µ–≥–æ –æ–∫–Ω–∞
                    bar_minutes = getattr(cfg.pair_selection, "bar_minutes", None) or 15
                    bar_delta = pd.Timedelta(minutes=bar_minutes)

                    training_start = step_df.index.min()
                    training_end = training_start + pd.Timedelta(days=cfg.walk_forward.training_period_days) - bar_delta
                    testing_start = training_end + bar_delta
                    testing_end = min(
                        testing_start + pd.Timedelta(days=cfg.walk_forward.testing_period_days),
                        step_df.index.max()
                    )

                    training_data = step_df.loc[training_start:training_end]
                    testing_data = step_df.loc[testing_start:testing_end]

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–∞—Ä—ã
            if s1 not in step_df.columns or s2 not in step_df.columns:
                print(f"DEBUG: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–∞—Ä—ã {s1}/{s2} –≤ step_df")
                return None, 0

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–∞—Ä—ã –≤ –æ–±–æ–∏—Ö –ø–µ—Ä–∏–æ–¥–∞—Ö
            if s1 not in training_data.columns or s2 not in training_data.columns:
                return None, 0
            if s1 not in testing_data.columns or s2 not in testing_data.columns:
                return None, 0

            training_pair_data = training_data[[s1, s2]].dropna()
            testing_pair_data = testing_data[[s1, s2]].dropna()

            if len(training_pair_data) < cfg.backtest.rolling_window + 10:
                return None, 0
            if len(testing_pair_data) < cfg.backtest.rolling_window + 10:
                return None, 0

            # –ù–ï –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –±—ç–∫—Ç–µ—Å—Ç–µ—Ä–∞!
            # FullNumbaPairBacktester –æ–∂–∏–¥–∞–µ—Ç –°–´–†–´–ï —Ü–µ–Ω—ã –∏ —Å–∞–º –≤—ã—á–∏—Å–ª—è–µ—Ç z-scores
            # –ü–µ—Ä–µ–¥–∞–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞–ø—Ä—è–º—É—é
            raw_pair_data = testing_pair_data.copy()
            raw_pair_data.columns = ['symbol1', 'symbol2']
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø—É—Å—Ç—ã–µ
            if raw_pair_data.empty:
                print(f"   ‚ö†Ô∏è –ü—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–∞—Ä—ã {s1}-{s2}")
                return None, 0

            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø–æ—Ä—Ç—Ñ–µ–ª—å –¥–ª—è —ç—Ç–æ–π –ø–∞—Ä—ã
            temp_portfolio = Portfolio(
                initial_capital=cfg.portfolio.initial_capital,
                max_active_positions=1,
                config=cfg.portfolio,
            )

            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è cooldown_hours -> cooldown_periods –¥–ª—è –±—ç–∫—Ç–µ—Å—Ç–µ—Ä–∞
            bar_minutes = getattr(cfg.pair_selection, "bar_minutes", None) or 15
            cooldown_periods = getattr(cfg.backtest, "cooldown_periods", 0) or 0
            cooldown_hours = getattr(cfg.backtest, "cooldown_hours", None)
            if cooldown_hours is not None:
                cooldown_periods = self.convert_hours_to_periods(cooldown_hours, bar_minutes)

            # –£–°–ö–û–†–ï–ù–ò–ï: –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é Numba-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±—ç–∫—Ç–µ—Å—Ç–µ—Ä –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è
            # –í–ê–ñ–ù–û: –ü–µ—Ä–µ–¥–∞–µ–º –°–´–†–´–ï —Ü–µ–Ω—ã, –±—ç–∫—Ç–µ—Å—Ç–µ—Ä —Å–∞–º –≤—ã—á–∏—Å–ª–∏—Ç z-scores
            # –ö–†–ò–¢–ò–ß–ù–û: –ü–µ—Ä–µ–¥–∞–µ–º beta –∏–∑ –∫–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ä–∞—Å—á–µ—Ç–∞ —Å–ø—Ä–µ–¥–∞
            backtester = FullNumbaPairBacktester(
                pair_data=raw_pair_data,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ!
                beta=beta,  # –ü–µ—Ä–µ–¥–∞–µ–º beta –∏–∑ –æ—Ç–±–æ—Ä–∞ –ø–∞—Ä!
                rolling_window=cfg.backtest.rolling_window,
                z_threshold=cfg.backtest.zscore_threshold,
                z_exit=getattr(cfg.backtest, 'zscore_exit', 0.0),
                commission_pct=getattr(cfg.backtest, 'commission_pct', 0.0),
                slippage_pct=getattr(cfg.backtest, 'slippage_pct', 0.0),
                cooldown_periods=cooldown_periods,
                portfolio=temp_portfolio,
                capital_at_risk=cfg.portfolio.initial_capital,
            )

            # –ó–∞–ø—É—Å–∫–∞–µ–º –±—ç–∫—Ç–µ—Å—Ç (FullNumbaPairBacktester –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏–º–µ–Ω —Å–∏–º–≤–æ–ª–æ–≤)
            results = backtester.run_numba_full()

            if results is None:
                print(f"‚ö†Ô∏è –†–µ–∑—É–ª—å—Ç–∞—Ç –±—ç–∫—Ç–µ—Å—Ç–∞ –¥–ª—è {s1}-{s2} —Ä–∞–≤–µ–Ω None")
                return None, 0
            
            # FullNumbaPairBacktester –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç FullNumbaBacktestResult –æ–±—ä–µ–∫—Ç
            # —Å –ø–æ–ª—è–º–∏: pnl_series, trades_series, total_pnl
            if not hasattr(results, 'pnl_series'):
                print(f"‚ö†Ô∏è –†–µ–∑—É–ª—å—Ç–∞—Ç –±—ç–∫—Ç–µ—Å—Ç–∞ –¥–ª—è {s1}-{s2} –Ω–µ –∏–º–µ–µ—Ç pnl_series, —Ç–∏–ø: {type(results)}")
                return None, 0

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ—Ç—É PnL —Å–µ—Ä–∏–∏
            pnl_series = results.pnl_series
            if pnl_series is None or len(pnl_series) == 0:
                return None, 0

            if isinstance(pnl_series, np.ndarray):
                pnl_series = pd.Series(pnl_series, index=raw_pair_data.index)

            pair_trades = 0
            if hasattr(results, 'trades_series'):
                # –°—á–∏—Ç–∞–µ–º –Ω–µ–Ω—É–ª–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ trades_series
                pair_trades = int(np.sum(results.trades_series != 0))
            else:
                # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–±: —Å—á–∏—Ç–∞–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–π
                if hasattr(results, 'positions'):
                    position_changes = np.diff(results.positions)
                    pair_trades = int(np.sum(position_changes != 0))

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º PnL —Å–µ—Ä–∏—é –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫

            return pnl_series, pair_trades

        except Exception as e:
            import traceback
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–∞—Ä—ã {pair_dict.get('s1', 'unknown') if 'pair_dict' in locals() else 'unknown'}: {e}")
            # –î–æ–±–∞–≤–∏–º –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            if "NoneType" in str(e):
                print(f"   DEBUG: pair_dict —Å–æ–¥–µ—Ä–∂–∏—Ç: {pair_dict.keys() if 'pair_dict' in locals() and isinstance(pair_dict, dict) else '–Ω–µ —Å–ª–æ–≤–∞—Ä—å'}")
                print(f"   DEBUG: normalization_stats —Ç–∏–ø: {type(normalization_stats)}")
                print(f"   DEBUG: normalization_stats —Å–æ–¥–µ—Ä–∂–∏—Ç: {normalization_stats.keys() if isinstance(normalization_stats, dict) else '–Ω–µ —Å–ª–æ–≤–∞—Ä—å'}")
                traceback.print_exc()
            return None, 0

    def _simulate_realistic_portfolio(self, all_pnls, cfg):
        """
        –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è —Å–∏–º—É–ª—è—Ü–∏—è –ø–æ—Ä—Ç—Ñ–µ–ª—è —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –ø–æ–∑–∏—Ü–∏—è–º–∏.

        –í–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ–≥–æ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏—è PnL –≤—Å–µ—Ö –ø–∞—Ä, —Å–∏–º—É–ª–∏—Ä—É–µ–º —Ä–µ–∞–ª—å–Ω—É—é —Ä–∞–±–æ—Ç—É –ø–æ—Ä—Ç—Ñ–µ–ª—è:
        1. –ù–∞ –∫–∞–∂–¥–æ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–º —à–∞–≥–µ —Å–æ–±–∏—Ä–∞–µ–º —Å–∏–≥–Ω–∞–ª—ã –æ—Ç –≤—Å–µ—Ö –ø–∞—Ä
        2. –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–∏–º–∏—Ç max_active_positions
        3. –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–µ —Å–∏–≥–Ω–∞–ª—ã –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏
        4. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π PnL –ø–æ—Ä—Ç—Ñ–µ–ª—è

        Args:
            all_pnls: –°–ø–∏—Å–æ–∫ PnL —Å–µ—Ä–∏–π –æ—Ç –≤—Å–µ—Ö –ø–∞—Ä
            cfg: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ—Ä—Ç—Ñ–µ–ª—è

        Returns:
            pd.Series: –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π PnL –ø–æ—Ä—Ç—Ñ–µ–ª—è —Å —É—á–µ—Ç–æ–º –ª–∏–º–∏—Ç–æ–≤ –ø–æ–∑–∏—Ü–∏–π
        """
        if not all_pnls:
            return pd.Series(dtype=float)

        # –°–æ–∑–¥–∞–µ–º DataFrame —Å–æ –≤—Å–µ–º–∏ PnL —Å–µ—Ä–∏—è–º–∏
        pnl_df = pd.concat({f'pair_{i}': pnl.fillna(0) for i, pnl in enumerate(all_pnls)}, axis=1)

        # –°–æ–∑–¥–∞–µ–º DataFrame —Å —Å–∏–≥–Ω–∞–ª–∞–º–∏ (–ø–æ–∑–∏—Ü–∏—è –∞–∫—Ç–∏–≤–Ω–∞ –µ—Å–ª–∏ PnL != 0)
        signals_df = pd.concat({f'pair_{i}': (pnl != 0).astype(int) for i, pnl in enumerate(all_pnls)}, axis=1)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Ä—Ç—Ñ–µ–ª—å
        max_positions = cfg.portfolio.max_active_positions
        portfolio_pnl = pd.Series(0.0, index=pnl_df.index)
        active_positions = {}  # {pair_name: entry_timestamp}

        print(f"üéØ –°–ò–ú–£–õ–Ø–¶–ò–Ø –ü–û–†–¢–§–ï–õ–Ø: {len(all_pnls)} –ø–∞—Ä, –ª–∏–º–∏—Ç {max_positions} –ø–æ–∑–∏—Ü–∏–π")

        # –°–∏–º—É–ª–∏—Ä—É–µ–º —Ç–æ—Ä–≥–æ–≤–ª—é –ø–æ –∫–∞–∂–¥–æ–º—É –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É —à–∞–≥—É
        for timestamp in pnl_df.index:
            current_signals = signals_df.loc[timestamp]
            current_pnls = pnl_df.loc[timestamp]

            # 1. –ó–∞–∫—Ä—ã–≤–∞–µ–º –ø–æ–∑–∏—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –±–æ–ª—å—à–µ –Ω–µ –∞–∫—Ç–∏–≤–Ω—ã
            positions_to_close = []
            for pair_name in list(active_positions.keys()):
                if current_signals[pair_name] == 0:  # –°–∏–≥–Ω–∞–ª –∏—Å—á–µ–∑
                    positions_to_close.append(pair_name)

            for pair_name in positions_to_close:
                del active_positions[pair_name]

            # 2. –ò—â–µ–º –Ω–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã –¥–ª—è –æ—Ç–∫—Ä—ã—Ç–∏—è –ø–æ–∑–∏—Ü–∏–π
            new_signals = []
            for pair_name in current_signals.index:
                if current_signals[pair_name] == 1 and pair_name not in active_positions:
                    # –ù–æ–≤—ã–π —Å–∏–≥–Ω–∞–ª –æ—Ç –ø–∞—Ä—ã, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ –≤ –ø–æ—Ä—Ç—Ñ–µ–ª–µ
                    new_signals.append((pair_name, abs(current_pnls[pair_name])))

            # 3. –°–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã –ø–æ —Å–∏–ª–µ (–∞–±—Å–æ–ª—é—Ç–Ω—ã–π PnL)
            new_signals.sort(key=lambda x: x[1], reverse=True)

            # 4. –û—Ç–∫—Ä—ã–≤–∞–µ–º –Ω–æ–≤—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ª–∏–º–∏—Ç–∞
            available_slots = max_positions - len(active_positions)
            for i, (pair_name, signal_strength) in enumerate(new_signals):
                if i >= available_slots:
                    break  # –î–æ—Å—Ç–∏–≥–ª–∏ –ª–∏–º–∏—Ç–∞ –ø–æ–∑–∏—Ü–∏–π
                active_positions[pair_name] = timestamp

            # 5. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º PnL –ø–æ—Ä—Ç—Ñ–µ–ª—è –Ω–∞ —ç—Ç–æ–º —à–∞–≥–µ
            step_pnl = 0.0
            for pair_name in active_positions:
                step_pnl += current_pnls[pair_name]

            portfolio_pnl[timestamp] = step_pnl

        # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        total_signals = signals_df.sum(axis=1)
        avg_active_pairs = len([p for p in active_positions]) if active_positions else 0
        max_signals = total_signals.max()
        avg_signals = total_signals.mean()

        print(f"   üìà –ú–∞–∫—Å. –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤: {max_signals}")
        print(f"   üìä –°—Ä–µ–¥–Ω. —Å–∏–≥–Ω–∞–ª–æ–≤ –∑–∞ –ø–µ—Ä–∏–æ–¥: {avg_signals:.1f}")
        print(f"   üéØ –§–∏–Ω–∞–ª—å–Ω—ã—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π: {avg_active_pairs}")

        utilization = (total_signals.clip(upper=max_positions) / max_positions).mean()
        print(f"   ‚ö° –£—Ç–∏–ª–∏–∑–∞—Ü–∏—è –ª–∏–º–∏—Ç–∞ –ø–æ–∑–∏—Ü–∏–π: {utilization:.1%}")

        return portfolio_pnl

    def _run_fast_backtest_with_reports(self, params, trial):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –±—ã—Å—Ç—Ä—ã–π –±—ç–∫—Ç–µ—Å—Ç —Å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º–∏ –æ—Ç—á–µ—Ç–∞–º–∏ –¥–ª—è pruning."""

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –ª–æ–≥–∏–∫—É —á—Ç–æ –∏ –≤ _run_fast_backtest, –Ω–æ —Å –æ—Ç—á–µ—Ç–∞–º–∏
        cfg = self.base_config.model_copy(deep=True)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (—Å–æ–∫—Ä–∞—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)

        for key, value in params.items():
            if key in ["ssd_top_n", "kpss_pvalue_threshold", "coint_pvalue_threshold",
                      "min_half_life_days", "max_half_life_days", "min_mean_crossings"]:
                if hasattr(cfg, 'pair_selection'):
                    setattr(cfg.pair_selection, key, value)
            elif key in ["zscore_threshold", "zscore_exit", "rolling_window", "stop_loss_multiplier",
                        "time_stop_multiplier", "cooldown_hours", "commission_pct", "slippage_pct"]:
                if hasattr(cfg, 'backtest'):
                    setattr(cfg.backtest, key, value)
            elif key in ["max_active_positions", "risk_per_position_pct", "max_position_size_pct"]:
                if hasattr(cfg, 'portfolio'):
                    setattr(cfg.portfolio, key, value)
            elif key in ["normalization_method", "min_history_ratio"]:
                if hasattr(cfg, 'data_processing'):
                    setattr(cfg.data_processing, key, value)

        if hasattr(cfg, 'backtest'):
            cfg.backtest.zscore_entry_threshold = cfg.backtest.zscore_threshold

        start_date = pd.to_datetime(cfg.walk_forward.start_date)
        end_date = pd.to_datetime(getattr(cfg.walk_forward, 'end_date', start_date + pd.Timedelta(days=cfg.walk_forward.testing_period_days)))
        step_size_days = _resolve_step_size_days(cfg)

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Å–µ walk-forward —à–∞–≥–∏
        walk_forward_steps = []
        current_test_start = start_date
        bar_minutes = getattr(cfg.pair_selection, "bar_minutes", None) or 15
        bar_delta = pd.Timedelta(minutes=bar_minutes)

        if step_size_days < cfg.walk_forward.testing_period_days:
            logger.warning(
                f"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: step_size_days ({step_size_days}) < testing_period_days ({cfg.walk_forward.testing_period_days}). "
                f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º step_size_days = testing_period_days –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤."
            )
            step_size_days = cfg.walk_forward.testing_period_days

        while current_test_start < end_date:
            training_start = current_test_start - pd.Timedelta(days=cfg.walk_forward.training_period_days)
            training_end = current_test_start - bar_delta
            testing_start = current_test_start
            testing_end = min(testing_start + pd.Timedelta(days=cfg.walk_forward.testing_period_days), end_date)

            walk_forward_steps.append({
                'training_start': training_start,
                'training_end': training_end,
                'testing_start': testing_start,
                'testing_end': testing_end
            })

            current_test_start += pd.Timedelta(days=step_size_days)

        max_steps = getattr(cfg.walk_forward, 'max_steps', None)
        if max_steps is not None:
            try:
                max_steps = int(max_steps)
            except (TypeError, ValueError):
                max_steps = None

        if max_steps and max_steps > 0:
            walk_forward_steps = walk_forward_steps[:max_steps]

        print(f"\nüîÑ –ì–ï–ù–ï–†–ò–†–û–í–ê–ù–û {len(walk_forward_steps)} WALK-FORWARD –®–ê–ì–û–í (—Å –æ—Ç—á–µ—Ç–∞–º–∏)")

        all_step_results = []

        for step_idx, step in enumerate(walk_forward_steps):
            print(f"\nüîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ walk-forward —à–∞–≥–∞ {step_idx + 1}/{len(walk_forward_steps)} (—Å –æ—Ç—á–µ—Ç–∞–º–∏)")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç—Ç–æ–≥–æ —à–∞–≥–∞
            step_data = self._load_data_for_step(
                step['training_start'], step['training_end'],
                step['testing_start'], step['testing_end']
            )
            step_df = step_data['full_data']

            if step_df is None:
                print(f"   ‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —à–∞–≥–∞ {step_idx + 1}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —ç—Ç–æ—Ç —à–∞–≥
            step_result = self._process_single_walk_forward_step(
                cfg, step_data, step_idx
            )

            if step_result is not None and step_result['pnls']:
                all_step_results.append(step_result)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        if not all_step_results:
            print("‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ walk-forward —à–∞–≥–∞")
            return {"sharpe_ratio_abs": PENALTY_SOFT, "total_trades": 0, "max_drawdown": 0, "win_rate": 0}

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö —à–∞–≥–æ–≤
        all_pnls = []
        total_trades = 0

        for step_result in all_step_results:
            all_pnls.extend(step_result['pnls'])
            total_trades += step_result['trades']

        print(f"\nüìä –ê–ì–†–ï–ì–ò–†–û–í–ê–ù–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–°–ï–• {len(all_step_results)} –®–ê–ì–û–í (—Å –æ—Ç—á–µ—Ç–∞–º–∏):")
        print(f"   üìà –í—Å–µ–≥–æ PnL —Å–µ—Ä–∏–π: {len(all_pnls)}")
        print(f"   üîÑ –í—Å–µ–≥–æ —Å–¥–µ–ª–æ–∫: {total_trades}")

        accumulated_pnls = []

        # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –æ—Ç—á–µ—Ç—ã –ø–æ —à–∞–≥–∞–º –¥–ª—è pruning
        for step_idx, step_result in enumerate(all_step_results):
            try:
                # –î–æ–±–∞–≤–ª—è–µ–º PnL —Ç–µ–∫—É—â–µ–≥–æ —à–∞–≥–∞ –∫ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º
                step_pnls = step_result['pnls']
                if step_pnls:
                    accumulated_pnls.extend(step_pnls)

                    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—É—é –º–µ—Ç—Ä–∏–∫—É –Ω–∞ –ù–ê–ö–û–ü–õ–ï–ù–ù–´–• –¥–∞–Ω–Ω—ã—Ö –¥–æ —Ç–µ–∫—É—â–µ–≥–æ —à–∞–≥–∞
                    if len(accumulated_pnls) > 0:

                        if len(accumulated_pnls) == 1:
                            combined_pnl = accumulated_pnls[0].fillna(0)
                        else:
                            combined_pnl = self._simulate_realistic_portfolio(accumulated_pnls, cfg)
                        equity_curve = cfg.portfolio.initial_capital + combined_pnl.cumsum()
                        daily_returns = equity_curve.resample('1D').last().ffill().pct_change(fill_method=None).dropna()

                        if len(daily_returns) > 0 and daily_returns.std() > 0:
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –µ–¥–∏–Ω—ã–π annualization factor –¥–ª—è –∫—Ä–∏–ø—Ç–æ
                            ann_factor = get_annualization_factor("1d", "sharpe")  # sqrt(365)
                            intermediate_sharpe = daily_returns.mean() / daily_returns.std() * ann_factor

                            # –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–∏–º–µ–Ω—è–µ–º —à—Ç—Ä–∞—Ñ—ã –¥–ª—è pruner'–∞, —á—Ç–æ–±—ã –æ–Ω –ø—Ä–∏–Ω–∏–º–∞–ª –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è


                            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –Ω–µ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è


                            if intermediate_sharpe > 10.0:


                                penalized_sharpe = 10.0  # Cap –Ω–∞ –Ω–µ—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ –≤—ã—Å–æ–∫–∏–µ


                            elif intermediate_sharpe < -10.0:


                                penalized_sharpe = -10.0  # Cap –Ω–∞ –æ—á–µ–Ω—å –Ω–∏–∑–∫–∏–µ


                            else:


                                penalized_sharpe = intermediate_sharpe


                            


                            trial.report(float(penalized_sharpe), step=step_idx)
                            print(f"   üìä –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π –æ—Ç—á–µ—Ç —à–∞–≥ {step_idx}: Sharpe={intermediate_sharpe:.4f} (reported={penalized_sharpe:.4f})")

                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º pruning
                            if trial.should_prune():
                                print(f"Trial pruned at walk-forward step {step_idx} (—à–∞–≥ {step_idx + 1}/{len(all_step_results)})")
                                raise optuna.TrialPruned(f"Pruned at step {step_idx}")

            except optuna.TrialPruned:
                raise  # –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º pruning
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –¥–ª—è —à–∞–≥–∞ {step_idx + 1}: {e}")

        # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        if not all_pnls:
            return {"sharpe_ratio_abs": PENALTY_SOFT, "total_trades": total_trades, "max_drawdown": 0, "win_rate": 0}

        try:

            if len(all_pnls) == 1:
                combined_pnl = all_pnls[0].fillna(0)
            else:
                combined_pnl = self._simulate_realistic_portfolio(all_pnls, cfg)
            equity_curve = cfg.portfolio.initial_capital + combined_pnl.cumsum()

            daily_returns = equity_curve.resample('1D').last().ffill().pct_change(fill_method=None).dropna()

            if len(daily_returns) == 0 or daily_returns.std() == 0:
                return {"sharpe_ratio_abs": PENALTY_SOFT, "total_trades": total_trades, "max_drawdown": 0, "win_rate": 0}

            # –î–ª—è –∫—Ä–∏–ø—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º 365 –¥–Ω–µ–π, —Ç–∞–∫ –∫–∞–∫ —Ç–æ—Ä–≥–æ–≤–ª—è –∏–¥–µ—Ç 365 –¥–Ω–µ–π –≤ –≥–æ–¥—É
            raw_sharpe = daily_returns.mean() / daily_returns.std() * np.sqrt(365)
            
            # –í–ê–õ–ò–î–ê–¶–ò–Ø: –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ä–∞—Å—á–µ—Ç–∞ Sharpe
            validation_result = self.sharpe_validator.validate_sharpe(
                raw_sharpe, 
                pnl_series=combined_pnl,
                num_trades=total_trades
            )
            
            if not validation_result.is_valid:
                logger.warning(f"‚ö†Ô∏è Sharpe –≤–∞–ª–∏–¥–∞—Ü–∏—è: {validation_result.issue}")
                sharpe = validation_result.sharpe_ratio  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            else:
                sharpe = raw_sharpe
            running_max = equity_curve.expanding().max()
            drawdown = (equity_curve - running_max) / running_max
            max_dd = abs(drawdown.min()) if len(drawdown) > 0 else 0

            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ PnL —Å–¥–µ–ª–æ–∫ –∏–∑ –≤—Å–µ—Ö —à–∞–≥–æ–≤
            all_trade_pnls = []
            for step_result in all_step_results:
                if 'trade_pnls' in step_result:
                    all_trade_pnls.extend(step_result['trade_pnls'])
                elif 'trades_log' in step_result and step_result['trades_log']:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º PnL –∏–∑ trades_log –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
                    for trade in step_result['trades_log']:
                        if isinstance(trade, dict) and 'pnl' in trade:
                            all_trade_pnls.append(trade['pnl'])

            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º win_rate –ø–æ —Å–¥–µ–ª–∫–∞–º
            if all_trade_pnls:
                win_rate = float(sum(1 for pnl in all_trade_pnls if pnl > 0) / len(all_trade_pnls))
            else:
                # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–Ω–µ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–¥–µ–ª–∫–∞—Ö
                daily_pnl = combined_pnl.resample('1D').sum()
                win_rate = float((daily_pnl > 0).mean()) if len(daily_pnl) > 0 else 0.0

            return {"sharpe_ratio_abs": sharpe, "total_trades": total_trades, "max_drawdown": max_dd, "win_rate": win_rate}

        except (ValueError, KeyError, TypeError) as e:
            print(f"–û—à–∏–±–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å—á–µ—Ç–∞: {e}")
            return {"sharpe_ratio_abs": PENALTY_HARD, "total_trades": total_trades, "max_drawdown": 0, "win_rate": 0}

    def _simulate_realistic_portfolio(self, all_pnls, cfg):
        """
        –°–∏–º—É–ª–∏—Ä—É–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –ø–æ—Ä—Ç—Ñ–µ–ª—è —Å —É—á–µ—Ç–æ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∫–∞–ø–∏—Ç–∞–ª–∞.
        
        Args:
            all_pnls: –°–ø–∏—Å–æ–∫ PnL —Å–µ—Ä–∏–π –æ—Ç —Ä–∞–∑–Ω—ã—Ö –ø–∞—Ä
            cfg: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –±—ç–∫—Ç–µ—Å—Ç–∞
            
        Returns:
            pd.Series: –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è PnL —Å–µ—Ä–∏—è –ø–æ—Ä—Ç—Ñ–µ–ª—è
        """
        if not all_pnls:
            return pd.Series()
            
        # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –≤—Å–µ —Å–µ—Ä–∏–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        aligned_pnls = pd.DataFrame(all_pnls).T
        
        # –£—á–∏—Ç—ã–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π
        max_positions = getattr(cfg.backtest, 'max_active_positions', 15)
        
        # –ü—Ä–æ—Å—Ç–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: —Ä–∞–≤–Ω–æ–≤–µ—Å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞–ø–∏—Ç–∞–ª–∞
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ —É—á–∏—Ç—ã–≤–∞—Ç—å —Å–∏–≥–Ω–∞–ª—ã –≤—Ö–æ–¥–∞/–≤—ã—Ö–æ–¥–∞
        weights = 1.0 / min(max_positions, len(all_pnls))
        
        # –°—É–º–º–∏—Ä—É–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω—ã–µ PnL
        portfolio_pnl = (aligned_pnls * weights).sum(axis=1).fillna(0)
        
        return portfolio_pnl
        
    def quick_trial_filter(self, params):
        """
        –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ–¥–æ–º–æ –ø–ª–æ—Ö–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.

        Args:
            params: –°–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ trial

        Returns:
            tuple: (is_valid, reason) - –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∏ –ø—Ä–∏—á–∏–Ω–∞ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–≥–∏—á–Ω–æ—Å—Ç—å —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        zscore_threshold = params.get('zscore_threshold', 1.0)
        zscore_exit = params.get('zscore_exit', 0.3)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: zscore_exit –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –º–µ–Ω—å—à–µ zscore_threshold
        if zscore_exit >= zscore_threshold:
            return False, f"zscore_exit ({zscore_exit:.3f}) >= zscore_threshold ({zscore_threshold:.3f})"

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –†–∞–∑—É–º–Ω—ã–π –≥–∏—Å—Ç–µ—Ä–µ–∑–∏—Å (—Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –ø–æ—Ä–æ–≥–∞–º–∏)
        hysteresis = zscore_threshold - zscore_exit
        if hysteresis < 0.2:  # –£–≤–µ–ª–∏—á–µ–Ω –º–∏–Ω–∏–º—É–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            return False, f"–°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π –≥–∏—Å—Ç–µ—Ä–µ–∑–∏—Å: {hysteresis:.3f}"
        if hysteresis > 3.0:  # –£–≤–µ–ª–∏—á–µ–Ω –ª–∏–º–∏—Ç –¥–ª—è –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
            return False, f"–°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π –≥–∏—Å—Ç–µ—Ä–µ–∑–∏—Å: {hysteresis:.3f}"

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 3: –†–∞–∑—É–º–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –ø–æ–∑–∏—Ü–∏–π
        risk_per_position = params.get('risk_per_position_pct', 0.02)
        max_position_size = params.get('max_position_size_pct', 0.1)
        max_positions = params.get('max_active_positions', 15)

        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–∫—Å–ø–æ–∑–∏—Ü–∏—è –Ω–µ –¥–æ–ª–∂–Ω–∞ –ø—Ä–µ–≤—ã—à–∞—Ç—å 100%
        max_exposure = risk_per_position * max_positions
        if max_exposure > 1.0:
            return False, f"–°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è —ç–∫—Å–ø–æ–∑–∏—Ü–∏—è: {max_exposure:.1%}"

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 4: –†–∞–∑—É–º–Ω—ã–µ —Å—Ç–æ–ø-–ª–æ—Å—Å—ã
        stop_loss_mult = params.get('stop_loss_multiplier', 3.0)
        time_stop_mult = params.get('time_stop_multiplier', 5.0)

        if stop_loss_mult < 1.5:
            return False, f"–°–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π —Å—Ç–æ–ø-–ª–æ—Å—Å: {stop_loss_mult}"
        if time_stop_mult < stop_loss_mult:
            return False, f"time_stop_multiplier ({time_stop_mult}) < stop_loss_multiplier ({stop_loss_mult})"

        return True, "OK"

    def _get_cached_data(self, cache_key):
        """
        –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫—ç—à–∞.

        Args:
            cache_key: –ö–ª—é—á –∫—ç—à–∞

        Returns:
            –î–∞–Ω–Ω—ã–µ –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã
        """
        with self.data_cache_lock:
            return self.data_cache.get(cache_key)

    def _cache_data(self, cache_key, data):
        """
        –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –∫—ç—à —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞.

        Args:
            cache_key: –ö–ª—é—á –∫—ç—à–∞
            data: –î–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        with self.data_cache_lock:
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
            if len(self.data_cache) >= self.max_cache_size:
                # –£–¥–∞–ª—è–µ–º —Å–∞–º—ã–π —Å—Ç–∞—Ä—ã–π —ç–ª–µ–º–µ–Ω—Ç (FIFO)
                oldest_key = next(iter(self.data_cache))
                del self.data_cache[oldest_key]

            self.data_cache[cache_key] = data

    def __call__(self, trial_or_params):
        """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å objective.py.

        Args:
            trial_or_params: optuna.Trial –æ–±—ä–µ–∫—Ç –∏–ª–∏ —Å–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

        Returns:
            float: –ó–Ω–∞—á–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
        """
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à –≤ –∫–∞–∂–¥–æ–º –¥–æ—á–µ—Ä–Ω–µ–º –ø—Ä–æ—Ü–µ—Å—Å–µ
        from coint2.core.global_rolling_cache import get_global_rolling_manager
        manager = get_global_rolling_manager()
        if not manager.initialized:
            print(f"üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫—ç—à–∞ –≤ –¥–æ—á–µ—Ä–Ω–µ–º –ø—Ä–æ—Ü–µ—Å—Å–µ (PID: {os.getpid()})")
            cache_initialized = self._initialize_global_rolling_cache()
            if cache_initialized:
                print(f"‚úÖ –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ {os.getpid()}")
            else:
                print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫—ç—à –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ {os.getpid()}")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if hasattr(trial_or_params, 'suggest_float'):  # –≠—Ç–æ optuna.Trial
            trial = trial_or_params
            params = self._suggest_parameters(trial)
            trial_number = trial.number
        else:  # –≠—Ç–æ —Å–ª–æ–≤–∞—Ä—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            params = trial_or_params
            trial_number = params.get("trial_number", -1)

        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        is_valid, reason = self.quick_trial_filter(params)
        if not is_valid:
            logger.info(f"Trial #{trial_number}: –ë—ã—Å—Ç—Ä–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω - {reason}")
            if hasattr(trial_or_params, 'suggest_float') and hasattr(trial_or_params, "set_user_attr"):
                trial_or_params.set_user_attr("quick_filter_reason", reason)
                raise optuna.TrialPruned(f"Quick filter: {reason}")
            return PENALTY_SOFT

        try:

            try:
                validated_params = validate_params(params)
            except ValueError as e:
                logger.warning(f"Trial #{trial_number}: –ù–µ–≤–∞–ª–∏–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {e}")
                if hasattr(trial_or_params, 'suggest_float') and hasattr(trial_or_params, "set_user_attr"):
                    trial_or_params.set_user_attr("error_type", "validation_error")
                    trial_or_params.set_user_attr("validation_message", str(e))
                    trial_or_params.set_user_attr("invalid_params", params)
                    raise optuna.TrialPruned(f"Parameter validation failed: {e}")

                return PENALTY_SOFT
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –±—ã—Å—Ç—Ä—ã–π –±—ç–∫—Ç–µ—Å—Ç —Å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–º–∏ –æ—Ç—á–µ—Ç–∞–º–∏ (–µ—Å–ª–∏ —ç—Ç–æ trial)
            if hasattr(trial_or_params, 'suggest_float'):
                metrics = self._run_fast_backtest_with_reports(validated_params, trial)
            else:
                metrics = self._run_fast_backtest(validated_params)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –µ–¥–∏–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é extract_sharpe
            sharpe = extract_sharpe(metrics)

            if sharpe is None or not isinstance(sharpe, (int, float)) or np.isnan(sharpe) or np.isinf(sharpe):
                logger.warning(f"Trial #{trial_number}: –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π Sharpe ratio: {sharpe}")
                if hasattr(trial_or_params, 'suggest_float') and hasattr(trial_or_params, "set_user_attr"):
                    trial_or_params.set_user_attr("error_type", "invalid_sharpe")
                    trial_or_params.set_user_attr("sharpe_value", str(sharpe))
                    trial_or_params.set_user_attr("metrics_available", list(metrics.keys()) if metrics else [])
                    raise optuna.TrialPruned(f"Invalid Sharpe ratio: {sharpe}")

                return PENALTY_SOFT
            
            logger.debug(f"Trial #{trial_number}: {metrics.get('total_trades', 0)} —Å–¥–µ–ª–æ–∫, Sharpe: {sharpe:.4f}")

            max_dd = metrics.get("max_drawdown", 0)

            win_rate = metrics.get('win_rate', 0.0)

            # –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è win_rate –±–æ–Ω—É—Å–æ–≤/—à—Ç—Ä–∞—Ñ–æ–≤
            WIN_RATE_BONUS_THRESHOLD = 0.55  # 55% win rate –¥–ª—è –±–æ–Ω—É—Å–∞
            WIN_RATE_BONUS_MULTIPLIER = 0.5  # –ú–Ω–æ–∂–∏—Ç–µ–ª—å –±–æ–Ω—É—Å–∞
            WIN_RATE_PENALTY_THRESHOLD = 0.40  # 40% win rate –¥–ª—è —à—Ç—Ä–∞—Ñ–∞
            WIN_RATE_PENALTY_MULTIPLIER = 1.0  # –ú–Ω–æ–∂–∏—Ç–µ–ª—å —à—Ç—Ä–∞—Ñ–∞

            # –ë–æ–Ω—É—Å –∑–∞ –≤—ã—Å–æ–∫–∏–π win rate (> 55%)
            win_rate_bonus = 0
            if win_rate > WIN_RATE_BONUS_THRESHOLD:
                win_rate_bonus = (win_rate - WIN_RATE_BONUS_THRESHOLD) * WIN_RATE_BONUS_MULTIPLIER

            # –®—Ç—Ä–∞—Ñ –∑–∞ –Ω–∏–∑–∫–∏–π win rate (< 40%)
            win_rate_penalty = 0
            if win_rate < WIN_RATE_PENALTY_THRESHOLD:
                win_rate_penalty = (WIN_RATE_PENALTY_THRESHOLD - win_rate) * WIN_RATE_PENALTY_MULTIPLIER

            total_trades = metrics.get('total_trades', 0)
            win_rate = metrics.get('win_rate', 0.0)  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—É—á–µ–Ω–∏–µ win_rate –∏–∑ metrics
            logger.debug(f"Trial #{trial_number}: {total_trades} —Å–¥–µ–ª–æ–∫, Sharpe: {sharpe:.4f}")

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è walk-forward (—Ç–∞–∫ –∫–∞–∫ –ø–µ—Ä–∏–æ–¥ –∫–æ—Ä–æ—Ç–∫–∏–π)
            min_trades_wf = 5  # –î–ª—è walk-forward –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ 5 —Å–¥–µ–ª–æ–∫
            if total_trades < min_trades_wf:
                logger.warning(f"Trial #{trial_number}: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–¥–µ–ª–æ–∫ ({total_trades} < {min_trades_wf})")
                if hasattr(trial_or_params, 'suggest_float') and hasattr(trial_or_params, "set_user_attr"):
                    trial_or_params.set_user_attr("error_type", "insufficient_trades")
                    trial_or_params.set_user_attr("trades_count", total_trades)
                    trial_or_params.set_user_attr("min_required", min_trades_wf)
                    raise optuna.TrialPruned(f"Insufficient trades: {total_trades} < {min_trades_wf}")

                return PENALTY_SOFT

            dd_penalty = 0
            if max_dd > MAX_DRAWDOWN_SOFT_THRESHOLD:
                dd_penalty = (max_dd - MAX_DRAWDOWN_SOFT_THRESHOLD) * DD_PENALTY_SOFT_MULTIPLIER

            if max_dd > MAX_DRAWDOWN_HARD_THRESHOLD:
                dd_penalty += (max_dd - MAX_DRAWDOWN_HARD_THRESHOLD) * DD_PENALTY_HARD_MULTIPLIER

            # –ù–µ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –∏—Ö –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ª–æ–≥–∏–∫–∏

            # BEST PRACTICE: –ê–Ω—Ç–∏-—á—É—Ä–Ω —à—Ç—Ä–∞—Ñ –∑–∞ —á–∞—Å—Ç—ã–µ —Å–¥–µ–ª–∫–∏
            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ search_space –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç—ã
            anti_churn_penalty_coeff = 0.02
            max_trades_per_day = 5

            if hasattr(self, 'search_space') and 'metrics' in self.search_space:
                metrics_config = self.search_space['metrics']
                anti_churn_penalty_coeff = metrics_config.get('anti_churn_penalty', 0.02)
                max_trades_per_day = metrics_config.get('max_trades_per_day', 5)

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—É—é –æ—Ü–µ–Ω–∫—É —Ç–æ—Ä–≥–æ–≤—ã—Ö –¥–Ω–µ–π
            calendar_days = self.base_config.walk_forward.testing_period_days
            trading_days = max(1, int(calendar_days * 0.7))  # –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (~70% –æ—Ç –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã—Ö –¥–Ω–µ–π)

            trades_per_day = total_trades / trading_days

            # –®—Ç—Ä–∞—Ñ –∑–∞ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ –ª–∏–º–∏—Ç–∞ —Å–¥–µ–ª–æ–∫ –≤ –¥–µ–Ω—å
            anti_churn_penalty = anti_churn_penalty_coeff * max(0, trades_per_day - max_trades_per_day)

            pairs_skipped = 0  # –í fast-—Ä–µ–∂–∏–º–µ –ø–∞—Ä—ã –ø—Ä–µ–¥–æ—Ç–æ–±—Ä–∞–Ω—ã, –ø–æ—ç—Ç–æ–º—É –ø—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–µ—Ç
            skipped_ratio = 0.0
            skipped_penalty = 0.0

            final_score = sharpe - dd_penalty + win_rate_bonus - win_rate_penalty - anti_churn_penalty - skipped_penalty

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ trial (–µ—Å–ª–∏ —ç—Ç–æ Optuna trial)
            if hasattr(trial_or_params, 'suggest_float') and hasattr(trial_or_params, "set_user_attr"):
                # –ü–æ–ª—É—á–∞–µ–º zscore –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                zscore_threshold = validated_params.get('zscore_threshold', 0)
                zscore_exit = validated_params.get('zscore_exit', 0)
                hysteresis = zscore_threshold - zscore_exit if zscore_threshold > zscore_exit else 0
                rolling_window = validated_params.get('rolling_window', 0)

                trial_or_params.set_user_attr("metrics", {
                    "sharpe": float(sharpe),
                    "max_drawdown": float(max_dd),
                    "win_rate": float(win_rate),
                    "total_trades": int(total_trades),
                    "trades_per_day": float(trades_per_day),
                    "zscore_threshold": float(zscore_threshold),
                    "zscore_exit": float(zscore_exit),
                    "hysteresis": float(hysteresis),
                    "rolling_window": int(rolling_window),
                    "dd_penalty": float(dd_penalty),
                    "win_rate_bonus": float(win_rate_bonus),  # –∏—Å–ø–æ–ª—å–∑—É–µ–º win_rate_bonus
                    "win_rate_penalty": float(win_rate_penalty),  # –∏—Å–ø–æ–ª—å–∑—É–µ–º win_rate_penalty
                    "anti_churn_penalty": float(anti_churn_penalty),
                    "skipped_penalty": float(skipped_penalty),  # —à—Ç—Ä–∞—Ñ –∑–∞ –ø—Ä–æ–ø—É—Å–∫–∏
                    "pairs_skipped": int(pairs_skipped),  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –ø–∞—Ä
                    "skipped_ratio": float(skipped_ratio),  # –¥–æ–ª—è –ø—Ä–æ–ø—É—Å–∫–æ–≤
                    "final_score": float(final_score)
                })

                # –õ–æ–≥–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–∏—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π trial_number)
                logger.info(f"Trial #{trial_number}: SUCCESS - "
                           f"Sharpe={sharpe:.4f}, Trades={total_trades}, DD={max_dd:.2%}, Score={final_score:.4f}")

            return final_score
            
        except optuna.TrialPruned:
            # –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º TrialPruned –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
            raise
        except Exception as e:

            error_type = type(e).__name__
            error_msg = str(e)

            data_related_errors = [
                "ValueError",  # –ü—Ä–æ–±–ª–µ–º—ã —Å –¥–∞–Ω–Ω—ã–º–∏, –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
                "KeyError",    # –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏/–∫–ª—é—á–∏
                "IndexError",  # –ü—Ä–æ–±–ª–µ–º—ã —Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π –¥–∞–Ω–Ω—ã—Ö
            ]

            # ZeroDivisionError –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–∞–∫ –ø—Ä–æ–±–ª–µ–º–æ–π –¥–∞–Ω–Ω—ã—Ö, —Ç–∞–∫ –∏ –ª–æ–≥–∏—á–µ—Å–∫–æ–π –æ—à–∏–±–∫–æ–π
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –ª—É—á—à–µ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            calculation_errors = [
                "ZeroDivisionError",  # –î–µ–ª–µ–Ω–∏–µ –Ω–∞ –Ω–æ–ª—å –≤ —Ä–∞—Å—á–µ—Ç–∞—Ö
                "FloatingPointError",  # –ü—Ä–æ–±–ª–µ–º—ã —Å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è–º–∏
            ]

            if error_type in data_related_errors or "data" in error_msg.lower() or "empty" in error_msg.lower():
                logger.warning(f"Trial #{trial_number}: –ü—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö ({error_type}): {error_msg}")
                if hasattr(trial_or_params, 'suggest_float') and hasattr(trial_or_params, "set_user_attr"):
                    trial_or_params.set_user_attr("error_type", "data_problem")
                    trial_or_params.set_user_attr("exception_type", error_type)
                    trial_or_params.set_user_attr("exception_message", error_msg)
                    raise optuna.TrialPruned(f"Data problem: {error_type} - {error_msg}")

                return PENALTY_SOFT
            elif error_type in calculation_errors:

                logger.warning(f"Trial #{trial_number}: –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞ ({error_type}): {error_msg}")
                if hasattr(trial_or_params, 'suggest_float') and hasattr(trial_or_params, "set_user_attr"):
                    trial_or_params.set_user_attr("error_type", "calculation_error")
                    trial_or_params.set_user_attr("exception_type", error_type)
                    trial_or_params.set_user_attr("exception_message", error_msg)
                    # –î–ª—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –æ—à–∏–±–æ–∫ –∏—Å–ø–æ–ª—å–∑—É–µ–º pruning (–æ–±—ã—á–Ω–æ –ø—Ä–æ–±–ª–µ–º–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
                    raise optuna.TrialPruned(f"Calculation error: {error_type} - {error_msg}")
                return PENALTY_SOFT
            else:
                # –°–∏—Å—Ç–µ–º–Ω—ã–µ –æ—à–∏–±–∫–∏ - –ª–æ–≥–∏—Ä—É–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º FAIL —á–µ—Ä–µ–∑ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
                logger.error(f"Trial #{trial_number}: –°–∏—Å—Ç–µ–º–Ω–∞—è –æ—à–∏–±–∫–∞ ({error_type}): {error_msg}")
                import traceback
                logger.error(traceback.format_exc())
                if hasattr(trial_or_params, 'suggest_float') and hasattr(trial_or_params, "set_user_attr"):
                    trial_or_params.set_user_attr("error_type", "system_error")
                    trial_or_params.set_user_attr("exception_type", error_type)
                    trial_or_params.set_user_attr("exception_message", error_msg)
                # –î–ª—è —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –æ—à–∏–±–æ–∫ –ø—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ, —á—Ç–æ–±—ã trial –ø–æ–ª—É—á–∏–ª TrialState.FAIL
                raise
