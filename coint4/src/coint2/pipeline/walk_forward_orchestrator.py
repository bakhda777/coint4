"""Walk-forward analysis orchestrator."""

from __future__ import annotations

from concurrent.futures import ThreadPoolExecutor
from collections import Counter
from pathlib import Path
from typing import Tuple, List
import time
import multiprocessing
import os

import pandas as pd

from coint2.core import math_utils, performance
from coint2.utils.config import AppConfig
from coint2.utils.logging_config import get_trading_logger, setup_logging_from_config
from coint2.utils.logging_utils import get_logger
from coint2.utils.timing_utils import ProgressTracker, logged_time, time_block
from coint2.utils.pairs_loader import load_pair_tuples
from coint2.utils.visualization import calculate_extended_metrics, create_performance_report, format_metrics_summary
from coint2.monitoring.metrics import TradingMetrics, DashboardGenerator

# Import directly from the file path rather than the module
from coint2.core.data_loader import DataHandler, load_master_dataset, resolve_data_filters
from coint2.core.normalization_improvements import (
    preprocess_and_normalize_data,
    apply_production_normalization,
    apply_normalization_with_params,
)
from coint2.core.memory_optimization import (
    consolidate_price_data, initialize_global_price_data, get_price_data_view,
    setup_blas_threading_limits, monitor_memory_usage, verify_no_data_copies,
    cleanup_global_data, GLOBAL_PRICE
)
# from optimiser.metric_utils import validate_params  # Moved to function level to avoid circular import
from coint2.engine.numba_engine import NumbaPairBacktester as PairBacktester
from coint2.core.portfolio import Portfolio
from coint2.utils.vectorized_ops import VectorizedStatsCalculator, vectorized_eval_expression

def convert_hours_to_periods(hours: float, bar_minutes: int) -> int:
    """
    Convert hours to number of periods based on bar timeframe.

    Args:
        hours: Number of hours to convert
        bar_minutes: Minutes per bar (timeframe)

    Returns:
        Number of periods
    """
    if hours <= 0:
        return 0
    return int(hours * 60 / bar_minutes)


def _parallel_map(
    func,
    items,
    n_jobs: int | None,
    use_processes: bool = False,
    initializer=None,
    initargs=(),
) -> list:
    if not items:
        return []
    if not n_jobs or n_jobs < 1:
        n_jobs = 1
    max_workers = min(n_jobs, len(items))
    if max_workers <= 1:
        return [func(item) for item in items]
    if use_processes:
        return _fork_process_map(
            func,
            items,
            max_workers,
            initializer=initializer,
            initargs=initargs,
        )
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        return list(executor.map(func, items))


def _process_chunk(conn, func, chunk, initializer=None, initargs=()):
    try:
        if initializer:
            initializer(*initargs)
        if func and conn:
            results = [func(item) for item in chunk]
            conn.send({"results": results})
            return
        conn.send({"error": "invalid process chunk"})
    except Exception as exc:
        conn.send({"error": str(exc)})
    finally:
        if conn:
            conn.close()


def _fork_process_map(func, items, n_jobs: int, initializer=None, initargs=()) -> list:
    if not items:
        return []
    max_workers = min(n_jobs, len(items))
    if max_workers <= 1:
        return [func(item) for item in items]
    try:
        ctx = multiprocessing.get_context("fork")
    except ValueError:
        ctx = multiprocessing.get_context()
    chunks = [items[i::max_workers] for i in range(max_workers)]
    processes = []
    conns = []
    for chunk in chunks:
        parent_conn, child_conn = ctx.Pipe(duplex=False)
        proc = ctx.Process(
            target=_process_chunk,
            args=(child_conn, func, chunk, initializer, initargs),
        )
        proc.start()
        child_conn.close()
        processes.append(proc)
        conns.append(parent_conn)
    results = []
    errors = []
    for conn in conns:
        payload = conn.recv()
        conn.close()
        if isinstance(payload, dict) and payload.get("error"):
            errors.append(payload["error"])
        else:
            results.extend(payload.get("results", []))
    for proc in processes:
        proc.join()
    if errors:
        raise RuntimeError(f"Process worker error: {errors[0]}")
    return results


def _process_pair_mmap_worker(args):
    return process_single_pair_mmap(*args)


def _process_pair_worker(args):
    return process_single_pair(*args)


def _init_worker_global_price(consolidated_path: str) -> None:
    if not consolidated_path:
        return
    from coint2.core.memory_optimization import GLOBAL_PRICE, initialize_global_price_data
    if GLOBAL_PRICE is None:
        initialize_global_price_data(consolidated_path)

def _simulate_realistic_portfolio(all_pnls, cfg, all_positions=None, all_scores=None):
    """
    –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è —Å–∏–º—É–ª—è—Ü–∏—è –ø–æ—Ä—Ç—Ñ–µ–ª—è —Å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –ø–æ–∑–∏—Ü–∏—è–º–∏.

    –í–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ–≥–æ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏—è PnL –≤—Å–µ—Ö –ø–∞—Ä, —Å–∏–º—É–ª–∏—Ä—É–µ–º —Ä–µ–∞–ª—å–Ω—É—é —Ä–∞–±–æ—Ç—É –ø–æ—Ä—Ç—Ñ–µ–ª—è:
    1. –ù–∞ –∫–∞–∂–¥–æ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–º —à–∞–≥–µ —Å–æ–±–∏—Ä–∞–µ–º —Å–∏–≥–Ω–∞–ª—ã –æ—Ç –≤—Å–µ—Ö –ø–∞—Ä
    2. –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–∏–º–∏—Ç max_active_positions
    3. –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–µ —Å–∏–≥–Ω–∞–ª—ã –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏
    4. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π PnL –ø–æ—Ä—Ç—Ñ–µ–ª—è

    Args:
        all_pnls: –°–ø–∏—Å–æ–∫ PnL —Å–µ—Ä–∏–π –æ—Ç –≤—Å–µ—Ö –ø–∞—Ä
        cfg: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ—Ä—Ç—Ñ–µ–ª—è
        all_positions: –°–ø–∏—Å–æ–∫ —Å–µ—Ä–∏–π –ø–æ–∑–∏—Ü–∏–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        all_scores: –°–ø–∏—Å–æ–∫ —Å–µ—Ä–∏–π —Å–∏–≥–Ω–∞–ª–æ–≤/—Å–∫–æ—Ä–∏–Ω–≥–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–∞–ø—Ä–∏–º–µ—Ä z-score)

    Returns:
        pd.Series: –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π PnL –ø–æ—Ä—Ç—Ñ–µ–ª—è —Å —É—á–µ—Ç–æ–º –ª–∏–º–∏—Ç–æ–≤ –ø–æ–∑–∏—Ü–∏–π
    """
    logger = get_logger(__name__)

    if not all_pnls:
        return pd.Series(dtype=float)

    # –°–æ–∑–¥–∞–µ–º DataFrame —Å–æ –≤—Å–µ–º–∏ PnL —Å–µ—Ä–∏—è–º–∏
    pnl_df = pd.concat({f'pair_{i}': pnl.fillna(0) for i, pnl in enumerate(all_pnls)}, axis=1)

    positions_df = None
    if all_positions:
        positions_df = pd.concat({f'pair_{i}': pos.fillna(0) for i, pos in enumerate(all_positions)}, axis=1)

    scores_df = None
    if all_scores:
        scores_df = pd.concat({f'pair_{i}': score for i, score in enumerate(all_scores)}, axis=1)

    # –°–æ–∑–¥–∞–µ–º DataFrame —Å —Å–∏–≥–Ω–∞–ª–∞–º–∏
    if positions_df is not None:
        signals_df = (positions_df != 0).astype(int)
        entry_signals_df = (positions_df.shift(1).fillna(0) == 0) & (positions_df != 0)
    else:
        signals_df = pd.concat({f'pair_{i}': (pnl != 0).astype(int) for i, pnl in enumerate(all_pnls)}, axis=1)
        entry_signals_df = signals_df

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Ä—Ç—Ñ–µ–ª—å
    max_positions = cfg.portfolio.max_active_positions
    portfolio_pnl = pd.Series(0.0, index=pnl_df.index)
    active_positions = {}  # {pair_name: entry_timestamp}

    logger.info(f"üéØ –°–ò–ú–£–õ–Ø–¶–ò–Ø –ü–û–†–¢–§–ï–õ–Ø: {len(all_pnls)} –ø–∞—Ä, –ª–∏–º–∏—Ç {max_positions} –ø–æ–∑–∏—Ü–∏–π")

    # –°–∏–º—É–ª–∏—Ä—É–µ–º —Ç–æ—Ä–≥–æ–≤–ª—é –ø–æ –∫–∞–∂–¥–æ–º—É –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É —à–∞–≥—É
    for timestamp in pnl_df.index:
        current_signals = signals_df.loc[timestamp]
        current_entries = entry_signals_df.loc[timestamp]
        current_pnls = pnl_df.loc[timestamp]

        # 1. –û—Ç–º–µ—á–∞–µ–º –ø–æ–∑–∏—Ü–∏–∏ –Ω–∞ –∑–∞–∫—Ä—ã—Ç–∏–µ (–ø–æ –ø–æ–∑–∏—Ü–∏–∏, –∞ –Ω–µ –ø–æ PnL)
        positions_to_close = []
        if positions_df is not None:
            current_positions = positions_df.loc[timestamp]
            for pair_name in list(active_positions.keys()):
                if current_positions[pair_name] == 0:
                    positions_to_close.append(pair_name)
        else:
            for pair_name in list(active_positions.keys()):
                if current_signals[pair_name] == 0:
                    positions_to_close.append(pair_name)

        # 2. –ò—â–µ–º –Ω–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã –¥–ª—è –æ—Ç–∫—Ä—ã—Ç–∏—è –ø–æ–∑–∏—Ü–∏–π (—Ç–æ–ª—å–∫–æ –Ω–∞ –≤—Ö–æ–¥–µ)
        new_signals = []
        for pair_name in current_signals.index:
            if current_entries[pair_name] and pair_name not in active_positions:
                if scores_df is not None and pair_name in scores_df.columns:
                    strength = abs(scores_df.loc[timestamp, pair_name])
                else:
                    strength = abs(current_pnls[pair_name])
                new_signals.append((pair_name, strength))

        # 3. –°–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ —Å–∏–≥–Ω–∞–ª—ã –ø–æ —Å–∏–ª–µ
        new_signals.sort(key=lambda x: x[1], reverse=True)

        # 4. –û—Ç–∫—Ä—ã–≤–∞–µ–º –Ω–æ–≤—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ª–∏–º–∏—Ç–∞ (—É—á–∏—Ç—ã–≤–∞–µ–º –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è)
        available_slots = max_positions - (len(active_positions) - len(positions_to_close))
        if available_slots < 0:
            available_slots = 0
        for i, (pair_name, _strength) in enumerate(new_signals):
            if i >= available_slots:
                break
            active_positions[pair_name] = timestamp

        # 5. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º PnL –ø–æ—Ä—Ç—Ñ–µ–ª—è –Ω–∞ —ç—Ç–æ–º —à–∞–≥–µ
        step_pnl = 0.0
        for pair_name in active_positions:
            step_pnl += current_pnls[pair_name]

        portfolio_pnl[timestamp] = step_pnl

        # 6. –ó–∞–∫—Ä—ã–≤–∞–µ–º –ø–æ–∑–∏—Ü–∏–∏ –ø–æ—Å–ª–µ —É—á–µ—Ç–∞ PnL –Ω–∞ —Ç–µ–∫—É—â–µ–º –±–∞—Ä–µ
        for pair_name in positions_to_close:
            if pair_name in active_positions:
                del active_positions[pair_name]

    # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
    total_signals = signals_df.sum(axis=1)
    avg_active_pairs = len([p for p in active_positions]) if active_positions else 0
    max_signals = total_signals.max()
    avg_signals = total_signals.mean()

    logger.info(f"   üìà –ú–∞–∫—Å. –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤: {max_signals}")
    logger.info(f"   üìä –°—Ä–µ–¥–Ω. —Å–∏–≥–Ω–∞–ª–æ–≤ –∑–∞ –ø–µ—Ä–∏–æ–¥: {avg_signals:.1f}")
    logger.info(f"   üéØ –§–∏–Ω–∞–ª—å–Ω—ã—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π: {avg_active_pairs}")

    utilization = (total_signals.clip(upper=max_positions) / max_positions).mean()
    logger.info(f"   ‚ö° –£—Ç–∏–ª–∏–∑–∞—Ü–∏—è –ª–∏–º–∏—Ç–∞ –ø–æ–∑–∏—Ü–∏–π: {utilization:.1%}")

    return portfolio_pnl


def _emit_monitoring_metrics(metrics: dict) -> None:
    logger = get_logger("walk_forward.monitoring")
    try:
        trading_logger = get_trading_logger()
        drawdown_pct = abs(float(metrics.get("max_drawdown_on_equity", 0.0) or 0.0))

        monitoring = TradingMetrics()
        monitoring.total_pnl = float(metrics.get("total_pnl", 0.0) or 0.0)
        monitoring.win_rate = float(metrics.get("win_rate_trades", metrics.get("win_rate", 0.0)) or 0.0)
        monitoring.max_drawdown = drawdown_pct
        monitoring.current_drawdown = drawdown_pct
        monitoring.trade_count = int(metrics.get("total_trades", 0) or 0)
        monitoring.active_positions = int(metrics.get("total_pairs_traded", 0) or 0)
        monitoring.sharpe_ratio = float(metrics.get("sharpe_ratio_abs", 0.0) or 0.0)
        monitoring.avg_trade_duration = float(metrics.get("avg_trade_duration", 0.0) or 0.0)
        monitoring.last_update_time = pd.Timestamp.now().isoformat()

        for key, value in {
            "total_pnl": monitoring.total_pnl,
            "sharpe_ratio_abs": monitoring.sharpe_ratio,
            "max_drawdown_pct": monitoring.max_drawdown,
            "total_trades": monitoring.trade_count,
            "total_pairs_traded": monitoring.active_positions,
            "total_costs": float(metrics.get("total_costs", 0.0) or 0.0),
        }.items():
            trading_logger.log_metric(key, float(value))

        DashboardGenerator().generate_dashboard(monitoring)
    except Exception as exc:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {exc}")

def _run_backtest_for_pair(pair_data, s1, s2, cfg, capital_per_pair, bar_minutes, period_label, metrics):
    """
    Helper function to run backtest for a pair with given data.
    
    Args:
        pair_data: Normalized pair data DataFrame
        s1, s2: Symbol names
        cfg: Configuration object
        capital_per_pair: Capital allocated per pair
        bar_minutes: Minutes per bar
        period_label: Label for the current period
        metrics: Pair metrics dictionary
        
    Returns:
        Dictionary with pair results
    """
    from coint2.utils.logging_utils import get_logger
    logger = get_logger("pair_processing")
    
    try:
        # Create a temporary portfolio for this pair
        from coint2.core.portfolio import Portfolio
        temp_portfolio = Portfolio(
            initial_capital=capital_per_pair,
            max_active_positions=1,  # Single pair
            config=cfg.portfolio,
        )
        
        bt = PairBacktester(
            pair_data=pair_data,
            rolling_window=cfg.backtest.rolling_window,
            portfolio=temp_portfolio,
            pair_name=f"{s1}-{s2}",
            z_threshold=cfg.backtest.zscore_threshold,
            z_exit=getattr(cfg.backtest, 'zscore_exit', 0.0),
            commission_pct=getattr(cfg.backtest, 'commission_pct', 0.0),
            slippage_pct=getattr(cfg.backtest, 'slippage_pct', 0.0),
            annualizing_factor=getattr(cfg.backtest, 'annualizing_factor', 365),
            capital_at_risk=capital_per_pair,
            stop_loss_multiplier=getattr(cfg.backtest, 'stop_loss_multiplier', 2.0),
            take_profit_multiplier=getattr(cfg.backtest, 'take_profit_multiplier', None),
            cooldown_periods=convert_hours_to_periods(getattr(cfg.backtest, 'cooldown_hours', 0), bar_minutes),
            wait_for_candle_close=getattr(cfg.backtest, 'wait_for_candle_close', False),
            max_margin_usage=getattr(cfg.portfolio, 'max_margin_usage', 1.0),
            half_life=metrics.get('half_life'),
            time_stop_multiplier=getattr(cfg.backtest, 'time_stop_multiplier', None),
            min_volatility=getattr(cfg.backtest, 'min_volatility', 0.001),
            # Enhanced risk management parameters
            use_kelly_sizing=getattr(cfg.backtest, 'use_kelly_sizing', True),
            max_kelly_fraction=getattr(cfg.backtest, 'max_kelly_fraction', 0.25),
            volatility_lookback=getattr(cfg.backtest, 'volatility_lookback', 96),
            adaptive_thresholds=getattr(cfg.backtest, 'adaptive_thresholds', True),
            var_confidence=getattr(cfg.backtest, 'var_confidence', 0.05),
            max_var_multiplier=getattr(cfg.backtest, 'max_var_multiplier', 3.0),
            # Market regime detection parameters
            market_regime_detection=getattr(cfg.backtest, 'market_regime_detection', True),
            hurst_window=getattr(cfg.backtest, 'hurst_window', 720),
            hurst_trending_threshold=getattr(cfg.backtest, 'hurst_trending_threshold', 0.5),
            variance_ratio_window=getattr(cfg.backtest, 'variance_ratio_window', 480),
            variance_ratio_trending_min=getattr(cfg.backtest, 'variance_ratio_trending_min', 1.2),
            variance_ratio_mean_reverting_max=getattr(cfg.backtest, 'variance_ratio_mean_reverting_max', 0.8),
            # Structural break protection parameters
            structural_break_protection=getattr(cfg.backtest, 'structural_break_protection', True),
            cointegration_test_frequency=getattr(cfg.backtest, 'cointegration_test_frequency', 2688),
            adf_pvalue_threshold=getattr(cfg.backtest, 'adf_pvalue_threshold', 0.05),
            exclusion_period_days=getattr(cfg.backtest, 'exclusion_period_days', 30),
            max_half_life_days=getattr(cfg.backtest, 'max_half_life_days', 10),
            min_correlation_threshold=getattr(cfg.backtest, 'min_correlation_threshold', 0.6),
            correlation_window=getattr(cfg.backtest, 'correlation_window', 720),
            # Performance optimization parameters
            regime_check_frequency=getattr(cfg.backtest, 'regime_check_frequency', 96),
            use_market_regime_cache=getattr(cfg.backtest, 'use_market_regime_cache', True),
            adf_check_frequency=getattr(cfg.backtest, 'adf_check_frequency', 5376),
            cache_cleanup_frequency=getattr(cfg.backtest, 'cache_cleanup_frequency', 1000),
            lazy_adf_threshold=getattr(cfg.backtest, 'lazy_adf_threshold', 0.1),
            hurst_neutral_band=getattr(cfg.backtest, 'hurst_neutral_band', 0.05),
            vr_neutral_band=getattr(cfg.backtest, 'vr_neutral_band', 0.2),
            # EW correlation parameters
            use_exponential_weighted_correlation=getattr(cfg.backtest, 'use_exponential_weighted_correlation', False),
            ew_correlation_alpha=getattr(cfg.backtest, 'ew_correlation_alpha', 0.1),
            # Cost modeling parameters
            slippage_stress_multiplier=getattr(cfg.backtest, 'slippage_stress_multiplier', 1.0),
            always_model_slippage=getattr(cfg.backtest, 'always_model_slippage', True),
            # Volatility-based position sizing parameters
            volatility_based_sizing=getattr(cfg.portfolio, 'volatility_based_sizing', False),
            volatility_lookback_hours=getattr(cfg.portfolio, 'volatility_lookback_hours', 24),
            min_position_size_pct=getattr(cfg.portfolio, 'min_position_size_pct', 0.005),
            max_position_size_pct=getattr(cfg.portfolio, 'max_position_size_pct', 0.02),
            volatility_adjustment_factor=getattr(cfg.portfolio, 'volatility_adjustment_factor', 2.0),
            config=cfg.backtest,
        )
        
        # Run backtest
        logger.debug(f"üîÑ –ó–∞–ø—É—Å–∫–∞–µ–º –±—ç–∫—Ç–µ—Å—Ç –¥–ª—è –ø–∞—Ä—ã {s1}-{s2}")
        bt.run()
        results = bt.get_results()
        
        # Process results
        pnl_series = results["pnl"]
        trades_log = results.get('trades_log', [])
        logger.debug(f"üìà –ë—ç–∫—Ç–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è {s1}-{s2}: {len(pnl_series)} –ø–µ—Ä–∏–æ–¥–æ–≤, {len(trades_log)} —Å–¥–µ–ª–æ–∫")
        
        # Calculate trade statistics
        if isinstance(results, dict):
            trades = results.get("trades", pd.Series())
            positions = results.get("position", pd.Series(dtype=float))
            costs = results.get("costs", pd.Series())
            z_scores = results.get("z_score", pd.Series(dtype=float))
        else:
            trades = results.get("trades", pd.Series())
            positions = results.get("position", pd.Series(dtype=float))
            costs = results.get("costs", pd.Series())
            z_scores = results.get("z_score", pd.Series(dtype=float))
        
        # Vectorized calculation of actual trade count
        if not positions.empty:
            import numpy as np
            positions_values = positions.ffill().values
            prev_positions_values = np.concatenate([[0], positions_values[:-1]])
            # Vectorized boolean operations
            is_trade_open_event = (prev_positions_values == 0) & (positions_values != 0)
            actual_trade_count = int(np.sum(is_trade_open_event))
        else:
            actual_trade_count = 0

        # Notional cap diagnostics (entry-level)
        entry_notional_count = 0
        entry_notional_cap_hits = 0
        entry_notional_below_min = 0
        entry_notional_avg = 0.0
        entry_notional_p50 = 0.0
        entry_notional_min = 0.0
        entry_notional_max = 0.0

        min_notional = float(getattr(cfg.portfolio, "min_notional_per_trade", 0.0) or 0.0)
        max_notional = float(getattr(cfg.portfolio, "max_notional_per_trade", 0.0) or 0.0)
        if actual_trade_count > 0 and (min_notional > 0.0 or max_notional > 0.0):
            try:
                import numpy as np

                entry_mask = is_trade_open_event
                if entry_mask.any():
                    if isinstance(results, dict):
                        y_vals = results.get("y")
                        x_vals = results.get("x")
                        beta_vals = results.get("beta")
                    else:
                        y_vals = results.get("y")
                        x_vals = results.get("x")
                        beta_vals = results.get("beta")

                    if y_vals is None or x_vals is None:
                        y_vals = pair_data.iloc[:, 0].values
                        x_vals = pair_data.iloc[:, 1].values
                    else:
                        y_vals = y_vals.values if hasattr(y_vals, "values") else np.asarray(y_vals)
                        x_vals = x_vals.values if hasattr(x_vals, "values") else np.asarray(x_vals)
                        if np.nanmax(np.abs(y_vals)) == 0.0 or np.nanmax(np.abs(x_vals)) == 0.0:
                            y_vals = pair_data.iloc[:, 0].values
                            x_vals = pair_data.iloc[:, 1].values

                    if beta_vals is None:
                        beta_vals = np.ones_like(y_vals, dtype=float)
                    else:
                        beta_vals = beta_vals.values if hasattr(beta_vals, "values") else np.asarray(beta_vals)
                        if beta_vals.shape[0] != y_vals.shape[0]:
                            beta_vals = np.ones_like(y_vals, dtype=float)

                    entry_idx = np.where(entry_mask)[0]
                    entry_positions = positions_values[entry_mask]
                    entry_notional = np.abs(entry_positions) * (
                        np.abs(y_vals[entry_idx]) + np.abs(beta_vals[entry_idx] * x_vals[entry_idx])
                    )
                    entry_notional = np.nan_to_num(entry_notional, nan=0.0, posinf=0.0, neginf=0.0)

                    entry_notional_count = int(entry_notional.size)
                    if entry_notional_count > 0:
                        entry_notional_avg = float(entry_notional.mean())
                        entry_notional_p50 = float(np.median(entry_notional))
                        entry_notional_min = float(entry_notional.min())
                        entry_notional_max = float(entry_notional.max())
                        if max_notional > 0.0:
                            entry_notional_cap_hits = int(
                                np.sum(entry_notional >= max_notional * 0.999)
                            )
                        if min_notional > 0.0:
                            entry_notional_below_min = int(
                                np.sum(entry_notional <= min_notional * 1.001)
                            )
            except Exception as exc:
                logger.debug(f"Notional diagnostics failed for {s1}-{s2}: {exc}")

        # Vectorized calculation of pair statistics using numpy
        if not pnl_series.empty:
            import numpy as np
            pnl_values = pnl_series.values
            pair_pnl = np.sum(pnl_values)
            win_days = int(np.sum(pnl_values > 0))
            lose_days = int(np.sum(pnl_values < 0))
            max_daily_gain = np.max(pnl_values)
            max_daily_loss = np.min(pnl_values)
        else:
            pair_pnl = 0.0
            win_days = lose_days = 0
            max_daily_gain = max_daily_loss = 0.0
        
        # CRITICAL FIX: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç –∏–∑–¥–µ—Ä–∂–µ–∫ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π NaN
        if not costs.empty:
            costs_clean = costs.fillna(0)  # –ó–∞–º–µ–Ω—è–µ–º NaN –Ω–∞ 0
            pair_costs = float(np.sum(costs_clean.values))
        else:
            pair_costs = 0.0
        
        trade_stat = {
            'pair': f'{s1}-{s2}',
            'period': period_label,
            'total_pnl': pair_pnl,
            'total_costs': pair_costs,
            'trade_count': actual_trade_count,
            'avg_pnl_per_trade': pair_pnl / max(actual_trade_count, 1),
            'win_days': win_days,
            'lose_days': lose_days,
            'total_days': len(pnl_series),
            'max_daily_gain': max_daily_gain,
            'max_daily_loss': max_daily_loss,
            'entry_notional_count': entry_notional_count,
            'entry_notional_cap_hits': entry_notional_cap_hits,
            'entry_notional_below_min': entry_notional_below_min,
            'entry_notional_avg': entry_notional_avg,
            'entry_notional_p50': entry_notional_p50,
            'entry_notional_min': entry_notional_min,
            'entry_notional_max': entry_notional_max,
        }
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        logger.debug(f"‚úÖ –ü–∞—Ä–∞ {s1}-{s2} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ: PnL={trade_stat['total_pnl']:.2f}, —Å–¥–µ–ª–æ–∫={trade_stat['trade_count']}")
        
        return {
            'pnl_series': pnl_series,
            'trades_log': trades_log,
            'trade_stat': trade_stat,
            'positions': positions,
            'z_scores': z_scores,
            'success': True
        }
        
    except Exception as e:
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        logger.warning(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–∞—Ä—ã {s1}-{s2}: {str(e)}")
        
        # Return empty results for failed pairs
        return {
            'pnl_series': pd.Series(dtype=float),
            'trades_log': [],
            'trade_stat': {
                'pair': f'{s1}-{s2}',
                'period': period_label,
                'total_pnl': 0.0,
                'total_costs': 0.0,
                'trade_count': 0,
                'avg_pnl_per_trade': 0.0,
                'win_days': 0,
                'lose_days': 0,
                'total_days': 0,
                'max_daily_gain': 0.0,
                'max_daily_loss': 0.0,
                'error': str(e)
            },
            'positions': pd.Series(dtype=float),
            'z_scores': pd.Series(dtype=float),
            'success': False,
            'error': str(e)
        }

def process_single_pair_mmap(pair_symbols, testing_start, testing_end, cfg, 
                            capital_per_pair, bar_minutes, period_label, pair_stats=None, training_normalization_params=None):
    """
    Process a single pair using memory-mapped data for optimal performance.
    
    Args:
        pair_symbols: Tuple of (s1, s2) symbol names
        testing_start, testing_end: Testing period boundaries
        cfg: Configuration object
        capital_per_pair: Capital allocated per pair
        bar_minutes: Minutes per bar
        period_label: Label for the current period
        pair_stats: Optional pre-computed pair statistics (beta, mean, std, metrics)
        
    Returns:
        Dictionary with pair results
    """
    # Setup BLAS threading limits to prevent oversubscription
    from coint2.core.memory_optimization import setup_blas_threading_limits
    setup_blas_threading_limits(num_threads=1, verbose=False)
    
    s1, s2 = pair_symbols
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–∞—Ä—ã
    from coint2.utils.logging_utils import get_logger
    logger = get_logger("pair_processing")
    logger.debug(f"üìä –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –ø–∞—Ä—ã {s1}-{s2} (memory-mapped)")
    
    # Extract pair statistics if provided
    if pair_stats is not None:
        beta, mean, std, metrics = pair_stats
    else:
        # Fallback values if no statistics provided
        beta, mean, std = 1.0, 0.0, 1.0
        metrics = {}
    
    try:
        # Get memory-mapped data view for this pair (no copy)
        pair_data_view = get_price_data_view([s1, s2], testing_start, testing_end)
        
        # Check if both symbols are available
        if s1 not in pair_data_view.columns or s2 not in pair_data_view.columns:
            return {
                'success': False,
                'error': f'Missing data for symbols {s1} or {s2}',
                'trade_stat': {
                    'pair': f'{s1}-{s2}',
                    'period': period_label,
                    'total_pnl': 0.0,
                    'trade_count': 0,
                    'win_rate': 0.0,
                    'avg_win': 0.0,
                    'avg_loss': 0.0,
                    'profit_factor': 0.0,
                    'max_drawdown': 0.0,
                    'sharpe_ratio': 0.0
                },
                'pnl_series': pd.Series(dtype=float),
                'trades_log': []
            }
            
        # Work with data view (no copy) and drop NaN
        pair_data = pair_data_view[[s1, s2]].dropna()
        
        if pair_data.empty:
            return {
                'success': False,
                'error': 'No valid data after filtering',
                'trade_stat': {
                    'pair': f'{s1}-{s2}',
                    'period': period_label,
                    'total_pnl': 0.0,
                    'trade_count': 0,
                    'win_rate': 0.0,
                    'avg_win': 0.0,
                    'avg_loss': 0.0,
                    'profit_factor': 0.0,
                    'max_drawdown': 0.0,
                    'sharpe_ratio': 0.0
                },
                'pnl_series': pd.Series(dtype=float),
                'trades_log': []
            }
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞ –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        import numpy as np
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å–ª–∏ training_normalization_params —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (–Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç)
        if training_normalization_params and isinstance(training_normalization_params, dict):
            method = training_normalization_params.get('method')
            if method in ("rolling_zscore", "percent", "log_returns"):
                # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞–º–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
                try:
                    pair_data = apply_production_normalization(
                        pair_data,
                        training_normalization_params
                    )
                    logger.debug(f"–ü—Ä–∏–º–µ–Ω–µ–Ω–∞ {method} –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è {s1}-{s2}")
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
                    return {
                        'success': False,
                        'error': f'Normalization error: {e}',
                        'trade_stat': {'pair': f'{s1}-{s2}', 'period': period_label, 'total_pnl': 0.0, 'trade_count': 0},
                        'pnl_series': pd.Series(dtype=float),
                        'trades_log': []
                    }
            elif method in ("minmax", "zscore"):
                try:
                    params = training_normalization_params.get('params', {})
                    fill_method = training_normalization_params.get('fill_method', 'ffill')
                    pair_data = apply_normalization_with_params(
                        pair_data,
                        params,
                        norm_method=method,
                        fill_method=fill_method
                    )
                    if s1 not in pair_data.columns or s2 not in pair_data.columns:
                        raise ValueError("Missing symbols after normalization")
                    logger.debug(f"–ü—Ä–∏–º–µ–Ω–µ–Ω–∞ {method} –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è {s1}-{s2}")
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
                    return {
                        'success': False,
                        'error': f'Normalization error: {e}',
                        'trade_stat': {'pair': f'{s1}-{s2}', 'period': period_label, 'total_pnl': 0.0, 'trade_count': 0},
                        'pnl_series': pd.Series(dtype=float),
                        'trades_log': []
                    }
            elif s1 in training_normalization_params and s2 in training_normalization_params:
                # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç —Å –ø–µ—Ä–≤—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)
                data_values = pair_data.to_numpy(copy=False)
                norm_s1 = training_normalization_params[s1]
                norm_s2 = training_normalization_params[s2]
                
                if norm_s1 != 0 and norm_s2 != 0:
                    first_row = np.array([norm_s1, norm_s2])
                    logger.debug(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –¥–ª—è {s1}-{s2}: {first_row}")
                    normalized_values = np.divide(data_values, first_row[np.newaxis, :]) * 100
                    pair_data = pd.DataFrame(normalized_values, index=pair_data.index, columns=pair_data.columns)
                else:
                    logger.warning(f"–ù—É–ª–µ–≤–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è {s1} –∏–ª–∏ {s2}. –ü—Ä–æ–ø—É—Å–∫.")
                    return {
                        'success': False,
                        'error': f'Zero normalization parameter for {s1} or {s2}',
                        'trade_stat': {'pair': f'{s1}-{s2}', 'period': period_label, 'total_pnl': 0.0, 'trade_count': 0},
                        'pnl_series': pd.Series(dtype=float),
                        'trades_log': []
                    }
            else:
                # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—É, –µ—Å–ª–∏ –Ω–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                logger.error(f"–ù–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è {s1}-{s2}. –ü—Ä–æ–ø—É—Å–∫.")
                return {
                    'success': False,
                    'error': f'Missing normalization parameters for {s1} or {s2}',
                    'trade_stat': {'pair': f'{s1}-{s2}', 'period': period_label, 'total_pnl': 0.0, 'trade_count': 0},
                    'pnl_series': pd.Series(dtype=float),
                    'trades_log': []
                }
        else:
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—É, –µ—Å–ª–∏ –Ω–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            logger.error(f"–ù–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è {s1}-{s2}. –ü—Ä–æ–ø—É—Å–∫.")
            return {
                'success': False,
                'error': f'Missing normalization parameters for {s1} or {s2}',
                'trade_stat': {'pair': f'{s1}-{s2}', 'period': period_label, 'total_pnl': 0.0, 'trade_count': 0},
                'pnl_series': pd.Series(dtype=float),
                'trades_log': []
            }
        
        # Use helper function to run backtest
        return _run_backtest_for_pair(
            pair_data=pair_data,
            s1=s1,
            s2=s2,
            cfg=cfg,
            capital_per_pair=capital_per_pair,
            bar_minutes=bar_minutes,
            metrics=metrics,
            period_label=period_label
        )
        
    except Exception as e:
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        logger.warning(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–∞—Ä—ã {s1}-{s2}: {str(e)}")
        
        # Return empty results for failed pairs
        return {
            'pnl_series': pd.Series(dtype=float),
            'trades_log': [],
            'trade_stat': {
                'pair': f'{s1}-{s2}',
                'period': period_label,
                'total_pnl': 0.0,
                'total_costs': 0.0,
                'trade_count': 0,
                'avg_pnl_per_trade': 0.0,
                'win_days': 0,
                'lose_days': 0,
                'total_days': 0,
                'max_daily_gain': 0.0,
                'max_daily_loss': 0.0,
                'error': str(e)
            },
            'success': False,
            'error': str(e)
        }

def process_single_pair(pair_data_tuple, step_df, testing_start, testing_end, cfg, 
                       capital_per_pair, bar_minutes, period_label, training_normalization_params=None):
    """
    Process a single pair for parallel execution with vectorized optimizations.
    
    Args:
        pair_data_tuple: Tuple of (s1, s2, beta, mean, std, metrics)
        step_df: DataFrame with price data
        testing_start, testing_end: Testing period boundaries
        cfg: Configuration object
        capital_per_pair: Capital allocated per pair
        bar_minutes: Minutes per bar
        period_label: Label for the current period
        training_normalization_params: Dict with normalization parameters from training data
        
    Returns:
        Dictionary with pair results
    """
    # Setup BLAS threading limits to prevent oversubscription
    from coint2.core.memory_optimization import setup_blas_threading_limits
    setup_blas_threading_limits(num_threads=1, verbose=False)
    
    from coint2.utils.logger import get_logger
    logger = get_logger(__name__)
    
    s1, s2, beta, mean, std, metrics = pair_data_tuple
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–∞—Ä—ã
    logger.debug(f"üîÑ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –ø–∞—Ä—ã {s1}-{s2} (–ø–µ—Ä–∏–æ–¥: {period_label})")
    
    try:
        pair_data = step_df.loc[testing_start:testing_end, [s1, s2]].dropna()
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï LOOKAHEAD BIAS: –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞
        if not pair_data.empty:
            import numpy as np
            data_values = pair_data.values

            if training_normalization_params and isinstance(training_normalization_params, dict):
                method = training_normalization_params.get('method')
                if method in ("rolling_zscore", "percent", "log_returns"):
                    try:
                        pair_data = apply_production_normalization(
                            pair_data,
                            training_normalization_params
                        )
                        logger.debug(f"–ü—Ä–∏–º–µ–Ω–µ–Ω–∞ {method} –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è {s1}-{s2}")
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
                        return {
                            'success': False,
                            'error': f'Normalization error: {e}',
                            'trade_stat': {'pair': f'{s1}-{s2}', 'period': period_label, 'total_pnl': 0.0, 'trade_count': 0},
                            'pnl_series': pd.Series(dtype=float),
                            'trades_log': []
                        }
                elif method in ("minmax", "zscore"):
                    try:
                        params = training_normalization_params.get('params', {})
                        fill_method = training_normalization_params.get('fill_method', 'ffill')
                        pair_data = apply_normalization_with_params(
                            pair_data,
                            params,
                            norm_method=method,
                            fill_method=fill_method
                        )
                        if s1 not in pair_data.columns or s2 not in pair_data.columns:
                            raise ValueError("Missing symbols after normalization")
                        logger.debug(f"–ü—Ä–∏–º–µ–Ω–µ–Ω–∞ {method} –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è {s1}-{s2}")
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
                        return {
                            'success': False,
                            'error': f'Normalization error: {e}',
                            'trade_stat': {'pair': f'{s1}-{s2}', 'period': period_label, 'total_pnl': 0.0, 'trade_count': 0},
                            'pnl_series': pd.Series(dtype=float),
                            'trades_log': []
                        }
                elif s1 in training_normalization_params and s2 in training_normalization_params:
                    norm_s1 = training_normalization_params[s1]
                    norm_s2 = training_normalization_params[s2]

                    if norm_s1 != 0 and norm_s2 != 0:
                        # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –∫ —Å—Ç–æ–ª–±—Ü–∞–º DataFrame
                        first_row = np.array([norm_s1, norm_s2])
                        logger.debug(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –∏–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞ –¥–ª—è {s1}-{s2}: {first_row}")

                        # Vectorized division and multiplication
                        normalized_values = np.divide(data_values, first_row[np.newaxis, :]) * 100
                        pair_data = pd.DataFrame(normalized_values, index=pair_data.index, columns=pair_data.columns)
                    else:
                        # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—É, —Ç–∞–∫ –∫–∞–∫ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞
                        logger.warning(f"–ù—É–ª–µ–≤–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è {s1} –∏–ª–∏ {s2}. –ü—Ä–æ–ø—É—Å–∫.")
                        return {
                            'success': False,
                            'error': f'Zero normalization parameter for {s1} or {s2}',
                            'trade_stat': {'pair': f'{s1}-{s2}', 'period': period_label, 'total_pnl': 0.0, 'trade_count': 0},
                            'pnl_series': pd.Series(dtype=float),
                            'trades_log': []
                        }
                else:
                    # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—É, –µ—Å–ª–∏ –Ω–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                    logger.error(f"–ù–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è {s1}-{s2}. –ü—Ä–æ–ø—É—Å–∫.")
                    return {
                        'success': False,
                        'error': f'Missing normalization parameters for {s1} or {s2}',
                        'trade_stat': {'pair': f'{s1}-{s2}', 'period': period_label, 'total_pnl': 0.0, 'trade_count': 0},
                        'pnl_series': pd.Series(dtype=float),
                        'trades_log': []
                    }
            else:
                # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—É, –µ—Å–ª–∏ –Ω–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                logger.error(f"–ù–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è {s1}-{s2}. –ü—Ä–æ–ø—É—Å–∫.")
                return {
                    'success': False,
                    'error': f'Missing normalization parameters for {s1} or {s2}',
                    'trade_stat': {'pair': f'{s1}-{s2}', 'period': period_label, 'total_pnl': 0.0, 'trade_count': 0},
                    'pnl_series': pd.Series(dtype=float),
                    'trades_log': []
                }
        
        # Use helper function to run backtest
        return _run_backtest_for_pair(
            pair_data=pair_data,
            s1=s1,
            s2=s2,
            cfg=cfg,
            capital_per_pair=capital_per_pair,
            bar_minutes=bar_minutes,
            metrics=metrics,
            period_label=period_label
        )
        
    except Exception as e:
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        logger.warning(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–∞—Ä—ã {s1}-{s2}: {str(e)}")
        
        # Return empty results for failed pairs
        return {
            'pnl_series': pd.Series(dtype=float),
            'trades_log': [],
            'trade_stat': {
                'pair': f'{s1}-{s2}',
                'period': period_label,
                'total_pnl': 0.0,
                'total_costs': 0.0,
                'trade_count': 0,
                'avg_pnl_per_trade': 0.0,
                'win_days': 0,
                'lose_days': 0,
                'total_days': 0,
                'max_daily_gain': 0.0,
                'max_daily_loss': 0.0
            },
            'success': False,
            'error': str(e)
        }

def validate_time_windows(
    training_start: pd.Timestamp,
    training_end: pd.Timestamp,
    testing_start: pd.Timestamp,
    testing_end: pd.Timestamp,
    bar_minutes: int
) -> bool:
    """
    Validate that time windows don't have look-ahead bias.
    
    Args:
        training_start: Start of training period
        training_end: End of training period
        testing_start: Start of testing period
        testing_end: End of testing period
        bar_minutes: Minutes per bar for minimum gap validation
        
    Returns:
        True if windows are valid, False otherwise
    """
    logger = get_logger("walk_forward.validation")
    
    # Check basic chronological order
    if training_start >= training_end:
        logger.error(f"‚ùå Training period invalid: start {training_start} >= end {training_end}")
        return False
        
    if testing_start >= testing_end:
        logger.error(f"‚ùå Testing period invalid: start {testing_start} >= end {testing_end}")
        return False
    
    # Critical: Check for look-ahead bias - training must end before testing starts
    if training_end >= testing_start:
        logger.error(f"‚ùå LOOK-AHEAD BIAS: Training ends {training_end} >= Testing starts {testing_start}")
        logger.error("   This would allow future data to leak into training!")
        return False
    
    # Ensure minimum gap between training and testing (at least one bar)
    min_gap = pd.Timedelta(minutes=bar_minutes)
    actual_gap = testing_start - training_end
    if actual_gap < min_gap:
        logger.warning(f"‚ö†Ô∏è  Small gap between training and testing: {actual_gap} < {min_gap}")
        logger.warning("   Consider increasing gap to avoid potential data leakage")
    
    # Log validation success
    logger.debug(f"‚úÖ Time windows validated:")
    logger.debug(f"   Training: {training_start} ‚Üí {training_end}")
    logger.debug(f"   Testing:  {testing_start} ‚Üí {testing_end}")
    logger.debug(f"   Gap:      {actual_gap}")
    
    return True

def validate_walk_forward_data(
    df: pd.DataFrame,
    training_start: pd.Timestamp,
    training_end: pd.Timestamp,
    testing_start: pd.Timestamp,
    testing_end: pd.Timestamp,
    min_training_days: float = 30,
    min_testing_days: float = 1,
    bar_minutes: int = 15,
) -> Tuple[bool, str]:
    """
    Validate that sufficient data exists for walk-forward step.
    
    Args:
        df: DataFrame with timestamp index
        training_start: Start of training period
        training_end: End of training period
        testing_start: Start of testing period
        testing_end: End of testing period
        min_training_days: Minimum required training days
        min_testing_days: Minimum required testing days
        bar_minutes: Bar size in minutes (used for duration/coverage calculations)
        
    Returns:
        Tuple of (is_valid, error_message)
    """
    logger = get_logger("walk_forward.data_validation")
    
    if df.empty:
        return False, "DataFrame is empty"
    
    # Check data availability for training period
    training_data = df.loc[training_start:training_end]
    if training_data.empty:
        return False, f"No training data available for period {training_start} to {training_end}"
    
    # Check data availability for testing period
    testing_data = df.loc[testing_start:testing_end]
    if testing_data.empty:
        return False, f"No testing data available for period {testing_start} to {testing_end}"
    
    bar_delta = pd.Timedelta(minutes=bar_minutes)

    # Check minimum training period length (inclusive of last bar)
    training_days = (training_end - training_start + bar_delta) / pd.Timedelta(days=1)
    if training_days < min_training_days:
        return False, f"Training period too short: {training_days:.2f} days < {min_training_days} required"
    
    # Check minimum testing period length (inclusive of last bar)
    testing_days = (testing_end - testing_start + bar_delta) / pd.Timedelta(days=1)
    if testing_days < min_testing_days:
        return False, f"Testing period too short: {testing_days:.2f} days < {min_testing_days} required"
    
    # Check for significant data gaps in training period
    training_data_days = len(training_data)
    expected_training_periods = training_days * (24 * 60 / bar_minutes)
    data_coverage_ratio = (
        training_data_days / expected_training_periods if expected_training_periods else 0.0
    )
    
    if data_coverage_ratio < 0.5:  # Less than 50% data coverage
        logger.warning(f"‚ö†Ô∏è  Low training data coverage: {data_coverage_ratio:.1%}")
        logger.warning(f"   Expected ~{expected_training_periods:.0f} periods, got {training_data_days}")
    
    # Check for significant data gaps in testing period
    testing_data_days = len(testing_data)
    expected_testing_periods = testing_days * (24 * 60 / bar_minutes)
    testing_coverage_ratio = (
        testing_data_days / expected_testing_periods if expected_testing_periods else 0.0
    )
    
    if testing_coverage_ratio < 0.3:  # Less than 30% data coverage
        logger.warning(f"‚ö†Ô∏è  Low testing data coverage: {testing_coverage_ratio:.1%}")
        logger.warning(f"   Expected ~{expected_testing_periods:.0f} periods, got {testing_data_days}")
    
    logger.debug(f"‚úÖ Data validation passed:")
    logger.debug(f"   Training: {training_data_days} periods ({data_coverage_ratio:.1%} coverage)")
    logger.debug(f"   Testing:  {testing_data_days} periods ({testing_coverage_ratio:.1%} coverage)")
    
    return True, "Data validation successful"

@logged_time("run_walk_forward_analysis")
def run_walk_forward(cfg: AppConfig, use_memory_map: bool = True) -> dict[str, float]:
    """Run walk-forward analysis and return aggregated performance metrics.
    
    Args:
        cfg: Application configuration
        use_memory_map: Whether to use memory-mapped data optimization
    """
    start_time = time.time()
    setup_logging_from_config(cfg)
    logger = get_logger("walk_forward")

    # Setup BLAS threading limits before any parallel processing to prevent oversubscription
    from coint2.core.memory_optimization import setup_blas_threading_limits
    blas_info = setup_blas_threading_limits(num_threads=1, verbose=True)
    logger.info(f"üßµ BLAS threading configured: {blas_info['status']}")
    logger.info(f"üßµ Environment variables set: {len(blas_info['env_vars_set'])}")
    
    if use_memory_map:
        logger.info("üß† Memory-mapped optimization enabled")
    else:
        logger.info("üìä Using traditional data processing")

    # Setup and initial data loading
    with time_block("initializing data handler"):
        handler = DataHandler(cfg)
        handler.clear_cache()
    clean_window, excluded_symbols = resolve_data_filters(cfg)

    start_date = pd.to_datetime(cfg.walk_forward.start_date)
    end_date = pd.to_datetime(cfg.walk_forward.end_date)
    # –î–ª—è –Ω–æ–≤–æ–π –ª–æ–≥–∏–∫–∏: –Ω—É–∂–Ω—ã –¥–∞–Ω–Ω—ã–µ –Ω–∞ training_period_days —Ä–∞–Ω—å—à–µ start_date
    full_range_start = start_date - pd.Timedelta(days=cfg.walk_forward.training_period_days)
    
    logger.info("üìÖ –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: start_date = –Ω–∞—á–∞–ª–æ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    logger.info(f"üìä –î–∞–Ω–Ω—ã–µ –Ω—É–∂–Ω—ã —Å: {full_range_start.date()} (–¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –ø–µ—Ä–≤–æ–≥–æ —à–∞–≥–∞)")
    
    logger.info(f"üéØ Walk-Forward –∞–Ω–∞–ª–∏–∑: {start_date.date()} ‚Üí {end_date.date()}")
    logger.info(f"Training –ø–µ—Ä–∏–æ–¥: {cfg.walk_forward.training_period_days} –¥–Ω–µ–π")
    logger.info(f"Testing –ø–µ—Ä–∏–æ–¥: {cfg.walk_forward.testing_period_days} –¥–Ω–µ–π")
    logger.info(f"üí∞ –ù–∞—á–∞–ª—å–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª: ${cfg.portfolio.initial_capital:,.0f}")
    logger.info(f"‚öôÔ∏è –ú–∞–∫—Å–∏–º—É–º –ø–æ–∑–∏—Ü–∏–π: {cfg.portfolio.max_active_positions}")
    logger.info(f"üìä –†–∏—Å–∫ –Ω–∞ –ø–æ–∑–∏—Ü–∏—é: {cfg.portfolio.risk_per_position_pct:.1%}")
    
    # Validate configuration parameters
    logger.info("üîç –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏...")
    try:
        # Extract parameters for validation
        params = {
            'z_entry': getattr(cfg.backtest, 'zscore_threshold', 2.0),
            'z_exit': getattr(cfg.backtest, 'zscore_exit', 0.0),
            'sl_mult': getattr(cfg.backtest, 'stop_loss_multiplier', 2.0),
            'time_stop_mult': getattr(cfg.backtest, 'time_stop_multiplier', None),
            'max_active_positions': cfg.portfolio.max_active_positions,
            'max_position_size_pct': getattr(cfg.portfolio, 'max_position_size_pct', 1.0),
            'risk_per_position_pct': cfg.portfolio.risk_per_position_pct
        }
        
        from optimiser.metric_utils import validate_params
        validated_params = validate_params(params)
        logger.info("‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤–∞–ª–∏–¥–Ω—ã")
        
        # Update config with validated parameters if they were corrected
        if validated_params != params:
            logger.info("üìù –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –±—ã–ª–∏ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω—ã:")
            if hasattr(cfg.backtest, 'zscore_threshold'):
                cfg.backtest.zscore_threshold = validated_params['z_entry']
            if hasattr(cfg.backtest, 'zscore_exit'):
                cfg.backtest.zscore_exit = validated_params['z_exit']
            if hasattr(cfg.backtest, 'stop_loss_multiplier'):
                cfg.backtest.stop_loss_multiplier = validated_params['sl_mult']
            if hasattr(cfg.backtest, 'time_stop_multiplier') and validated_params['time_stop_mult'] is not None:
                cfg.backtest.time_stop_multiplier = validated_params['time_stop_mult']
            cfg.portfolio.max_active_positions = validated_params['max_active_positions']
            if hasattr(cfg.portfolio, 'max_position_size_pct'):
                cfg.portfolio.max_position_size_pct = validated_params['max_position_size_pct']
            cfg.portfolio.risk_per_position_pct = validated_params['risk_per_position_pct']
            
            for key, (old_val, new_val) in zip(params.keys(), zip(params.values(), validated_params.values())):
                if old_val != new_val:
                    logger.info(f"   {key}: {old_val} ‚Üí {new_val}")
                    
    except ValueError as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {e}")
        raise

    fixed_pairs = None
    pairs_file = getattr(cfg.walk_forward, "pairs_file", None)
    if pairs_file:
        fixed_pairs = load_pair_tuples(pairs_file)
        if not fixed_pairs:
            raise ValueError(f"–§–∞–π–ª pairs_file –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–∞—Ä: {pairs_file}")
        logger.info(f"üîí WFA: —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π universe –∏–∑ {pairs_file} ({len(fixed_pairs)} –ø–∞—Ä)")
    else:
        logger.info("üß≠ WFA: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –æ—Ç–±–æ—Ä –ø–∞—Ä –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ")

    # Calculate walk-forward steps
    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: start_date —Ç–µ–ø–µ—Ä—å –Ω–∞—á–∞–ª–æ –¢–ï–°–¢–û–í–û–ì–û –ø–µ—Ä–∏–æ–¥–∞, –∞ –Ω–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ
    current_test_start = start_date
    walk_forward_steps = []
    bar_minutes = getattr(cfg.pair_selection, "bar_minutes", None) or 15
    bar_delta = pd.Timedelta(minutes=bar_minutes)
    while current_test_start < end_date:
        # –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –ø–µ—Ä–∏–æ–¥ –ü–†–ï–î–®–ï–°–¢–í–£–ï–¢ —Ç–µ—Å—Ç–æ–≤–æ–º—É
        training_start = current_test_start - pd.Timedelta(days=cfg.walk_forward.training_period_days)
        # –ó–∞–≤–µ—Ä—à–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –ø–µ—Ä–∏–æ–¥ –∑–∞ –æ–¥–∏–Ω –±–∞—Ä –¥–æ –Ω–∞—á–∞–ª–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ
        training_end = current_test_start - bar_delta
        testing_start = current_test_start
        testing_end = testing_start + pd.Timedelta(days=cfg.walk_forward.testing_period_days)
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏—è, –µ—Å–ª–∏ –Ω–∞—á–∞–ª–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ—Å—Ç–∞ –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ —Ä–∞–º–∫–∏ end_date
        if testing_start >= end_date:
            break

        logger.info(f"  –®–∞–≥ {len(walk_forward_steps)+1}: —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ {training_start.date()}-{training_end.date()}, —Ç–µ—Å—Ç {testing_start.date()}-{testing_end.date()}")
        walk_forward_steps.append((training_start, training_end, testing_start, testing_end))
        current_test_start = testing_end

    logger.info(f"üìà –ó–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ {len(walk_forward_steps)} walk-forward —à–∞–≥–æ–≤")

    max_steps = getattr(cfg.walk_forward, "max_steps", None)
    if max_steps is not None:
        if max_steps < 1:
            raise ValueError("walk_forward.max_steps –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å >= 1 –∏–ª–∏ null")
        if len(walk_forward_steps) > max_steps:
            logger.info(f"‚õî –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ WFA —à–∞–≥–æ–≤: {len(walk_forward_steps)} ‚Üí {max_steps}")
            walk_forward_steps = walk_forward_steps[:max_steps]
    
    # –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è lookahead bias
    for i, (tr_start, tr_end, te_start, te_end) in enumerate(walk_forward_steps):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ testing –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è training
        if te_start < tr_end:
            raise ValueError(
                f"‚ùå LOOKAHEAD BIAS DETECTED –≤ —à–∞–≥–µ {i+1}! "
                f"Testing –Ω–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è –ø–æ—Å–ª–µ training: "
                f"training_end={tr_end.date()}, testing_start={te_start.date()}"
            )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –±—É—Ñ–µ—Ä–∞ –º–µ–∂–¥—É training –∏ testing
        gap_days = (te_start - tr_end).days
        if gap_days < 1:
            logger.warning(
                f"‚ö†Ô∏è –ú–∞–ª–µ–Ω—å–∫–∏–π –±—É—Ñ–µ—Ä –º–µ–∂–¥—É training –∏ testing –≤ —à–∞–≥–µ {i+1}: {gap_days} –¥–Ω–µ–π. "
                f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 1 –¥–µ–Ω—å."
            )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ —à–∞–≥–∞–º–∏
        if i > 0:
            prev_te_end = walk_forward_steps[i-1][3]
            if te_start < prev_te_end:
                logger.warning(
                    f"‚ö†Ô∏è –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ testing –ø–µ—Ä–∏–æ–¥–æ–≤ –≤ —à–∞–≥–∞—Ö {i} –∏ {i+1}: "
                    f"–ø—Ä–µ–¥—ã–¥—É—â–∏–π testing_end={prev_te_end.date()}, —Ç–µ–∫—É—â–∏–π testing_start={te_start.date()}"
                )
    
    # –û–¢–õ–ê–î–û–ß–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø—Ä–æ–±–ª–µ–º
    if len(walk_forward_steps) == 0:
        logger.warning("‚ö†Ô∏è  –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ù–µ—Ç walk-forward —à–∞–≥–æ–≤!")
        logger.warning(f"   start_date (–Ω–∞—á–∞–ª–æ —Ç–µ—Å—Ç–∞): {start_date.date()}")
        logger.warning(f"   end_date (–∫–æ–Ω–µ—Ü –≤—ã–±–æ—Ä–∫–∏): {end_date.date()}")
        logger.warning(f"   training_period_days: {cfg.walk_forward.training_period_days}")
        logger.warning(f"   testing_period_days: {cfg.walk_forward.testing_period_days}")
        
        # –†–∞—Å—Å—á–∏—Ç–∞–µ–º, —á—Ç–æ –¥–æ–ª–∂–Ω–æ –±—ã–ª–æ –±—ã—Ç—å
        would_be_testing_end = start_date + pd.Timedelta(days=cfg.walk_forward.testing_period_days)
        needed_training_start = start_date - pd.Timedelta(days=cfg.walk_forward.training_period_days)
        
        logger.warning(f"   testing_end –±—ã–ª –±—ã: {would_be_testing_end.date()}")
        logger.warning(f"   –Ω—É–∂–Ω—ã –¥–∞–Ω–Ω—ã–µ —Å: {needed_training_start.date()}")
        
        if would_be_testing_end > end_date:
            logger.warning(f"   ‚ùå –ü–†–ò–ß–ò–ù–ê: testing_end ({would_be_testing_end.date()}) > end_date ({end_date.date()})")
            logger.warning(f"   ‚úÖ –†–ï–®–ï–ù–ò–ï: –ü—Ä–æ–¥–ª–∏—Ç–µ end_date –¥–æ {would_be_testing_end.date()} –∏–ª–∏ —Å–æ–∫—Ä–∞—Ç–∏—Ç–µ testing_period_days")
        
    else:
        logger.info("‚úÖ –®–∞–≥–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω—ã:")
        for i, (tr_start, tr_end, te_start, te_end) in enumerate(walk_forward_steps, 1):
            logger.info(f"   –®–∞–≥ {i}: —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ {tr_start.date()}-{tr_end.date()}, —Ç–µ—Å—Ç {te_start.date()}-{te_end.date()}")

    # Memory-mapped data optimization
    memory_map_path: str | None = None
    if use_memory_map and walk_forward_steps:
        logger.info("üóÇÔ∏è Consolidating price data for memory-mapped access...")
        
        # Calculate full data range needed for all steps
        earliest_start = min(step[0] for step in walk_forward_steps)  # earliest training_start
        latest_end = max(step[3] for step in walk_forward_steps)      # latest testing_end
        
        # Add buffer for safety
        data_start = earliest_start - pd.Timedelta(days=5)
        data_end = latest_end + pd.Timedelta(days=5)
        
        logger.info(f"üìä Consolidating data range: {data_start.date()} ‚Üí {data_end.date()}")
        
        try:
            # Consolidate all price data into a single memory-mapped file
            consolidated_path = Path(cfg.data_dir).parent / ".cache" / "consolidated_prices.parquet"
            consolidated_ok = consolidate_price_data(
                str(cfg.data_dir),
                str(consolidated_path),
                data_start,
                data_end,
                clean_window=clean_window,
                exclude_symbols=excluded_symbols,
            )

            if not consolidated_ok:
                raise RuntimeError("consolidate_price_data failed")
            
            # Initialize global memory-mapped data
            initialize_global_price_data(str(consolidated_path))
            memory_map_path = str(consolidated_path)
            
            # Verify memory mapping is working
            if GLOBAL_PRICE is not None and not GLOBAL_PRICE.empty:
                sample_symbols = list(GLOBAL_PRICE.columns[: min(2, len(GLOBAL_PRICE.columns))])
                if sample_symbols:
                    data_view = get_price_data_view(sample_symbols)
                    verify_no_data_copies(data_view, GLOBAL_PRICE)
            
            logger.info("‚úÖ Memory-mapped data initialized successfully")
            
            # Start memory monitoring if in debug mode
            if logger.isEnabledFor(10):  # DEBUG level
                monitor_memory_usage()
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Memory-mapped optimization failed: {e}")
            logger.warning("Falling back to traditional data loading")
            use_memory_map = False

    # Initialize tracking variables
    aggregated_pnl = pd.Series(dtype=float)
    daily_pnl = []
    equity_data = []
    pair_count_data = []
    trade_stats = []
    all_trades_log = []
    pair_history: list[list[tuple[str, str]]] = []

    portfolio = Portfolio(
        initial_capital=cfg.portfolio.initial_capital,
        max_active_positions=cfg.portfolio.max_active_positions,
        config=cfg.portfolio,
    )
    equity_data.append((start_date, portfolio.get_current_equity()))

    # Execute walk-forward steps
    step_tracker = ProgressTracker(len(walk_forward_steps), "Walk-forward steps", step=1)
    
    for step_idx, (training_start, training_end, testing_start, testing_end) in enumerate(walk_forward_steps, 1):
        step_tag = f"WF-—à–∞–≥ {step_idx}/{len(walk_forward_steps)}"
        
        # Initialize equity curve with first test window start date (removes artificial 1970-01-01)
        if step_idx == 1:
            portfolio.initialize_equity_curve(testing_start)
            logger.info(f"üìà –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è equity –∫—Ä–∏–≤–æ–π —Å –ø–µ—Ä–≤–æ–π —Ç–µ—Å—Ç–æ–≤–æ–π –¥–∞—Ç—ã: {testing_start}")
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –í–ê–õ–ò–î–ê–¶–ò–Ø: –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫–æ–Ω –Ω–∞ look-ahead bias
        logger.info(f"üîç {step_tag}: –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫–æ–Ω...")
        if not validate_time_windows(training_start, training_end, testing_start, testing_end, bar_minutes):
            logger.error(f"‚ùå {step_tag}: –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫–æ–Ω –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫ —à–∞–≥–∞")
            continue
        
        with time_block(f"{step_tag}: training {training_start.date()}-{training_end.date()}, testing {testing_start.date()}-{testing_end.date()}"):
            # Load data only for this step
            with time_block("loading step data"):
                step_df_long = load_master_dataset(
                    cfg.data_dir,
                    training_start,
                    testing_end,
                    clean_window=clean_window,
                    exclude_symbols=excluded_symbols,
                )

            if step_df_long.empty:
                logger.warning(f"  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —à–∞–≥–∞ {step_idx}, –ø—Ä–æ–ø—É—Å–∫.")
                continue

            step_df = step_df_long.pivot_table(index="timestamp", columns="symbol", values="close", observed=False)
            
            # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –í–ê–õ–ò–î–ê–¶–ò–Ø: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫–æ–Ω
            is_data_valid, data_error = validate_walk_forward_data(
                step_df,
                training_start,
                training_end,
                testing_start,
                testing_end,
                min_training_days=cfg.walk_forward.training_period_days,
                min_testing_days=cfg.walk_forward.testing_period_days,
                bar_minutes=bar_minutes,
            )
            if not is_data_valid:
                logger.warning(f"‚ö†Ô∏è  {step_tag}: {data_error}, –ø—Ä–æ–ø—É—Å–∫ —à–∞–≥–∞")
                continue
            
            training_slice = step_df.loc[training_start:training_end]
            testing_slice = step_df.loc[testing_start:testing_end]
            
            # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü –¥–∞–Ω–Ω—ã—Ö
            logger.info(f"üìä {step_tag}: –ì—Ä–∞–Ω–∏—Ü—ã –¥–∞–Ω–Ω—ã—Ö:")
            logger.info(f"   Training: {training_start} ‚Üí {training_end} ({training_slice.shape[0]:,} —Å—Ç—Ä–æ–∫ √ó {training_slice.shape[1]} —Å–∏–º–≤–æ–ª–æ–≤)")
            logger.info(f"   Testing:  {testing_start} ‚Üí {testing_end} ({testing_slice.shape[0]:,} —Å—Ç—Ä–æ–∫ √ó {testing_slice.shape[1]} —Å–∏–º–≤–æ–ª–æ–≤)")
            logger.info(f"   –í—Ä–µ–º–µ–Ω–Ω–æ–π —Ä–∞–∑—Ä—ã–≤: {testing_start - training_end}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞—â–∏—Ç–∞)
            if training_slice.index.max() >= testing_slice.index.min():
                logger.error(f"‚ùå {step_tag}: –û–ë–ù–ê–†–£–ñ–ï–ù–û –ü–ï–†–ï–ö–†–´–¢–ò–ï –î–ê–ù–ù–´–•!")
                logger.error(f"   –ü–æ—Å–ª–µ–¥–Ω—è—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è –º–µ—Ç–∫–∞: {training_slice.index.max()}")
                logger.error(f"   –ü–µ—Ä–≤–∞—è —Ç–µ—Å—Ç–æ–≤–∞—è –º–µ—Ç–∫–∞: {testing_slice.index.min()}")
                logger.error("   –≠—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ look-ahead bias! –ü—Ä–æ–ø—É—Å–∫ —à–∞–≥–∞.")
                continue
                
            training_normalization_params = {}
            if training_slice.empty or len(training_slice.columns) < 2:
                logger.warning(f"  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤ —à–∞–≥–µ {step_idx}")
                pairs = []
            else:
                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï LOOKAHEAD BIAS: –≥–æ—Ç–æ–≤–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                fallback_first_values = {}
                if not training_slice.empty:
                    for col in training_slice.columns:
                        first_valid_idx = training_slice[col].first_valid_index()
                        if first_valid_idx is not None:
                            fallback_first_values[col] = training_slice.loc[first_valid_idx, col]
                
                # Normalize training data
                with time_block("normalizing training data"):
                    logger.debug(f"  –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {len(training_slice.columns)} —Å–∏–º–≤–æ–ª–æ–≤")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤ –¥–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
                    symbols_before = set(training_slice.columns)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏–º–≤–æ–ª—ã —Å –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–π —Ü–µ–Ω–æ–π (max=min)
                    constant_price_symbols = [col for col in training_slice.columns
                                             if training_slice[col].max() == training_slice[col].min()]
                    if constant_price_symbols:
                        logger.info(f"  –°–∏–º–≤–æ–ª—ã —Å –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–π —Ü–µ–Ω–æ–π: {len(constant_price_symbols)}")
                        logger.debug(f"  –°–ø–∏—Å–æ–∫: {', '.join(sorted(constant_price_symbols))}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏–º–≤–æ–ª—ã —Å NaN –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
                    nan_symbols = [col for col in training_slice.columns
                                  if training_slice[col].isna().any()]
                    if nan_symbols:
                        logger.info(f"  –°–∏–º–≤–æ–ª—ã —Å NaN –∑–Ω–∞—á–µ–Ω–∏—è–º–∏: {len(nan_symbols)}")
                        logger.debug(f"  –°–ø–∏—Å–æ–∫: {', '.join(sorted(nan_symbols))}")
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
                    norm_method = getattr(cfg.data_processing, 'normalization_method', 'minmax')
                    fill_method = getattr(cfg.data_processing, 'fill_method', 'ffill')
                    min_history_ratio = getattr(cfg.data_processing, 'min_history_ratio', 0.8)
                    handle_constant = getattr(cfg.data_processing, 'handle_constant', True)
                    
                    logger.info(f"  –ü—Ä–∏–º–µ–Ω—è–µ–º –º–µ—Ç–æ–¥ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {norm_method}")
                    rolling_window = getattr(cfg.backtest, 'rolling_window', None)
                    normalized_training, norm_stats = preprocess_and_normalize_data(
                        training_slice,
                        min_history_ratio=min_history_ratio,
                        fill_method=fill_method,
                        norm_method=norm_method,
                        handle_constant=handle_constant,
                        rolling_window=rolling_window,
                        return_stats=True
                    )
                    
                    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
                    logger.info("  –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:")
                    logger.info(f"    –ò—Å—Ö–æ–¥–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã: {norm_stats['initial_symbols']}")
                    logger.info(f"    –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è: {norm_stats['low_history_ratio']}")
                    logger.info(f"    –ü–æ—Å—Ç–æ—è–Ω–Ω–∞—è —Ü–µ–Ω–∞: {norm_stats['constant_price']}")
                    logger.info(f"    NaN –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {norm_stats['nan_after_norm']}")
                    logger.info(f"    –ò—Ç–æ–≥–æ–≤—ã–µ —Å–∏–º–≤–æ–ª—ã: {norm_stats['final_symbols']} ({norm_stats['final_symbols']/norm_stats['initial_symbols']*100:.1f}%)")
                    
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Ç–µ—Ä—è–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                    symbols_after = set(normalized_training.columns)
                    dropped_symbols = symbols_before - symbols_after
                    logger.info(f"  –ü–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {len(normalized_training.columns)} —Å–∏–º–≤–æ–ª–æ–≤ (–ø–æ—Ç–µ—Ä—è–Ω–æ {len(dropped_symbols)})")
                    if dropped_symbols and len(dropped_symbols) <= 20:
                        logger.info(f"  –û—Ç–±—Ä–æ—à–µ–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã: {', '.join(sorted(dropped_symbols))}")
                    elif dropped_symbols:
                        logger.info(f"  –û—Ç–±—Ä–æ—à–µ–Ω–æ –º–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: {len(dropped_symbols)} –∏–∑ {len(symbols_before)}")
                    
                normalization_stats = norm_stats.get('normalization_stats', {})
                training_normalization_params = {}
                if normalization_stats:
                    method = normalization_stats.get('method', norm_method)
                    if method in ("minmax", "zscore"):
                        params = {}
                        if method == "minmax":
                            mins = normalization_stats.get('min', {})
                            maxs = normalization_stats.get('max', {})
                            for symbol in normalized_training.columns:
                                if symbol in mins and symbol in maxs:
                                    params[symbol] = {'min': mins[symbol], 'max': maxs[symbol]}
                        else:
                            means = normalization_stats.get('mean', {})
                            stds = normalization_stats.get('std', {})
                            for symbol in normalized_training.columns:
                                if symbol in means and symbol in stds:
                                    params[symbol] = {'mean': means[symbol], 'std': stds[symbol]}
                        training_normalization_params = {
                            'method': method,
                            'params': params,
                            'fill_method': fill_method,
                        }
                    else:
                        training_normalization_params = dict(normalization_stats)
                        training_normalization_params['fill_method'] = fill_method
                elif fallback_first_values:
                    training_normalization_params = fallback_first_values

                if len(normalized_training.columns) < 2:
                    logger.warning("  –ü–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Å—Ç–∞–ª–æ—Å—å –º–µ–Ω–µ–µ 2 —Å–∏–º–≤–æ–ª–æ–≤")
                    pairs = []
                else:
                    pairs_for_filter = []
                    if fixed_pairs:
                        available_symbols = set(training_slice.columns)
                        pairs_for_filter = [
                            (s1, s2)
                            for s1, s2 in fixed_pairs
                            if s1 in available_symbols and s2 in available_symbols
                        ]
                        dropped = len(fixed_pairs) - len(pairs_for_filter)
                        logger.info(
                            f"  –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π universe: {len(pairs_for_filter)} –ø–∞—Ä "
                            f"(–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {dropped} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö)"
                        )
                    else:
                        # SSD computation
                        with time_block("SSD computation"):
                            logger.info("  –†–∞—Å—á–µ—Ç SSD –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä (–±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è)")
                            # –°–Ω–∞—á–∞–ª–∞ —Å—á–∏—Ç–∞–µ–º SSD –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä
                            ssd = math_utils.calculate_ssd(normalized_training, top_k=None)
                            logger.info(f"  SSD —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–≤—Å–µ –ø–∞—Ä—ã): {len(ssd)} –ø–∞—Ä")

                            # –ó–∞—Ç–µ–º –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ top-N –ø–∞—Ä –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                            ssd_top_n = cfg.pair_selection.ssd_top_n
                            if len(ssd) > ssd_top_n:
                                logger.info(f"  –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ top-{ssd_top_n} –ø–∞—Ä –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                                ssd = ssd.sort_values().head(ssd_top_n)
                        pairs_for_filter = [(s1, s2) for s1, s2 in ssd.index]

                    if not pairs_for_filter:
                        logger.warning("  –ù–µ—Ç –ø–∞—Ä –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
                        pairs = []
                    else:
                        # Filter pairs
                        with time_block("filtering pairs by cointegration and half-life"):
                            from coint2.pipeline.filters import filter_pairs_by_coint_and_half_life
                            filter_n_jobs = getattr(cfg.backtest, 'n_jobs', None)
                            if filter_n_jobs == -1:
                                filter_n_jobs = os.cpu_count() or 1
                            if not filter_n_jobs or filter_n_jobs < 1:
                                filter_n_jobs = 1
                            filter_backend = os.getenv("COINT_FILTER_BACKEND", "threads")
                            filtered_pairs = filter_pairs_by_coint_and_half_life(
                                pairs_for_filter,
                                training_slice,
                                pvalue_threshold=cfg.pair_selection.coint_pvalue_threshold,
                                min_beta=cfg.filter_params.min_beta,
                                max_beta=cfg.filter_params.max_beta,
                                min_half_life=cfg.filter_params.min_half_life_days,
                                max_half_life=cfg.filter_params.max_half_life_days,
                                min_mean_crossings=cfg.filter_params.min_mean_crossings,
                                max_hurst_exponent=cfg.filter_params.max_hurst_exponent,
                                min_correlation=cfg.pair_selection.min_correlation,
                                save_filter_reasons=cfg.pair_selection.save_filter_reasons,
                                kpss_pvalue_threshold=cfg.pair_selection.kpss_pvalue_threshold,
                                n_jobs=filter_n_jobs,
                                parallel_backend=filter_backend,
                                # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã commission_pct –∏ slippage_pct —É–¥–∞–ª–µ–Ω—ã
                            )
                            current_pair_keys = [(s1, s2) for s1, s2, *_ in filtered_pairs]
                            stability_window = int(getattr(cfg.pair_selection, "pair_stability_window_steps", 0) or 0)
                            stability_min = int(getattr(cfg.pair_selection, "pair_stability_min_steps", 0) or 0)
                            if stability_window and stability_min:
                                history_steps = len(pair_history)
                                if history_steps >= stability_min:
                                    history_window = pair_history[-stability_window:]
                                    counts = Counter(pair for step in history_window for pair in step)
                                    stable_pairs = {pair for pair, count in counts.items() if count >= stability_min}
                                    before_count = len(filtered_pairs)
                                    filtered_pairs = [
                                        pair for pair in filtered_pairs if (pair[0], pair[1]) in stable_pairs
                                    ]
                                    logger.info(
                                        "  Pair stability filter: %d ‚Üí %d (window=%d, min_steps=%d, history=%d)",
                                        before_count,
                                        len(filtered_pairs),
                                        stability_window,
                                        stability_min,
                                        history_steps,
                                    )
                                else:
                                    logger.info(
                                        "  Pair stability filter: insufficient history (%d < %d), skip",
                                        history_steps,
                                        stability_min,
                                    )
                            pair_history.append(current_pair_keys)
                            logger.info(
                                f"  –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: {len(pairs_for_filter)} ‚Üí {len(filtered_pairs)} –ø–∞—Ä"
                            )
                            pairs = filtered_pairs

        # Select all filtered pairs for trading (no limit here)
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–∞—Ä—ã –ø–æ –∫–∞—á–µ—Å—Ç–≤—É (–ø–æ —É–±—ã–≤–∞–Ω–∏—é std, —á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç –±–æ–ª—å—à—É—é –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å —Å–ø—Ä–µ–¥–∞)
        # max_active_positions —Ç–µ–ø–µ—Ä—å –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫—Ä—ã—Ç—ã–µ –ø–æ–∑–∏—Ü–∏–∏
        if pairs:
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–∞—Ä—ã –ø–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—é —Å–ø—Ä–µ–¥–∞ (–±–æ–ª—å—à–µ = –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –±–æ–ª–µ–µ –ø—Ä–∏–±—ã–ª—å–Ω–æ)
            quality_sorted_pairs = sorted(pairs, key=lambda x: abs(x[4]), reverse=True)  # x[4] = std
            active_pairs = quality_sorted_pairs  # –ë–µ—Ä–µ–º –í–°–ï –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä—ã
            max_pairs = int(getattr(cfg.pair_selection, "max_pairs", 0) or 0)
            if max_pairs > 0 and len(active_pairs) > max_pairs:
                active_pairs = active_pairs[:max_pairs]
                logger.info(
                    "  –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ max_pairs=%d: %d ‚Üí %d",
                    max_pairs,
                    len(quality_sorted_pairs),
                    len(active_pairs),
                )
            elif max_pairs > 0:
                logger.info(
                    "  –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ max_pairs=%d: %d –ø–∞—Ä (–ª–∏–º–∏—Ç –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç)",
                    max_pairs,
                    len(active_pairs),
                )
            logger.info("  –¢–æ–ø-3 –ø–∞—Ä—ã –ø–æ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ —Å–ø—Ä–µ–¥–∞:")
            for i, (s1, s2, beta, mean, std, metrics) in enumerate(active_pairs[:3], 1):
                logger.info(f"    {i}. {s1}-{s2}: beta={beta:.4f}, std={std:.4f}")
        else:
            active_pairs = []
        
        num_active_pairs = len(active_pairs)
        logger.info(f"  –í—Å–µ–≥–æ –ø–∞—Ä –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏: {num_active_pairs} (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ {cfg.portfolio.max_active_positions} –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫—Ä—ã—Ç—ã–º –ø–æ–∑–∏—Ü–∏—è–º)")

        step_pnl = pd.Series(dtype=float)
        total_step_pnl = 0.0

        current_equity = portfolio.get_current_equity()
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º target_concurrency –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        target_concurrency = min(cfg.portfolio.max_active_positions, num_active_pairs) if num_active_pairs > 0 else 0
        
        logger.info(
            f"  üí∞ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –∫–∞–ø–∏—Ç–∞–ª–∞: equity=${current_equity:,.2f}, –∫–∞–Ω–¥–∏–¥–∞—Ç–Ω—ã—Ö_–ø–∞—Ä={num_active_pairs}, max_–ø–æ–∑–∏—Ü–∏–π={cfg.portfolio.max_active_positions}"
        )
        logger.info(
            f"  üéØ Target concurrency: {target_concurrency} (–æ–∂–∏–¥–∞–µ–º–æ–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ –ø–æ–∑–∏—Ü–∏–π)"
        )
        
        if num_active_pairs > 0:
            capital_per_pair = portfolio.calculate_position_risk_capital(
                risk_per_position_pct=cfg.portfolio.risk_per_position_pct,
                max_position_size_pct=getattr(cfg.portfolio, 'max_position_size_pct', 1.0),
                num_selected_pairs=num_active_pairs
            )
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ä–∞—Å—á–µ—Ç–æ–≤
            risk_capital = current_equity * cfg.portfolio.risk_per_position_pct
            max_position_capital = current_equity * getattr(cfg.portfolio, 'max_position_size_pct', 1.0)
            base_capital_per_pair = current_equity / target_concurrency if target_concurrency > 0 else 0
            
            logger.info(f"  üìä –î–µ—Ç–∞–ª–∏ —Ä–∞—Å—á–µ—Ç–∞ –∫–∞–ø–∏—Ç–∞–ª–∞:")
            logger.info(f"     ‚Ä¢ –ë–∞–∑–æ–≤—ã–π –∫–∞–ø–∏—Ç–∞–ª –Ω–∞ –ø–∞—Ä—É: ${base_capital_per_pair:,.2f} (equity / target_concurrency)")
            logger.info(f"     ‚Ä¢ –†–∏—Å–∫-–∫–∞–ø–∏—Ç–∞–ª: ${risk_capital:,.2f} ({cfg.portfolio.risk_per_position_pct:.1%} –æ—Ç equity)")
            logger.info(f"     ‚Ä¢ –ú–∞–∫—Å. —Ä–∞–∑–º–µ—Ä –ø–æ–∑–∏—Ü–∏–∏: ${max_position_capital:,.2f} ({getattr(cfg.portfolio, 'max_position_size_pct', 1.0):.1%} –æ—Ç equity)")
            logger.info(f"     ‚Ä¢ –ò—Ç–æ–≥–æ–≤—ã–π –∫–∞–ø–∏—Ç–∞–ª –Ω–∞ –ø–∞—Ä—É: ${capital_per_pair:,.2f}")
        else:
            capital_per_pair = 0.0

        if capital_per_pair < 0:
            logger.error(
                f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ö–∞–ø–∏—Ç–∞–ª –Ω–∞ –ø–∞—Ä—É —Å—Ç–∞–ª –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º ({capital_per_pair})."
            )
            capital_per_pair = 0.0

        period_label = f"{training_start.strftime('%m/%d')}-{testing_end.strftime('%m/%d')}"
        pair_count_data.append((period_label, len(active_pairs)))

        # Run backtests for active pairs
        if active_pairs:
            # Create portfolio for position management
            position_portfolio = Portfolio(
                initial_capital=current_equity,
                max_active_positions=cfg.portfolio.max_active_positions,
                config=cfg.portfolio,
            )
            pair_tracker = ProgressTracker(len(active_pairs), f"{step_tag} backtests", step=max(1, len(active_pairs)//5))
            logger.info(f"üöÄ {step_tag}: –ù–∞—á–∏–Ω–∞–µ–º –±—ç–∫—Ç–µ—Å—Ç—ã –¥–ª—è {len(active_pairs)} –ø–∞—Ä...")
            
            # Determine number of parallel jobs
            n_jobs = getattr(cfg.backtest, 'n_jobs', -1)  # -1 means use all available cores
            if n_jobs == -1:
                n_jobs = os.cpu_count()
            
            logger.info(f"üöÄ {step_tag}: –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(active_pairs)} –ø–∞—Ä –Ω–∞ {n_jobs} —è–¥—Ä–∞—Ö...")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –ø–∞—Ä
            if len(active_pairs) > 50:
                logger.info(f"  üìä –ë–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –ø–∞—Ä ({len(active_pairs)}), –æ–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è: ~{len(active_pairs)*2//60+1} –º–∏–Ω")
                logger.info(f"  üîÑ –ü—Ä–æ–≥—Ä–µ—Å—Å –±—É–¥–µ—Ç –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å—Å—è –∫–∞–∂–¥—ã–µ {max(1, len(active_pairs)//10)} –ø–∞—Ä")
            
            # Process pairs in parallel with progress tracking
            start_time = time.time()
            
            if use_memory_map and memory_map_path:
                logger.info(f"üöÄ {step_tag}: –†–µ–∂–∏–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç–∏: –ø—Ä–æ—Ü–µ—Å—Å—ã")
                pair_args = [
                    (
                        (s1, s2),
                        testing_start,
                        testing_end,
                        cfg,
                        capital_per_pair,
                        bar_minutes,
                        period_label,
                        (beta, mean, std, metrics),
                        training_normalization_params,
                    )
                    for (s1, s2, beta, mean, std, metrics) in active_pairs
                ]
                pair_results = _parallel_map(
                    _process_pair_mmap_worker,
                    pair_args,
                    n_jobs,
                    use_processes=True,
                    initializer=_init_worker_global_price,
                    initargs=(memory_map_path,),
                )
            else:
                logger.info(f"üöÄ {step_tag}: –†–µ–∂–∏–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç–∏: –ø–æ—Ç–æ–∫–∏")
                pair_args = [
                    (
                        pair_data_tuple,
                        step_df,
                        testing_start,
                        testing_end,
                        cfg,
                        capital_per_pair,
                        bar_minutes,
                        period_label,
                        training_normalization_params,
                    )
                    for pair_data_tuple in active_pairs
                ]
                pair_results = _parallel_map(
                    _process_pair_worker,
                    pair_args,
                    n_jobs,
                    use_processes=False,
                )
            
            processing_time = time.time() - start_time
            logger.info(f"  ‚è±Ô∏è –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {processing_time:.1f}—Å ({len(active_pairs)/processing_time:.1f} –ø–∞—Ä/—Å)")
            
            # Vectorized aggregation of results from parallel processing
            import numpy as np
            
            # Separate successful and failed results
            successful_results = [r for r in pair_results if r['success']]
            failed_results = [r for r in pair_results if not r['success']]
            
            successful_count = len(successful_results)
            failed_count = len(failed_results)
            
            if successful_results:
                # Vectorized calculation of statistics
                pnl_values = np.array([r['trade_stat']['total_pnl'] for r in successful_results])
                trade_counts = np.array([r['trade_stat']['trade_count'] for r in successful_results])

                # Vectorized boolean operations
                profitable_count = int(np.sum(pnl_values > 0))
                total_trades_count = int(np.sum(trade_counts))

                all_pnl_series = [result['pnl_series'] for result in successful_results]
                all_positions = [result.get('positions', pd.Series(dtype=float)) for result in successful_results]
                all_scores = [result.get('z_scores', pd.Series(dtype=float)) for result in successful_results]
                step_pnl = _simulate_realistic_portfolio(
                    all_pnl_series,
                    cfg,
                    all_positions=all_positions,
                    all_scores=all_scores,
                )
                total_step_pnl = step_pnl.sum()

                # Aggregate other data
                for result in successful_results:
                    all_trades_log.extend(result['trades_log'])
                    trade_stats.append(result['trade_stat'])
            else:
                profitable_count = 0
                total_trades_count = 0
                total_step_pnl = 0.0
            
            # Process failed results
            for result in failed_results:
                logger.warning(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–∞—Ä—ã {result['trade_stat']['pair']}: {result.get('error', 'Unknown error')}")
                trade_stats.append(result['trade_stat'])
                
                pair_tracker.update()
                
                # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤
                if len(active_pairs) > 100 and i % (len(active_pairs)//4) == 0:
                    current_pnl = sum([r['trade_stat']['total_pnl'] for r in pair_results[:i] if r['success']])
                    logger.info(f"  üìà –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ ({i}/{len(active_pairs)}): P&L ${current_pnl:+,.0f}, {profitable_count}/{successful_count} –ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö")
            
            pair_tracker.finish()
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
            logger.info(f"  ‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {successful_count} —É—Å–ø–µ—à–Ω–æ, {failed_count} —Å –æ—à–∏–±–∫–∞–º–∏")
            if successful_count > 0:
                success_rate = (successful_count / len(active_pairs)) * 100
                profit_rate = (profitable_count / successful_count) * 100
                avg_trades_per_pair = total_trades_count / successful_count
                logger.info(f"  üìä –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1f}%, –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å: {profit_rate:.1f}%, —Å–¥–µ–ª–æ–∫/–ø–∞—Ä–∞: {avg_trades_per_pair:.1f}")
            logger.info(f"  ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω—ã –±—ç–∫—Ç–µ—Å—Ç—ã –¥–ª—è {len(active_pairs)} –ø–∞—Ä")
            
            # –°—á–µ—Ç—á–∏–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –ø–∞—Ä
            total_processed_pairs = sum(len(pair_results) for pair_results in [pair_results] if pair_results)
            logger.info(f"  üìä –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø–∞—Ä –≤ —ç—Ç–æ–º —à–∞–≥–µ: {total_processed_pairs}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –±—ç–∫—Ç–µ—Å—Ç–æ–≤
            successful_pairs = len([stat for stat in trade_stats if stat['period'] == period_label and stat['total_pnl'] != 0])
            profitable_pairs = len([stat for stat in trade_stats if stat['period'] == period_label and stat['total_pnl'] > 0])
            total_trades = sum([stat['trade_count'] for stat in trade_stats if stat['period'] == period_label])
            
            logger.info(f"  üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —à–∞–≥–∞: {profitable_pairs}/{len(active_pairs)} –ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö –ø–∞—Ä, {total_trades} —Å–¥–µ–ª–æ–∫")
            logger.info(f"  üí∞ P&L —à–∞–≥–∞: ${total_step_pnl:+,.2f}")
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–¥–µ–ª–∫–∞–º
            if all_trades_log:
                step_trades = [t for t in all_trades_log if t.get('period') == period_label]
                if step_trades:
                    winning_trades = [t for t in step_trades if t.get('pnl', 0) > 0]
                    losing_trades = [t for t in step_trades if t.get('pnl', 0) < 0]
                    win_rate = len(winning_trades) / len(step_trades) * 100 if step_trades else 0
                    avg_win = sum([t['pnl'] for t in winning_trades]) / len(winning_trades) if winning_trades else 0
                    avg_loss = sum([t['pnl'] for t in losing_trades]) / len(losing_trades) if losing_trades else 0
                    profit_factor = abs(sum([t['pnl'] for t in winning_trades]) / sum([t['pnl'] for t in losing_trades])) if losing_trades else float('inf')
                    
                    logger.info(f"  üìä –í–∏–Ω—Ä–µ–π—Ç: {win_rate:.1f}% ({len(winning_trades)}/{len(step_trades)})")
                    logger.info(f"  üíµ –°—Ä–µ–¥–Ω–∏–π –≤—ã–∏–≥—Ä—ã—à: ${avg_win:.0f}, —Å—Ä–µ–¥–Ω–∏–π –ø—Ä–æ–∏–≥—Ä—ã—à: ${avg_loss:.0f}")
                    logger.info(f"  üéØ Profit Factor: {profit_factor:.2f}")
                    
                    # –¢–æ–ø-3 –ø–∞—Ä—ã –ø–æ –ø—Ä–∏–±—ã–ª–∏ –≤ —ç—Ç–æ–º —à–∞–≥–µ
                    step_pair_stats = [ts for ts in trade_stats if ts['period'] == period_label and ts['total_pnl'] > 0]
                    top_pairs = sorted(step_pair_stats, key=lambda x: x['total_pnl'], reverse=True)[:3]
                    if top_pairs:
                        top_pairs_str = ', '.join([f"{p['pair']} ${p['total_pnl']:+,.0f}" for p in top_pairs])
                        logger.info(f"  üèÜ –¢–æ–ø –ø–∞—Ä—ã: {top_pairs_str}")
            
            logger.info(f"  ‚á¢ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞–ø–∏—Ç–∞–ª–∞ –∏ –¥–Ω–µ–≤–Ω–æ–≥–æ P&L...")

        # Update equity and daily P&L
        if not step_pnl.empty:
            running_equity = portfolio.get_current_equity()
            for date, pnl in step_pnl.items():
                daily_pnl.append((date, pnl))
                running_equity += pnl
                equity_data.append((date, running_equity))
                portfolio.record_daily_pnl(pd.Timestamp(date), pnl)

        aggregated_pnl = pd.concat([aggregated_pnl, step_pnl])

        logger.info(
            f"  üíº –®–∞–≥ P&L: ${total_step_pnl:+,.2f}, –ù–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª: ${portfolio.get_current_equity():,.2f}"
        )
        logger.info("-" * 60)
        step_tracker.update()

    step_tracker.finish()
    
    # Final comprehensive results
    final_equity = portfolio.get_current_equity()
    initial_capital = cfg.portfolio.initial_capital
    total_return = (final_equity - initial_capital) / initial_capital * 100
    total_pnl = final_equity - initial_capital
    
    # Vectorized comprehensive trade statistics
    total_trades = len(all_trades_log)
    if total_trades > 0:
        # Extract PnL values for vectorized operations
        trade_pnls = np.array([t.get('pnl', 0) for t in all_trades_log])
        
        # Vectorized win/loss calculations
        winning_mask = trade_pnls > 0
        losing_mask = trade_pnls < 0
        
        winning_trades_count = int(np.sum(winning_mask))
        losing_trades_count = int(np.sum(losing_mask))
        
        overall_win_rate = winning_trades_count / total_trades * 100
        total_wins_pnl = np.sum(trade_pnls[winning_mask])
        total_losses_pnl = np.sum(trade_pnls[losing_mask])
        overall_profit_factor = abs(total_wins_pnl / total_losses_pnl) if total_losses_pnl != 0 else float('inf')
        avg_trade_pnl = total_pnl / total_trades
        
        # Vectorized pair-level statistics
        unique_pairs = set([ts['pair'] for ts in trade_stats])
        
        # Use numpy for faster filtering
        trade_counts = np.array([ts['trade_count'] for ts in trade_stats])
        pair_pnls = np.array([ts['total_pnl'] for ts in trade_stats])
        
        active_pairs_mask = trade_counts > 0
        profitable_pairs_mask = pair_pnls > 0
        
        active_pairs = [ts for i, ts in enumerate(trade_stats) if active_pairs_mask[i]]
        profitable_pairs = [ts for i, ts in enumerate(trade_stats) if profitable_pairs_mask[i]]
    
    # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –ø–∞—Ä
    total_processed_pairs_all_steps = len(trade_stats)
    
    logger.info("="*80)
    logger.info(f"üèÅ Walk-forward –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    logger.info(f"‚è±Ô∏è –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(walk_forward_steps)} —à–∞–≥–æ–≤ –∑–∞ {time.time() - start_time:.1f} —Å–µ–∫—É–Ω–¥")
    logger.info(f"üìä –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø–∞—Ä –∑–∞ –≤–µ—Å—å –∞–Ω–∞–ª–∏–∑: {total_processed_pairs_all_steps}")
    logger.info(f"üí∞ –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª: ${final_equity:,.2f} (–±—ã–ª–æ ${initial_capital:,.2f})")
    logger.info(f"üìà –û–±—â–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {total_return:+.2f}% (${total_pnl:+,.2f})")
    logger.info(f"üìä –í—Å–µ–≥–æ —Å–¥–µ–ª–æ–∫: {total_trades}")
    
    if total_trades > 0:
        logger.info(f"üéØ –û–±—â–∏–π –≤–∏–Ω—Ä–µ–π—Ç: {overall_win_rate:.1f}% ({len(winning_trades)}/{total_trades})")
        logger.info(f"üíµ –°—Ä–µ–¥–Ω—è—è —Å–¥–µ–ª–∫–∞: ${avg_trade_pnl:+.0f}")
        logger.info(f"üî• Profit Factor: {overall_profit_factor:.2f}")
        logger.info(f"üîÑ –ê–∫—Ç–∏–≤–Ω—ã—Ö –ø–∞—Ä: {len(active_pairs)}/{len(unique_pairs)}, –ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö: {len(profitable_pairs)}")
        
        # –¢–æ–ø-5 –ø–∞—Ä –ø–æ –æ–±—â–µ–π –ø—Ä–∏–±—ã–ª–∏
        top_pairs_overall = sorted(profitable_pairs, key=lambda x: x['total_pnl'], reverse=True)[:5]
        if top_pairs_overall:
            top_pairs_overall_str = ', '.join([f"{p['pair']} ${p['total_pnl']:+,.0f}" for p in top_pairs_overall])
            logger.info(f"üèÜ –¢–æ–ø-5 –ø–∞—Ä: {top_pairs_overall_str}")
    
    logger.info("="*80)
    logger.info(f"‚úì –ó–∞–≤–µ—Ä—à–µ–Ω—ã –≤—Å–µ {len(walk_forward_steps)} WF-—à–∞–≥–æ–≤")
    logger.info(f"‚á¢ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ —Ä–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫...")

    # Process results and create metrics
    with time_block("processing final results"):
        aggregated_pnl = aggregated_pnl.dropna()
        logger.info(f"  ‚á¢ –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö P&L –∑–∞ {len(aggregated_pnl)} —Ç–æ—Ä–≥–æ–≤—ã—Ö –¥–Ω–µ–π")
        
        # Create series for analysis
        if daily_pnl:
            dates, pnls = zip(*daily_pnl)
            pnl_series = pd.Series(pnls, index=pd.to_datetime(dates))
            pnl_series = pnl_series.groupby(pnl_series.index.date).sum()
            pnl_series.index = pd.to_datetime(pnl_series.index)
        else:
            pnl_series = pd.Series(dtype=float)
        
        if not portfolio.equity_curve.empty:
            equity_series = portfolio.equity_curve.dropna()
        else:
            equity_series = pd.Series([cfg.portfolio.initial_capital])

        # Calculate metrics
        logger.info(f"  ‚á¢ –†–∞—Å—á–µ—Ç –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
        logger.info(f"    üìä –î–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(aggregated_pnl)} –∑–∞–ø–∏—Å–µ–π P&L, {len(equity_series)} —Ç–æ—á–µ–∫ equity")
        
        if aggregated_pnl.empty:
            base_metrics = {
                "sharpe_ratio_abs": 0.0,
                "sharpe_ratio_on_returns": 0.0,
                "max_drawdown_abs": 0.0,
                "max_drawdown_on_equity": 0.0,
                "total_pnl": 0.0,
            }
        else:
            cumulative = aggregated_pnl.cumsum()
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–ø–∏—Ç–∞–ª –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Ä–∏—Å–∫–∞ –Ω–∞ —Å–¥–µ–ª–∫—É –∏ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
            capital_per_pair = portfolio.calculate_position_risk_capital(
                risk_per_position_pct=cfg.portfolio.risk_per_position_pct,
                max_position_size_pct=getattr(cfg.portfolio, 'max_position_size_pct', 1.0),
                num_selected_pairs=1  # For final metrics calculation, use 1 as default
            )
            equity_returns = equity_series.ffill().pct_change(fill_method=None).dropna()
            periods_per_year = float(cfg.backtest.annualizing_factor)
            if bar_minutes and bar_minutes > 0:
                periods_per_year *= 24 * 60 / bar_minutes

            sharpe_abs = performance.sharpe_ratio(equity_returns, periods_per_year)
            sharpe_on_returns = performance.sharpe_ratio_on_returns(
                aggregated_pnl, capital_per_pair, periods_per_year
            )
            max_dd_abs = performance.max_drawdown(cumulative)
            max_dd_on_equity = performance.max_drawdown_on_equity(equity_series)
            base_metrics = {
                "sharpe_ratio_abs": sharpe_abs,
                "sharpe_ratio_on_returns": sharpe_on_returns,
                "max_drawdown_abs": max_dd_abs,
                "max_drawdown_on_equity": max_dd_on_equity,
                "total_pnl": cumulative.iloc[-1] if not cumulative.empty else 0.0,
            }
        
        logger.info(f"  ‚á¢ –†–∞—Å—á–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ (–≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å, Calmar ratio, –∏ –¥—Ä.)...")
        extended_metrics = calculate_extended_metrics(pnl_series, equity_series)
        logger.info(f"  ‚úÖ –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã: {len(extended_metrics)} –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π")
        
        # Trade statistics
        logger.info(f"  ‚á¢ –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ {len(trade_stats)} –ø–∞—Ä–∞–º...")
        trade_metrics = {}
        if trade_stats:
            trades_df = pd.DataFrame(trade_stats)
            trade_metrics = {
                'total_trades': trades_df['trade_count'].sum(),
                'total_pairs_traded': len(trades_df['pair'].unique()),
                'total_costs': trades_df['total_costs'].sum(),
                'avg_trades_per_pair': trades_df['trade_count'].mean(),
                'win_rate_trades': trades_df['win_days'].sum() / max(trades_df['total_days'].sum(), 1),
                'best_pair_pnl': trades_df['total_pnl'].max(),
                'worst_pair_pnl': trades_df['total_pnl'].min(),
                'avg_pnl_per_pair': trades_df['total_pnl'].mean(),
            }

            if 'entry_notional_count' in trades_df.columns:
                total_entry_count = trades_df['entry_notional_count'].sum()
                total_entry_notional = (trades_df['entry_notional_avg'] * trades_df['entry_notional_count']).sum()
                trade_metrics.update({
                    'entry_notional_count': float(total_entry_count),
                    'entry_notional_cap_hits': float(trades_df['entry_notional_cap_hits'].sum()),
                    'entry_notional_below_min': float(trades_df['entry_notional_below_min'].sum()),
                    'entry_notional_avg': float(total_entry_notional / max(total_entry_count, 1)),
                    'entry_notional_p50': float(trades_df['entry_notional_p50'].median()),
                    'entry_notional_min': float(trades_df['entry_notional_min'].min()),
                    'entry_notional_max': float(trades_df['entry_notional_max'].max()),
                })
        
        all_metrics = {**base_metrics, **extended_metrics, **trade_metrics}
        logger.info(f"‚úÖ –†–∞—Å—Å—á–∏—Ç–∞–Ω—ã –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ({len(all_metrics)} –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π)")
        logger.info(f"  üí∞ –û–±—â–∏–π P&L: ${all_metrics.get('total_pnl', 0):+,.2f}")
        logger.info(f"  üìä Sharpe Ratio: {all_metrics.get('sharpe_ratio_abs', 0):.3f}")
        logger.info(f"  üìâ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø—Ä–æ—Å–∞–¥–∫–∞: {all_metrics.get('max_drawdown_abs', 0):.2%}")
        logger.info(f"  üîÑ –í—Å–µ–≥–æ —Å–¥–µ–ª–æ–∫: {all_metrics.get('total_trades', 0)}")
        logger.info(f"  üìà –¢–æ—Ä–≥—É–µ–º—ã—Ö –ø–∞—Ä: {all_metrics.get('total_pairs_traded', 0)}")
        logger.info(f"  üí∏ –û–±—â–∏–µ –∏–∑–¥–µ—Ä–∂–∫–∏: ${all_metrics.get('total_costs', 0):,.2f}")

        _emit_monitoring_metrics(all_metrics)
    
    # Create reports
    logger.info(f"‚á¢ –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    with time_block("generating reports"):
        results_dir = Path(cfg.results_dir)
        
        try:
            logger.info(f"  ‚á¢ –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
            create_performance_report(
                equity_curve=equity_series,
                pnl_series=pnl_series,
                metrics=all_metrics,
                pair_counts=pair_count_data,
                results_dir=results_dir,
                strategy_name="CointegrationStrategy"
            )
            logger.info(f"  ‚úì –û—Ç—á–µ—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ–∑–¥–∞–Ω")
            
            summary = format_metrics_summary(all_metrics)
            print(summary)
            
            # Save data
            logger.info(f"  ‚á¢ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ CSV —Ñ–∞–π–ª—ã...")
            # –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∫ float –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º —Å —Ç–∏–ø–∞–º–∏ Arrow
            clean_metrics = {}
            for key, value in all_metrics.items():
                if isinstance(value, (int, float)):
                    clean_metrics[key] = float(value)
                else:
                    clean_metrics[key] = value
            
            metrics_df = pd.DataFrame([clean_metrics])
            metrics_df.to_csv(results_dir / "strategy_metrics.csv", index=False)
            logger.info(f"üìã –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_dir / 'strategy_metrics.csv'}")
            
            if not pnl_series.empty:
                pnl_series.to_csv(results_dir / "daily_pnl.csv", header=['PnL'])
                logger.info(f"üìà –î–Ω–µ–≤–Ω—ã–µ P&L —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_dir / 'daily_pnl.csv'} ({len(pnl_series)} –∑–∞–ø–∏—Å–µ–π)")
            
            if not equity_series.empty:
                equity_series.to_csv(results_dir / "equity_curve.csv", header=['Equity'])
                logger.info(f"üíπ –ö—Ä–∏–≤–∞—è –∫–∞–ø–∏—Ç–∞–ª–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {results_dir / 'equity_curve.csv'} ({len(equity_series)} —Ç–æ—á–µ–∫)")
            
            if trade_stats:
                trades_df = pd.DataFrame(trade_stats)
                trades_df.to_csv(results_dir / "trade_statistics.csv", index=False)
                logger.info(f"üìã –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–¥–µ–ª–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {results_dir / 'trade_statistics.csv'} ({len(trades_df)} –∑–∞–ø–∏—Å–µ–π)")

            if all_trades_log:
                trades_log_df = pd.DataFrame(all_trades_log)
                trades_log_df.to_csv(results_dir / "trades_log.csv", index=False)
                logger.info(f"üìì –î–µ—Ç–∞–ª—å–Ω—ã–π –ª–æ–≥ —Å–¥–µ–ª–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {results_dir / 'trades_log.csv'} ({len(trades_log_df)} –∑–∞–ø–∏—Å–µ–π)")
            
            logger.info(f"üéâ Walk-Forward –∞–Ω–∞–ª–∏–∑ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≤–µ—Ä—à–µ–Ω!")
            logger.info(f"üìä –í—Å–µ –æ—Ç—á–µ—Ç—ã –∏ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {results_dir}")
            logger.info(f"üíº –ò—Ç–æ–≥–æ–≤—ã–π –∫–∞–ø–∏—Ç–∞–ª: ${portfolio.get_current_equity():,.2f}")
            logger.info(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {time.time() - start_time:.1f} —Å–µ–∫—É–Ω–¥")
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –æ—Ç—á–µ—Ç–æ–≤: {e}")

    # Cleanup memory-mapped data
    if use_memory_map:
        try:
            cleanup_global_data()
            logger.info("üßπ Memory-mapped data cleaned up")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error cleaning up memory-mapped data: {e}")

    equity_curve = portfolio.equity_curve
    equity_returns = equity_curve.ffill().pct_change(fill_method=None).dropna()
    periods_per_year = float(cfg.backtest.annualizing_factor)
    if bar_minutes and bar_minutes > 0:
        periods_per_year *= 24 * 60 / bar_minutes

    sharpe = performance.sharpe_ratio(equity_returns, periods_per_year)
    print(f"Annualized Sharpe Ratio: {sharpe}")

    return base_metrics


class WalkForwardOrchestrator:
    """Compatibility wrapper for running walk-forward analysis."""

    def __init__(self, cfg):
        self.cfg = cfg

    def run(self):
        """Run walk-forward analysis."""
        return run_walk_forward(self.cfg)
