# üöÄ Complete Optimization System Documentation

## Overview

The Coint2 optimization system provides a comprehensive suite of tools for hyperparameter optimization, strategy validation, and performance monitoring. The system integrates with Streamlit UI for interactive optimization and supports advanced features like multi-objective optimization, distributed processing, and A/B testing.

## Components

### 1. üî¨ Real Optuna Integration

**Files:**
- `scripts/optimization/web_optimizer.py` - Core optimization backend
- `ui/optimizer_component.py` - UI integration with real-time updates
- `ui/optimization_runner.py` - Streamlit optimization runner
- `configs/search_spaces/web_ui.yaml` - Parameter search space configuration

**Features:**
- Real backtesting using `FastWalkForwardObjective`
- Support for TPE, Random, and Grid samplers
- Pruning for early stopping of bad trials
- Real-time progress tracking
- Optuna visualization (importance, parallel coordinates, contour plots)

**Usage:**
```python
from scripts.optimization.web_optimizer import WebOptimizer

optimizer = WebOptimizer(
    base_config_path="configs/main_2024.yaml",
    search_space_path="configs/search_spaces/web_ui.yaml"
)

results = optimizer.optimize(
    n_trials=100,
    param_ranges={'zscore_threshold': (0.5, 3.0)},
    target_metric="sharpe_ratio"
)
```

### 2. üéØ Multi-Objective Optimization

**File:** `ui/multi_objective_optimizer.py`

**Features:**
- Optimize multiple metrics simultaneously
- Pareto frontier visualization
- Solution selection based on preferences
- Support for 2D, 3D, and N-dimensional Pareto fronts

**Supported Metrics:**
- Sharpe Ratio
- Total PnL
- Win Rate
- Calmar Ratio
- Sortino Ratio
- Max Drawdown
- Profit Factor
- Recovery Factor

**Usage:**
```python
from ui.multi_objective_optimizer import MultiObjectiveOptimizer

optimizer = MultiObjectiveOptimizer()
optimizer.add_objective("sharpe_ratio", direction="maximize", weight=1.0)
optimizer.add_objective("max_drawdown", direction="minimize", weight=0.5)

study = optimizer.optimize(
    n_trials=200,
    param_ranges=param_ranges,
    n_jobs=4
)

pareto_solutions = optimizer.get_pareto_front()
```

### 3. üåê Distributed Optimization

**File:** `ui/distributed_optimizer.py`

**Features:**
- Parallel optimization with multiple workers
- Support for local processes, remote machines, and Docker containers
- Shared storage (SQLite, PostgreSQL, MySQL)
- Real-time progress monitoring
- Worker statistics and timeline visualization

**Deployment Modes:**
1. **Local Processes:** Uses multiprocessing for local parallelization
2. **Remote Machines:** SSH deployment to remote workers
3. **Docker Containers:** Containerized workers with orchestration

**Usage:**
```python
from ui.distributed_optimizer import DistributedOptimizer

optimizer = DistributedOptimizer(storage_type="postgresql")

study = optimizer.run_distributed_optimization(
    n_trials_total=1000,
    n_workers=8,
    param_ranges=param_ranges
)
```

### 4. üî¨ A/B Testing

**File:** `ui/ab_testing_component.py`

**Features:**
- Statistical comparison of strategy configurations
- Multiple test periods (single, multiple, walk-forward)
- Statistical significance testing (t-test, Mann-Whitney U)
- Effect size calculation (Cohen's d)
- Visualization of results

**Statistical Tests:**
- T-test for Sharpe ratio comparison
- Mann-Whitney U test for non-parametric comparison
- Cohen's d for effect size measurement

**Usage:**
```python
from ui.ab_testing_component import ABTestRunner

runner = ABTestRunner()

results = runner.run_ab_test(
    strategy_a=baseline_config,
    strategy_b=optimized_config,
    test_periods=[("2024-01-01", "2024-03-31")],
    pairs_file="artifacts/universe/pairs_universe.yaml"
)
```

### 5. üß™ Out-of-Sample Validation

**File:** `ui/validation_component.py`

**Features:**
- Validation on unseen data periods
- Parameter configuration from optimization results
- Performance metrics comparison
- Equity curve visualization
- Trade distribution analysis

**Validation Metrics:**
- Sharpe Ratio
- Total PnL
- Win Rate
- Max Drawdown
- Number of Trades
- Average Trade PnL

### 6. üíæ Study Management

**File:** `ui/study_manager.py`

**Features:**
- Save and load optimization studies
- Study metadata management
- Compare multiple studies
- Export/import functionality
- Convergence tracking

**Operations:**
- Save study with metadata and tags
- Load study by ID
- List studies with filtering
- Delete studies
- Export to different formats
- Compare study results

## Configuration Files

### Search Spaces

**`configs/search_spaces/web_ui.yaml`:**
```yaml
signals:
  zscore_threshold:
    type: float
    low: 0.5
    high: 3.0
  rolling_window:
    type: int
    low: 10
    high: 100

risk:
  stop_loss_multiplier:
    type: float
    low: 2.0
    high: 5.0

filters:
  coint_pvalue_threshold:
    type: float
    low: 0.01
    high: 0.10

costs:
  commission_pct:
    type: float
    low: 0.0002
    high: 0.0010
```

## Streamlit UI Integration

### Launch the UI:
```bash
streamlit run ui/app.py
```

### Navigation:
1. **üî¨ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è Tab:** Single-objective optimization with Optuna
2. **üéØ Multi-Objective:** Pareto optimization for multiple metrics
3. **üåê Distributed:** Parallel optimization with multiple workers
4. **üî¨ A/B Testing:** Statistical comparison of strategies
5. **üß™ Validation:** Out-of-sample testing of optimized parameters

## Optimization Workflow

### 1. Initial Optimization
```python
# Run initial optimization
optimizer = WebOptimizer()
results = optimizer.optimize(
    n_trials=100,
    param_ranges=search_space,
    target_metric="sharpe_ratio"
)
```

### 2. Multi-Objective Refinement
```python
# Refine with multiple objectives
multi_opt = MultiObjectiveOptimizer()
multi_opt.add_objective("sharpe_ratio", "maximize")
multi_opt.add_objective("max_drawdown", "minimize")
pareto_solutions = multi_opt.optimize(n_trials=200)
```

### 3. A/B Testing
```python
# Compare baseline vs optimized
ab_runner = ABTestRunner()
test_results = ab_runner.run_ab_test(
    strategy_a=baseline,
    strategy_b=optimized,
    test_periods=periods
)
```

### 4. Out-of-Sample Validation
```python
# Validate on new data
validator = ValidationRunner()
validation_results = validator.run_validation(
    best_params=optimized_params,
    test_start="2024-05-01",
    test_end="2024-06-30"
)
```

## Performance Considerations

### Optimization Speed

| Mode | Trials/Hour | Workers | Best For |
|------|-------------|---------|----------|
| Single Thread | ~50 | 1 | Development |
| Local Parallel | ~200 | 4 | Standard optimization |
| Distributed | ~1000+ | 8+ | Large-scale search |

### Storage Requirements

- SQLite: ~100MB per 1000 trials
- PostgreSQL: Better for distributed, ~150MB per 1000 trials
- Study exports: ~10MB per study (YAML/JSON)

## Best Practices

### 1. Parameter Ranges
- Start with wide ranges, then refine
- Use log scale for parameters with large ranges
- Consider parameter interactions

### 2. Optimization Strategy
- Use TPE sampler for initial exploration
- Switch to Grid sampler for fine-tuning
- Enable pruning for large search spaces

### 3. Validation
- Always validate on out-of-sample data
- Use multiple test periods for robustness
- Check for overfitting with A/B tests

### 4. Multi-Objective
- Limit to 2-3 objectives for interpretability
- Use equal weights initially, then adjust
- Analyze Pareto front for trade-offs

## Troubleshooting

### Common Issues

1. **Slow Optimization:**
   - Reduce `max_bars` in backtest configuration
   - Use distributed optimization
   - Enable pruning

2. **Memory Issues:**
   - Use PostgreSQL instead of SQLite for large studies
   - Limit parallel workers based on available RAM
   - Clear old studies regularly

3. **Convergence Problems:**
   - Increase number of trials
   - Adjust parameter ranges
   - Check for data quality issues

## Advanced Features

### Custom Objectives
```python
def custom_objective(trial):
    # Sample parameters
    params = {
        'custom_param': trial.suggest_float('custom_param', 0, 1)
    }
    
    # Run custom evaluation
    result = evaluate_strategy(params)
    
    # Return multiple objectives
    return [result.sharpe, -result.drawdown]
```

### Remote Worker Deployment
```bash
# Deploy worker to remote machine
ssh user@remote "cd /opt/coint2 && \
    python -m scripts.optimization.remote_worker \
    --study-name distributed_study \
    --storage postgresql://localhost/optuna \
    --n-trials 100"
```

### Hyperparameter Importance Analysis
```python
# Analyze parameter importance
importance = optuna.importance.get_param_importances(study)
for param, score in importance.items():
    print(f"{param}: {score:.3f}")
```

## Integration with CI/CD

### GitHub Actions
```yaml
name: Optimization Pipeline

on:
  schedule:
    - cron: '0 0 * * 0'  # Weekly

jobs:
  optimize:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Run Optimization
        run: |
          python scripts/core/optimize.py \
            --mode balanced \
            --n-trials 500 \
            --base-config configs/main_2024.yaml
      - name: Validate Results
        run: |
          python ui/validation_component.py \
            --params outputs/best_params.yaml \
            --test-period 2024-Q2
```

## Future Enhancements

1. **Bayesian Optimization:** Advanced sampling with Gaussian processes
2. **AutoML Integration:** Automatic feature engineering
3. **Cloud Deployment:** AWS/GCP/Azure integration
4. **Real-time Monitoring:** Live strategy performance tracking
5. **Ensemble Methods:** Combine multiple optimized strategies

## Conclusion

The optimization system provides a complete toolkit for strategy development, from initial parameter search through validation and deployment. The modular design allows for easy extension and customization while maintaining robustness and performance.

For questions or contributions, please refer to the project documentation or submit an issue on GitHub.